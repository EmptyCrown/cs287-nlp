{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.vocab import Vectors, GloVe\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from hw1.models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Our input $x$\n",
    "TEXT = torchtext.data.Field()\n",
    "    \n",
    "# Our labels $y$\n",
    "LABEL = torchtext.data.Field(sequential=False)\n",
    "\n",
    "\n",
    "train, val, test = torchtext.datasets.SST.splits(\n",
    "    TEXT, LABEL,\n",
    "    filter_pred=lambda ex: ex.label != 'neutral')\n",
    "\n",
    "# Build vocab\n",
    "TEXT.build_vocab(train)\n",
    "LABEL.build_vocab(train)\n",
    "\n",
    "train_iter, val_iter, test_iter = torchtext.data.BucketIterator.splits(\n",
    "(train, val, test), batch_size=10, device=-1)\n",
    "\n",
    "# Build the vocabulary with word embeddings\n",
    "url = 'https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.simple.vec'\n",
    "TEXT.vocab.load_vectors(vectors=Vectors('wiki.simple.vec', url=url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss: 1.102232\n",
      "Iteration 100, loss: 0.590585\n",
      "Iteration 200, loss: 0.736041\n",
      "Iteration 300, loss: 0.674967\n",
      "Iteration 400, loss: 0.670788\n",
      "Iteration 500, loss: 0.459173\n",
      "Iteration 600, loss: 0.630579\n",
      "Iteration 700, loss: 0.432746\n",
      "Iteration 800, loss: 0.598211\n",
      "Iteration 900, loss: 0.334833\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(TEXT, LABEL)\n",
    "trainer = TextTrainer(TEXT, LABEL, model)\n",
    "trainer.train(train_iter, num_iter=1000, skip_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, TEXT, LABEL):\n",
    "        super(CBOW, self).__init__()\n",
    "        # Embeddings vectors (should be trainable); [V, d]\n",
    "        # TODO: is default for requires_grad True?\n",
    "        self.embeddings = nn.EmbeddingBag(TEXT.vocab.vectors.size()[0],\n",
    "                                          TEXT.vocab.vectors.size()[1],\n",
    "                                          mode='sum')\n",
    "        self.embeddings.weight = nn.Parameter(TEXT.vocab.vectors, requires_grad=True)\n",
    "        \n",
    "        # Linear layer\n",
    "        self.linear = nn.Linear(TEXT.vocab.vectors.size()[1], len(LABEL.vocab))\n",
    "        \n",
    "    # Here bow is [len-of-sentence, N] -- it is an integer matrix\n",
    "    def forward(self, bow):\n",
    "        bow_features = self.embeddings(bow)\n",
    "        return F.log_softmax(self.linear(bow_features), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
