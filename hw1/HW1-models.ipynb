{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.vocab import Vectors, GloVe\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from hw1.models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Our input $x$\n",
    "TEXT = torchtext.data.Field()\n",
    "    \n",
    "# Our labels $y$\n",
    "LABEL = torchtext.data.Field(sequential=False)\n",
    "\n",
    "\n",
    "train, val, test = torchtext.datasets.SST.splits(\n",
    "    TEXT, LABEL,\n",
    "    filter_pred=lambda ex: ex.label != 'neutral')\n",
    "\n",
    "# Build vocab\n",
    "TEXT.build_vocab(train)\n",
    "LABEL.build_vocab(train)\n",
    "\n",
    "train_iter, val_iter, test_iter = torchtext.data.BucketIterator.splits(\n",
    "(train, val, test), batch_size=11, device=-1)\n",
    "\n",
    "# Build the vocabulary with word embeddings\n",
    "url = 'https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.simple.vec'\n",
    "TEXT.vocab.load_vectors(vectors=Vectors('wiki.simple.vec', url=url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss: 1.102232\n",
      "Iteration 100, loss: 0.590585\n",
      "Iteration 200, loss: 0.736041\n",
      "Iteration 300, loss: 0.674967\n",
      "Iteration 400, loss: 0.670788\n",
      "Iteration 500, loss: 0.459173\n",
      "Iteration 600, loss: 0.630579\n",
      "Iteration 700, loss: 0.432746\n",
      "Iteration 800, loss: 0.598211\n",
      "Iteration 900, loss: 0.334833\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(TEXT, LABEL)\n",
    "trainer = TextTrainer(TEXT, LABEL, model)\n",
    "trainer.train(train_iter, num_iter=1000, skip_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n"
     ]
    }
   ],
   "source": [
    "model = MultinomialNB2(TEXT, LABEL)\n",
    "model.train(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1500, 1821)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_eval2(model, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultinomialNB2(nn.Module):\n",
    "    def __init__(self, TEXT, LABEL):\n",
    "        super(MultinomialNB2, self).__init__()\n",
    "        self._TEXT = TEXT\n",
    "        self._LABEL = LABEL\n",
    "        self._text_vocab_len = len(self._TEXT.vocab)        \n",
    "        self.n_positive = 0\n",
    "        self.n_negative = 0\n",
    "        # Smoothing para is 1 for all features\n",
    "        self.p = torch.ones(self._text_vocab_len)\n",
    "        self.q = torch.ones(self._text_vocab_len)\n",
    "        self.r = None\n",
    "        self.index_pos = LABEL.vocab.itos.index('positive')\n",
    "        self.index_neg = LABEL.vocab.itos.index('negative')\n",
    "\n",
    "    # could use EmbeddingsBag, but there's not a huge difference in\n",
    "    # performance\n",
    "    def get_features(self, batch):\n",
    "        size_batch = batch.size()[0]\n",
    "        features = torch.zeros(size_batch, self._text_vocab_len)\n",
    "        for i in range(size_batch):\n",
    "            for j in batch[i, :]:\n",
    "                features[i, j.data[0]] += 1\n",
    "        return features\n",
    "        # return torch.Tensor(features)\n",
    "\n",
    "    def train(self, train_iter):\n",
    "        # There's probably a better way to do this\n",
    "        num_iter = len(train_iter)\n",
    "        train_iter = iter(train_iter)\n",
    "        for i in range(num_iter):\n",
    "            batch = next(train_iter)\n",
    "            if i % 100 == 0:\n",
    "                print(i)\n",
    "            # Should be [N, num-features]\n",
    "            features = self.get_features(torch.t(batch.text).contiguous())\n",
    "\n",
    "            # Using broadcasting\n",
    "            inds_pos = torch.nonzero(batch.label.data == self.index_pos)\n",
    "            inds_neg = torch.nonzero(batch.label.data == self.index_neg)\n",
    "\n",
    "\n",
    "            if inds_pos.size():\n",
    "                self.n_positive += inds_pos.size()[0]\n",
    "                self.p = torch.add(self.p, torch.sum(features[inds_pos, :], dim=0))                \n",
    "            if inds_neg.size():\n",
    "                self.n_negative += inds_neg.size()[0]\n",
    "                self.q = torch.add(self.q, torch.sum(features[inds_neg, :], dim=0))\n",
    "\n",
    "            # print(features)\n",
    "            # print(inds_neg, inds_pos)\n",
    "            # print(self.p.size(), torch.sum(features, dim=0).size())\n",
    "\n",
    "        self.r = torch.log((self.p / self.p.sum()) / (self.q / self.q.sum()))\n",
    "        \n",
    "    def forward(self, batch):\n",
    "        # for k in range(batch_text.size()[1]):\n",
    "        features = self.get_features(batch)\n",
    "        # Using broadcasting\n",
    "        return torch.matmul(features, torch.squeeze(self.r)) + \\\n",
    "            np.log(self.n_positive / self.n_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval2(model, test):\n",
    "    test_iter = torchtext.data.BucketIterator(test, train=False, batch_size=10,\n",
    "                                              device=-1)\n",
    "    cnt_correct = 0\n",
    "    cnt_total = 0\n",
    "    for batch in test_iter:\n",
    "        probs = model(torch.t(batch.text).contiguous())\n",
    "        if len(probs.size()) == 1 or (len(probs.size()) == 2 \\\n",
    "                                      and probes.size()[1] == 1):\n",
    "            signs = torch.sign(probs).type(torch.LongTensor)\n",
    "            classes = (signs + 1) * (model.index_pos - model.index_neg) / 2 + \\\n",
    "                      model.index_neg\n",
    "            # print(classes, probs)\n",
    "        else:\n",
    "            _, argmax = probs.max(1)\n",
    "            classes = argmax\n",
    "            \n",
    "        cnt_total += batch.text.size()[1]\n",
    "        # print(batch.label == argmax, (batch.label == argmax).sum().data[0])\n",
    "        cnt_correct += (classes == batch.label.data).sum()\n",
    "    return (cnt_correct, cnt_total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, TEXT, LABEL):\n",
    "        super(CBOW, self).__init__()\n",
    "        # Embeddings vectors (should be trainable); [V, d]\n",
    "        # TODO: is default for requires_grad True?\n",
    "        self.embeddings = nn.EmbeddingBag(TEXT.vocab.vectors.size()[0],\n",
    "                                          TEXT.vocab.vectors.size()[1],\n",
    "                                          mode='sum')\n",
    "        self.embeddings.weight = nn.Parameter(TEXT.vocab.vectors, requires_grad=True)\n",
    "        \n",
    "        # Linear layer\n",
    "        self.linear = nn.Linear(TEXT.vocab.vectors.size()[1], len(LABEL.vocab))\n",
    "        \n",
    "    # Here bow is [len-of-sentence, N] -- it is an integer matrix\n",
    "    def forward(self, bow):\n",
    "        bow_features = self.embeddings(bow)\n",
    "        return F.log_softmax(self.linear(bow_features), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
