{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Text text processing library\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torchtext.vocab import Vectors\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import itertools as it\n",
    "from models import *\n",
    "from helpers import *\n",
    "# import main\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import time\n",
    "import itertools as it\n",
    "MAX_LEN = 20\n",
    "MIN_FREQ = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "BOS_WORD = '<s>'\n",
    "EOS_WORD = '</s>'\n",
    "DE = data.Field(tokenize=tokenize_de)\n",
    "\n",
    "# only target needs BOS/EOS:\n",
    "EN = data.Field(tokenize=tokenize_en, init_token = BOS_WORD, eos_token = EOS_WORD) \n",
    "\n",
    "train, val, test = datasets.IWSLT.splits(exts=('.de', '.en'), fields=(DE, EN), \n",
    "                                         filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and \n",
    "                                         len(vars(x)['trg']) <= MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DE.build_vocab(train.src, min_freq=MIN_FREQ)\n",
    "EN.build_vocab(train.trg, min_freq=MIN_FREQ)\n",
    "\n",
    "pred_set = []\n",
    "for i, line in enumerate(open(\"source_test.txt\"), 1):\n",
    "    words = line.split()# [:-1]\n",
    "    pred_set.append([DE.vocab.stoi[s] for s in words])\n",
    "\n",
    "train_iter, val_iter, test_iter = data.BucketIterator.splits((train, val, test), batch_size=32, device=-1,\n",
    "                                                  repeat=False, sort_key=lambda x: len(x.src))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch = next(iter(test_iter))\n",
    "# sent_inspect(batch,4)\n",
    "def sent_inspect(batch, idx=0):\n",
    "    print(\"Source\")\n",
    "    print(' '.join([DE.vocab.itos[w] for w in batch.src.data[:,idx]]))\n",
    "    print(\"Target\")\n",
    "    print(' '.join([EN.vocab.itos[w] for w in batch.trg.data[:,idx]]))\n",
    "# print(DE.vocab.stoi['<pad>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wo sich der Lauf des <unk> früher befand .\n",
      "<s> We started to be able to see where the <unk> used to flow . </s> <pad> <pad>\n",
      "Das führte mich dazu , Satellitenbilder zu benutzen .\n",
      "<s> This is really what brought me to using satellite imagery . </s> <pad> <pad> <pad> <pad> <pad>\n",
      "Wie unterscheidet sich diese Geschichte von der anderen ?\n",
      "<s> How is this story different ? </s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Im Raum war ein junger Mann , <unk> .\n",
      "<s> One of the young men in the room was <unk> . </s> <pad> <pad> <pad> <pad> <pad>\n",
      "Ich brachte rund 90 junge <unk> Führungskräfte zusammen .\n",
      "<s> I brought together about 90 young Somali leaders . </s> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Es vergeht vielleicht ein Jahr , aber nichts .\n",
      "<s> Maybe a year passes , nothing . </s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Sein Dorf liegt in der Nähe von <unk> .\n",
      "<s> His village is near <unk> . </s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Ich denke , diese beiden Figuren sind Experten .\n",
      "<s> I think these people are experts . </s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Vielleicht ist es nicht nur das <unk> Kleid .\n",
      "<s> Maybe it 's not just the <unk> dress . </s> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "Wer sind diese Typen ? Was lernen sie ?\n",
      "<s> Who are these guys ? What are they learning ? </s> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(' '.join(DE.vocab.itos[w] for w in debug_set[i]))\n",
    "    print(' '.join(EN.vocab.itos[w] for w in debug_ans[i]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13353, 500])\n",
      "torch.Size([2000, 500])\n",
      "torch.Size([2000, 500])\n",
      "torch.Size([2000])\n",
      "torch.Size([2000])\n",
      "torch.Size([2000, 500])\n",
      "torch.Size([2000, 500])\n",
      "torch.Size([2000])\n",
      "torch.Size([2000])\n",
      "torch.Size([2000, 500])\n",
      "torch.Size([2000, 500])\n",
      "torch.Size([2000])\n",
      "torch.Size([2000])\n",
      "torch.Size([2000, 500])\n",
      "torch.Size([2000, 500])\n",
      "torch.Size([2000])\n",
      "torch.Size([2000])\n"
     ]
    }
   ],
   "source": [
    "state_dict = torch.load('saved_models/attn_med_2.epoch_12.ckpt.tar')\n",
    "for p in state_dict['model_encoder']:\n",
    "    print(state_dict['model_encoder'][p].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target padding token: 1\n",
      "Using CUDA...\n",
      "Target padding token: 1\n",
      "Using CUDA...\n",
      "Epoch 0, loss: 75.106003, norm: 5.068851, elapsed: 601.591950, lrn_rate: 0.700000\n",
      "Validation time: 1.177268 seconds\n",
      "Validation set metric: 15.046797\n",
      "Epoch 1, loss: 64.709015, norm: 6.823921, elapsed: 1202.462563, lrn_rate: 0.700000\n",
      "Validation time: 1.123111 seconds\n",
      "Validation set metric: 10.318951\n",
      "Epoch 2, loss: 55.162987, norm: 5.810015, elapsed: 1815.646121, lrn_rate: 0.700000\n",
      "Validation time: 1.158648 seconds\n",
      "Validation set metric: 8.200983\n",
      "Epoch 3, loss: 48.013638, norm: 6.298200, elapsed: 2430.358610, lrn_rate: 0.700000\n",
      "Validation time: 1.222892 seconds\n",
      "Validation set metric: 7.256750\n",
      "Epoch 4, loss: 55.855797, norm: 8.171973, elapsed: 3034.166331, lrn_rate: 0.700000\n",
      "Validation time: 1.161787 seconds\n",
      "Validation set metric: 7.055706\n",
      "Epoch 5, loss: 51.494362, norm: 9.309718, elapsed: 3637.575163, lrn_rate: 0.700000\n",
      "Validation time: 1.184436 seconds\n",
      "Validation set metric: 7.108363\n",
      "Decaying LR to 0.350000\n",
      "Epoch 6, loss: 34.349045, norm: 8.055377, elapsed: 4239.637285, lrn_rate: 0.350000\n",
      "Validation time: 1.196922 seconds\n",
      "Validation set metric: 6.700641\n",
      "Epoch 7, loss: 26.329096, norm: 8.606746, elapsed: 4846.299028, lrn_rate: 0.350000\n",
      "Validation time: 1.140525 seconds\n",
      "Validation set metric: 7.049315\n",
      "Decaying LR to 0.175000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-ffd2c44189bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                      lrn_decay='adaptive', reverse_enc_input=False)\n\u001b[1;32m      7\u001b[0m \u001b[0mevaluator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNMTEvaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbs_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs_decoder\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse_enc_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevaluator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-78-565b039c9b12>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, torch_train_iter, le, val_iter, **kwargs)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m                 \u001b[0mres_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'skip_iter'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-78-565b039c9b12>\u001b[0m in \u001b[0;36mtrain_batch\u001b[0;34m(self, batch, **kwargs)\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-78-565b039c9b12>\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m(self, batch, mode)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;31m# For attention, will use enc_output (not otherwise)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0menc_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_src\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprev_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc_hidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_attention\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-cab290137def>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_tsr, hidden)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# output is [batch, sent_len, hidden_size]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded_tsr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# TODO: perhaps add dropout to output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0mflat_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         )\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_packed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPackedSequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(input, *fargs, **fkwargs)\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhack_onnx_rnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mfargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36m_do_forward\u001b[0;34m(self, *input)\u001b[0m\n\u001b[1;32m    326\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nested_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0mflat_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_iter_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m         \u001b[0mflat_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNestedIOFunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mflat_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m         \u001b[0mnested_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nested_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0mnested_variables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nested_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mnested_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_map_variable_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nested_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_extended\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnested_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nested_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nested_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/_functions/rnn.py\u001b[0m in \u001b[0;36mforward_extended\u001b[0;34m(self, input, weight, hx)\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0mhy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m         \u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_for_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/backends/cudnn/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(fn, input, hx, weight, output, hy)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0;31m# init descriptors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m         \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_desc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_rnn_descriptor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_input_packed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m             \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_descs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescriptor_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/backends/cudnn/rnn.py\u001b[0m in \u001b[0;36minit_rnn_descriptor\u001b[0;34m(fn, handle)\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbidirectional\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatatype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     )\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/backends/cudnn/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, handle, hidden_size, num_layers, dropout_desc, input_mode, bidirectional, mode, datatype)\u001b[0m\n\u001b[1;32m    259\u001b[0m             ))\n\u001b[1;32m    260\u001b[0m             if version() >= 7000 and int(cuda[0]) >= 9 and (\n\u001b[0;32m--> 261\u001b[0;31m                     torch.cuda.get_device_capability(torch.cuda.current_device())[0] >= 7):\n\u001b[0m\u001b[1;32m    262\u001b[0m                 \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnnSetRNNMatrixMathType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCUDNN_DEFAULT_MATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdatatype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mCUDNN_DATA_HALF\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mget_device_capability\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \"\"\"\n\u001b[1;32m    246\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_getDeviceCapability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_iter, val_iter = data.BucketIterator.splits((train, val), batch_size=32, device=-1,\n",
    "                                                  repeat=False, sort_key=lambda x: len(x.src))\n",
    "bs_encoder = BaseEncoder(DE, hidden_size=500, num_layers=4, word_features=500)\n",
    "bs_decoder = BaseDecoder(EN, hidden_size=500, num_layers=4, word_features=500)\n",
    "trainer = NMTTrainer([bs_encoder, bs_decoder], DE, EN, lrn_rate=0.7, \n",
    "                     lrn_decay='adaptive', reverse_enc_input=False)\n",
    "evaluator = NMTEvaluator([bs_encoder, bs_decoder], DE, EN, reverse_enc_input=False)\n",
    "trainer.train(train_iter, verbose=True, le=evaluator, val_iter=val_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1.3333\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = autograd.Variable(torch.Tensor([[1,1],[2,2]]))\n",
    "b = torch.sum((a != 4).type(torch.FloatTensor)) / 3\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using final MLP\n",
      "Target padding token: 1\n",
      "Source padding token: 1\n",
      "Using CUDA...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "torch.index_select received an invalid combination of arguments - got (\u001b[32;1mtorch.FloatTensor\u001b[0m, \u001b[32;1mint\u001b[0m, \u001b[31;1mtorch.cuda.LongTensor\u001b[0m), but expected (torch.FloatTensor source, int dim, torch.LongTensor index)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-1d32511a44d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m evaluator = NMTEvaluator([bs_encoder, at_decoder], DE, EN, attention=True,\n\u001b[1;32m      8\u001b[0m                         record_attention=False, cuda=True)\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mevaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-30-e69fd452cf91>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, test_iter, num_iter)\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0mnll_cnt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sum'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m             \u001b[0;31m# TODO: make sure loss just has 1 element!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0mnll_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-30-e69fd452cf91>\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m(self, batch, mode)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;31m# For attention, will use enc_output (not otherwise)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0menc_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_src\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_enc_prev_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-f6832d1ec865>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_tsr, hidden)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tsr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# [batch_sz, sent_len, D]:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0membedded_tsr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tsr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         )\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/_functions/thnn/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(cls, ctx, indices, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: torch.index_select received an invalid combination of arguments - got (\u001b[32;1mtorch.FloatTensor\u001b[0m, \u001b[32;1mint\u001b[0m, \u001b[31;1mtorch.cuda.LongTensor\u001b[0m), but expected (torch.FloatTensor source, int dim, torch.LongTensor index)"
     ]
    }
   ],
   "source": [
    "bs_encoder = BaseEncoder(DE, hidden_size=650, num_layers=4, word_features=650, \n",
    "                         bidirectional=True, dropout=0.35)\n",
    "# TODO: decide whether to add dropout to output states of encoder!\n",
    "at_decoder = AttnDecoder(EN, hidden_size=650, num_layers=4, word_features=650, dropout=0.35,\n",
    "                         tie_weights=True, enc_linear=650,\n",
    "                         enc_bidirectional=True)\n",
    "evaluator = NMTEvaluator([bs_encoder, at_decoder], DE, EN, attention=True,\n",
    "                        record_attention=False, cuda=True)\n",
    "evaluator.evaluate(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using final MLP\n",
      "Target padding token: 1\n",
      "Source padding token: 1\n",
      "Using CUDA...\n",
      "Target padding token: 1\n",
      "Source padding token: 1\n",
      "Using CUDA...\n",
      "Innitializing parameters status:  True\n",
      "Epoch 0, loss: 46.713978, norm: 5.329768, elapsed: 1063.894383, lrn_rate: 1.200000\n",
      "Validation time: 1.941442 seconds\n",
      "Validation set metric: 10.928670\n",
      "Saving model to saved_models/attn_med_5.epoch_0.ckpt.tar\n",
      "Epoch 1, loss: 42.267780, norm: 7.347247, elapsed: 2163.232243, lrn_rate: 1.200000\n",
      "Validation time: 1.904391 seconds\n",
      "Validation set metric: 6.930205\n",
      "Saving model to saved_models/attn_med_5.epoch_1.ckpt.tar\n",
      "Epoch 2, loss: 36.884426, norm: 7.626665, elapsed: 3208.078403, lrn_rate: 1.200000\n",
      "Validation time: 1.817674 seconds\n",
      "Validation set metric: 5.795428\n",
      "Saving model to saved_models/attn_med_5.epoch_2.ckpt.tar\n",
      "Epoch 3, loss: 31.893240, norm: 6.090264, elapsed: 4255.479510, lrn_rate: 1.200000\n",
      "Validation time: 1.843599 seconds\n",
      "Validation set metric: 5.251451\n",
      "Saving model to saved_models/attn_med_5.epoch_3.ckpt.tar\n",
      "Epoch 4, loss: 34.269463, norm: 6.271408, elapsed: 5301.015239, lrn_rate: 1.200000\n",
      "Validation time: 1.878317 seconds\n",
      "Validation set metric: 4.977437\n",
      "Saving model to saved_models/attn_med_5.epoch_4.ckpt.tar\n",
      "Epoch 5, loss: 30.130325, norm: 6.102463, elapsed: 6348.086979, lrn_rate: 1.200000\n",
      "Validation time: 1.914788 seconds\n",
      "Validation set metric: 4.806715\n",
      "Saving model to saved_models/attn_med_5.epoch_5.ckpt.tar\n",
      "Epoch 6, loss: 30.701176, norm: 8.403505, elapsed: 7394.851094, lrn_rate: 1.200000\n",
      "Validation time: 1.970458 seconds\n",
      "Validation set metric: 4.643665\n",
      "Saving model to saved_models/attn_med_5.epoch_6.ckpt.tar\n",
      "Epoch 7, loss: 30.256733, norm: 7.704153, elapsed: 8446.352725, lrn_rate: 1.200000\n",
      "Validation time: 1.945064 seconds\n",
      "Validation set metric: 4.613053\n",
      "Saving model to saved_models/attn_med_5.epoch_7.ckpt.tar\n",
      "Decaying LR to 0.600000\n",
      "Epoch 8, loss: 26.882694, norm: 6.951996, elapsed: 9498.123146, lrn_rate: 0.600000\n",
      "Validation time: 1.780255 seconds\n",
      "Validation set metric: 4.367376\n",
      "Saving model to saved_models/attn_med_5.epoch_8.ckpt.tar\n",
      "Decaying LR to 0.300000\n",
      "Epoch 9, loss: 23.544054, norm: 7.642145, elapsed: 10547.934266, lrn_rate: 0.300000\n",
      "Validation time: 1.908327 seconds\n",
      "Validation set metric: 4.295395\n",
      "Saving model to saved_models/attn_med_5.epoch_9.ckpt.tar\n",
      "Decaying LR to 0.150000\n",
      "Epoch 10, loss: 21.876459, norm: 9.498790, elapsed: 11595.942855, lrn_rate: 0.150000\n",
      "Validation time: 1.931998 seconds\n",
      "Validation set metric: 4.293765\n",
      "Saving model to saved_models/attn_med_5.epoch_10.ckpt.tar\n",
      "Decaying LR to 0.075000\n",
      "Epoch 11, loss: 19.729061, norm: 6.939862, elapsed: 12646.532219, lrn_rate: 0.075000\n",
      "Validation time: 2.039602 seconds\n",
      "Validation set metric: 4.319367\n",
      "Saving model to saved_models/attn_med_5.epoch_11.ckpt.tar\n",
      "Decaying LR to 0.037500\n",
      "Epoch 12, loss: 19.253206, norm: 7.081698, elapsed: 13692.962103, lrn_rate: 0.037500\n",
      "Validation time: 1.980938 seconds\n",
      "Validation set metric: 4.336978\n",
      "Saving model to saved_models/attn_med_5.epoch_12.ckpt.tar\n",
      "Decaying LR to 0.018750\n",
      "Epoch 13, loss: 19.956722, norm: 9.532681, elapsed: 14742.002679, lrn_rate: 0.018750\n",
      "Validation time: 1.929855 seconds\n",
      "Validation set metric: 4.334616\n",
      "Saving model to saved_models/attn_med_5.epoch_13.ckpt.tar\n",
      "Decaying LR to 0.009375\n",
      "Epoch 14, loss: 19.673527, norm: 7.187007, elapsed: 15789.994987, lrn_rate: 0.009375\n",
      "Validation time: 1.889208 seconds\n",
      "Validation set metric: 4.336221\n",
      "Saving model to saved_models/attn_med_5.epoch_14.ckpt.tar\n",
      "Decaying LR to 0.004687\n",
      "Epoch 15, loss: 24.756987, norm: 9.835148, elapsed: 16837.432251, lrn_rate: 0.004687\n",
      "Validation time: 1.879341 seconds\n",
      "Validation set metric: 4.338009\n",
      "Saving model to saved_models/attn_med_5.epoch_15.ckpt.tar\n",
      "Decaying LR to 0.002344\n",
      "Epoch 16, loss: 19.212957, norm: 6.744505, elapsed: 17883.628807, lrn_rate: 0.002344\n",
      "Validation time: 1.859986 seconds\n",
      "Validation set metric: 4.337942\n",
      "Saving model to saved_models/attn_med_5.epoch_16.ckpt.tar\n",
      "Decaying LR to 0.001172\n",
      "Epoch 17, loss: 20.900364, norm: 11.724198, elapsed: 18928.885718, lrn_rate: 0.001172\n",
      "Validation time: 1.909735 seconds\n",
      "Validation set metric: 4.337952\n",
      "Saving model to saved_models/attn_med_5.epoch_17.ckpt.tar\n",
      "Decaying LR to 0.000586\n",
      "Epoch 18, loss: 23.690250, norm: 8.814668, elapsed: 19976.985049, lrn_rate: 0.000586\n",
      "Validation time: 1.840697 seconds\n",
      "Validation set metric: 4.338313\n",
      "Saving model to saved_models/attn_med_5.epoch_18.ckpt.tar\n",
      "Decaying LR to 0.000293\n",
      "Epoch 19, loss: 18.161837, norm: 7.431848, elapsed: 21027.056560, lrn_rate: 0.000293\n",
      "Validation time: 1.870655 seconds\n",
      "Validation set metric: 4.338334\n",
      "Saving model to saved_models/attn_med_5.epoch_19.ckpt.tar\n",
      "FINAL VAL PERF 4.338333858249031\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.338333858249031"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_iter, val_iter = data.BucketIterator.splits((train, val), batch_size=32, device=-1,\n",
    "                                                  repeat=False, sort_key=lambda x: len(x.src))\n",
    "bs_encoder = BaseEncoder(DE, hidden_size=650, num_layers=4, word_features=650, \n",
    "                         bidirectional=True, dropout=0.35)\n",
    "# TODO: decide whether to add dropout to output states of encoder!\n",
    "at_decoder = AttnDecoder(EN, hidden_size=650, num_layers=4, word_features=650, dropout=0.35,\n",
    "                         tie_weights=True, enc_linear=650,\n",
    "                         enc_bidirectional=True)\n",
    "trainer = NMTTrainer([bs_encoder, at_decoder], DE, EN, lrn_rate=1.2, \n",
    "                     lrn_decay='adaptive', attention=True,\n",
    "                     clip_norm=5, lrn_decay_force=8, cuda=True)\n",
    "evaluator = NMTEvaluator([bs_encoder, at_decoder], DE, EN, attention=True,\n",
    "                        record_attention=False, cuda=True)\n",
    "trainer.train(train_iter, verbose=True, le=evaluator, val_iter=val_iter,\n",
    "              save_model_fn='attn_med_5', num_iter=20)\n",
    "# KEYXX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 False 4 0.2\n",
      "Target padding token: 1\n",
      "Using CUDA...\n",
      "Target padding token: 1\n",
      "Using CUDA...\n",
      "Innitializing parameters status:  True\n",
      "Epoch 0, loss: 83.561142, norm: 4.071474, elapsed: 271.279075, lrn_rate: 1.000000\n",
      "Validation time: 0.360532 seconds\n",
      "Validation set metric: 22.966567\n",
      "Saving model to saved_models/attn_grid_0.epoch_0.ckpt.tar\n",
      "Epoch 1, loss: 82.306152, norm: 4.954175, elapsed: 548.610403, lrn_rate: 1.000000\n",
      "Validation time: 0.354557 seconds\n",
      "Validation set metric: 15.265598\n",
      "Saving model to saved_models/attn_grid_0.epoch_1.ckpt.tar\n",
      "Epoch 2, loss: 59.941078, norm: 4.696200, elapsed: 828.961941, lrn_rate: 1.000000\n",
      "Validation time: 0.392126 seconds\n",
      "Validation set metric: 9.756422\n",
      "Saving model to saved_models/attn_grid_0.epoch_2.ckpt.tar\n",
      "Epoch 3, loss: 55.912140, norm: 5.782441, elapsed: 1111.078448, lrn_rate: 1.000000\n",
      "Validation time: 0.360725 seconds\n",
      "Validation set metric: 7.972600\n",
      "Saving model to saved_models/attn_grid_0.epoch_3.ckpt.tar\n",
      "Decaying LR to 0.500000\n",
      "Epoch 4, loss: 54.147552, norm: 5.253220, elapsed: 1392.845590, lrn_rate: 0.500000\n",
      "Validation time: 0.376834 seconds\n",
      "Validation set metric: 6.815474\n",
      "Saving model to saved_models/attn_grid_0.epoch_4.ckpt.tar\n",
      "Decaying LR to 0.250000\n",
      "Epoch 5, loss: 43.327965, norm: 6.232311, elapsed: 1674.932675, lrn_rate: 0.250000\n",
      "Validation time: 0.379969 seconds\n",
      "Validation set metric: 6.307426\n",
      "Saving model to saved_models/attn_grid_0.epoch_5.ckpt.tar\n",
      "Decaying LR to 0.125000\n",
      "Epoch 6, loss: 50.873741, norm: 6.914220, elapsed: 1957.194886, lrn_rate: 0.125000\n",
      "Validation time: 0.361607 seconds\n",
      "Validation set metric: 6.090649\n",
      "Saving model to saved_models/attn_grid_0.epoch_6.ckpt.tar\n",
      "Decaying LR to 0.062500\n",
      "Epoch 7, loss: 48.331512, norm: 5.963121, elapsed: 2240.120492, lrn_rate: 0.062500\n",
      "Validation time: 0.389374 seconds\n",
      "Validation set metric: 6.008249\n",
      "Saving model to saved_models/attn_grid_0.epoch_7.ckpt.tar\n",
      "Decaying LR to 0.031250\n",
      "Epoch 8, loss: 44.363621, norm: 5.598820, elapsed: 2523.012309, lrn_rate: 0.031250\n",
      "Validation time: 0.377136 seconds\n",
      "Validation set metric: 5.965104\n",
      "Saving model to saved_models/attn_grid_0.epoch_8.ckpt.tar\n",
      "Decaying LR to 0.015625\n",
      "Epoch 9, loss: 43.252693, norm: 5.788473, elapsed: 2807.376643, lrn_rate: 0.015625\n",
      "Validation time: 0.353863 seconds\n",
      "Validation set metric: 5.945370\n",
      "Saving model to saved_models/attn_grid_0.epoch_9.ckpt.tar\n",
      "FINAL VAL PERF 5.945369884957124\n",
      "200 False 4 0.4\n",
      "Target padding token: 1\n",
      "Using CUDA...\n",
      "Target padding token: 1\n",
      "Using CUDA...\n",
      "Innitializing parameters status:  True\n",
      "Epoch 0, loss: 83.931732, norm: 4.161091, elapsed: 272.424800, lrn_rate: 1.000000\n",
      "Validation time: 0.351562 seconds\n",
      "Validation set metric: 22.726181\n",
      "Saving model to saved_models/attn_grid_1.epoch_0.ckpt.tar\n",
      "Epoch 1, loss: 87.767494, norm: 4.834038, elapsed: 545.748289, lrn_rate: 1.000000\n",
      "Validation time: 0.356378 seconds\n",
      "Validation set metric: 18.173647\n",
      "Saving model to saved_models/attn_grid_1.epoch_1.ckpt.tar\n",
      "Epoch 2, loss: 68.219200, norm: 4.212573, elapsed: 820.910688, lrn_rate: 1.000000\n",
      "Validation time: 0.348892 seconds\n",
      "Validation set metric: 14.374986\n",
      "Saving model to saved_models/attn_grid_1.epoch_2.ckpt.tar\n",
      "Epoch 3, loss: 63.636070, norm: 5.670929, elapsed: 1095.612525, lrn_rate: 1.000000\n",
      "Validation time: 0.355230 seconds\n",
      "Validation set metric: 10.502690\n",
      "Saving model to saved_models/attn_grid_1.epoch_3.ckpt.tar\n",
      "Decaying LR to 0.500000\n",
      "Epoch 4, loss: 62.408348, norm: 5.245264, elapsed: 1372.954282, lrn_rate: 0.500000\n",
      "Validation time: 0.378727 seconds\n",
      "Validation set metric: 8.745535\n",
      "Saving model to saved_models/attn_grid_1.epoch_4.ckpt.tar\n",
      "Decaying LR to 0.250000\n",
      "Epoch 5, loss: 51.085903, norm: 6.189110, elapsed: 1651.839379, lrn_rate: 0.250000\n",
      "Validation time: 0.354607 seconds\n",
      "Validation set metric: 7.971170\n",
      "Saving model to saved_models/attn_grid_1.epoch_5.ckpt.tar\n",
      "Decaying LR to 0.125000\n",
      "Epoch 6, loss: 60.441063, norm: 7.284904, elapsed: 1931.029200, lrn_rate: 0.125000\n",
      "Validation time: 0.360559 seconds\n",
      "Validation set metric: 7.591719\n",
      "Saving model to saved_models/attn_grid_1.epoch_6.ckpt.tar\n",
      "Decaying LR to 0.062500\n",
      "Epoch 7, loss: 57.327953, norm: 5.992233, elapsed: 2210.519960, lrn_rate: 0.062500\n",
      "Validation time: 0.349486 seconds\n",
      "Validation set metric: 7.432932\n",
      "Saving model to saved_models/attn_grid_1.epoch_7.ckpt.tar\n",
      "Decaying LR to 0.031250\n",
      "Epoch 8, loss: 52.496510, norm: 5.785281, elapsed: 2490.403746, lrn_rate: 0.031250\n",
      "Validation time: 0.355883 seconds\n",
      "Validation set metric: 7.341582\n",
      "Saving model to saved_models/attn_grid_1.epoch_8.ckpt.tar\n",
      "Decaying LR to 0.015625\n",
      "Epoch 9, loss: 51.212994, norm: 5.904938, elapsed: 2769.520470, lrn_rate: 0.015625\n",
      "Validation time: 0.361634 seconds\n",
      "Validation set metric: 7.306199\n",
      "Saving model to saved_models/attn_grid_1.epoch_9.ckpt.tar\n",
      "FINAL VAL PERF 7.306198857053239\n",
      "200 False 8 0.2\n",
      "Target padding token: 1\n",
      "Using CUDA...\n",
      "Target padding token: 1\n",
      "Using CUDA...\n",
      "Innitializing parameters status:  True\n",
      "Epoch 0, loss: 85.067680, norm: 4.042343, elapsed: 275.287274, lrn_rate: 1.000000\n",
      "Validation time: 0.355261 seconds\n",
      "Validation set metric: 24.352474\n",
      "Saving model to saved_models/attn_grid_2.epoch_0.ckpt.tar\n",
      "Epoch 1, loss: 84.108170, norm: 5.042083, elapsed: 554.569927, lrn_rate: 1.000000\n",
      "Validation time: 0.363921 seconds\n",
      "Validation set metric: 15.921558\n",
      "Saving model to saved_models/attn_grid_2.epoch_1.ckpt.tar\n",
      "Epoch 2, loss: 60.803658, norm: 4.668750, elapsed: 834.639272, lrn_rate: 1.000000\n",
      "Validation time: 0.388792 seconds\n",
      "Validation set metric: 10.366450\n",
      "Saving model to saved_models/attn_grid_2.epoch_2.ckpt.tar\n",
      "Epoch 3, loss: 56.618866, norm: 5.502721, elapsed: 1116.324159, lrn_rate: 1.000000\n",
      "Validation time: 0.402103 seconds\n",
      "Validation set metric: 8.323915\n",
      "Saving model to saved_models/attn_grid_2.epoch_3.ckpt.tar\n",
      "Epoch 4, loss: 56.617249, norm: 4.964903, elapsed: 1397.799498, lrn_rate: 1.000000\n",
      "Validation time: 0.398780 seconds\n",
      "Validation set metric: 7.570488\n",
      "Saving model to saved_models/attn_grid_2.epoch_4.ckpt.tar\n",
      "Epoch 5, loss: 46.629814, norm: 5.708736, elapsed: 1681.887902, lrn_rate: 1.000000\n",
      "Validation time: 0.381419 seconds\n",
      "Validation set metric: 6.961106\n",
      "Saving model to saved_models/attn_grid_2.epoch_5.ckpt.tar\n",
      "Epoch 6, loss: 54.142597, norm: 6.370638, elapsed: 1965.840851, lrn_rate: 1.000000\n",
      "Validation time: 0.392831 seconds\n",
      "Validation set metric: 6.730813\n",
      "Saving model to saved_models/attn_grid_2.epoch_6.ckpt.tar\n",
      "Epoch 7, loss: 51.517876, norm: 5.396397, elapsed: 2247.925877, lrn_rate: 1.000000\n",
      "Validation time: 0.362355 seconds\n",
      "Validation set metric: 6.421638\n",
      "Saving model to saved_models/attn_grid_2.epoch_7.ckpt.tar\n",
      "Decaying LR to 0.500000\n",
      "Epoch 8, loss: 43.787796, norm: 4.886232, elapsed: 2530.831523, lrn_rate: 0.500000\n",
      "Validation time: 0.370876 seconds\n",
      "Validation set metric: 5.952314\n",
      "Saving model to saved_models/attn_grid_2.epoch_8.ckpt.tar\n",
      "Decaying LR to 0.250000\n",
      "Epoch 9, loss: 40.818443, norm: 5.034775, elapsed: 2815.202959, lrn_rate: 0.250000\n",
      "Validation time: 0.371427 seconds\n",
      "Validation set metric: 5.723113\n",
      "Saving model to saved_models/attn_grid_2.epoch_9.ckpt.tar\n",
      "FINAL VAL PERF 5.72311309192254\n",
      "200 False 8 0.4\n",
      "Target padding token: 1\n",
      "Using CUDA...\n",
      "Target padding token: 1\n",
      "Using CUDA...\n",
      "Innitializing parameters status:  True\n",
      "Epoch 0, loss: 83.809555, norm: 4.057537, elapsed: 273.457965, lrn_rate: 1.000000\n",
      "Validation time: 0.365764 seconds\n",
      "Validation set metric: 23.071917\n",
      "Saving model to saved_models/attn_grid_3.epoch_0.ckpt.tar\n",
      "Epoch 1, loss: 86.704704, norm: 4.726288, elapsed: 547.393016, lrn_rate: 1.000000\n",
      "Validation time: 0.395032 seconds\n",
      "Validation set metric: 17.307456\n",
      "Saving model to saved_models/attn_grid_3.epoch_1.ckpt.tar\n",
      "Epoch 2, loss: 66.911827, norm: 4.402825, elapsed: 822.156838, lrn_rate: 1.000000\n",
      "Validation time: 0.356835 seconds\n",
      "Validation set metric: 13.374448\n",
      "Saving model to saved_models/attn_grid_3.epoch_2.ckpt.tar\n",
      "Epoch 3, loss: 62.270794, norm: 5.694883, elapsed: 1098.873116, lrn_rate: 1.000000\n",
      "Validation time: 0.353263 seconds\n",
      "Validation set metric: 9.893477\n",
      "Saving model to saved_models/attn_grid_3.epoch_3.ckpt.tar\n",
      "Epoch 4, loss: 62.905273, norm: 5.290662, elapsed: 1378.331791, lrn_rate: 1.000000\n",
      "Validation time: 0.373175 seconds\n",
      "Validation set metric: 8.682508\n",
      "Saving model to saved_models/attn_grid_3.epoch_4.ckpt.tar\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, loss: 52.777771, norm: 5.691047, elapsed: 1657.517624, lrn_rate: 1.000000\n",
      "Validation time: 0.355772 seconds\n",
      "Validation set metric: 7.938821\n",
      "Saving model to saved_models/attn_grid_3.epoch_5.ckpt.tar\n",
      "Epoch 6, loss: 61.794411, norm: 6.426108, elapsed: 1936.887601, lrn_rate: 1.000000\n",
      "Validation time: 0.348664 seconds\n",
      "Validation set metric: 7.579755\n",
      "Saving model to saved_models/attn_grid_3.epoch_6.ckpt.tar\n",
      "Epoch 7, loss: 58.316906, norm: 5.636143, elapsed: 2217.693143, lrn_rate: 1.000000\n",
      "Validation time: 0.371190 seconds\n",
      "Validation set metric: 7.299893\n",
      "Saving model to saved_models/attn_grid_3.epoch_7.ckpt.tar\n",
      "Decaying LR to 0.500000\n",
      "Epoch 8, loss: 51.196472, norm: 5.013029, elapsed: 2497.512262, lrn_rate: 0.500000\n",
      "Validation time: 0.349605 seconds\n",
      "Validation set metric: 6.763396\n",
      "Saving model to saved_models/attn_grid_3.epoch_8.ckpt.tar\n",
      "Decaying LR to 0.250000\n",
      "Epoch 9, loss: 47.804970, norm: 5.102167, elapsed: 2778.161504, lrn_rate: 0.250000\n",
      "Validation time: 0.367842 seconds\n",
      "Validation set metric: 6.543959\n",
      "Saving model to saved_models/attn_grid_3.epoch_9.ckpt.tar\n",
      "FINAL VAL PERF 6.543959344008501\n",
      "200 True 4 0.2\n",
      "Target padding token: 1\n",
      "Using CUDA...\n",
      "Target padding token: 1\n",
      "Using CUDA...\n",
      "Innitializing parameters status:  True\n",
      "Epoch 0, loss: 77.921799, norm: 4.201652, elapsed: 273.800331, lrn_rate: 1.000000\n",
      "Validation time: 0.404151 seconds\n",
      "Validation set metric: 18.321237\n",
      "Saving model to saved_models/attn_grid_4.epoch_0.ckpt.tar\n",
      "Epoch 1, loss: 80.061142, norm: 4.923499, elapsed: 550.228777, lrn_rate: 1.000000\n",
      "Validation time: 0.391620 seconds\n",
      "Validation set metric: 13.918483\n",
      "Saving model to saved_models/attn_grid_4.epoch_1.ckpt.tar\n",
      "Epoch 2, loss: 59.586132, norm: 4.608985, elapsed: 827.197041, lrn_rate: 1.000000\n",
      "Validation time: 0.353776 seconds\n",
      "Validation set metric: 9.644605\n",
      "Saving model to saved_models/attn_grid_4.epoch_2.ckpt.tar\n",
      "Epoch 3, loss: 55.045631, norm: 5.908368, elapsed: 1105.585392, lrn_rate: 1.000000\n",
      "Validation time: 0.361324 seconds\n",
      "Validation set metric: 7.915523\n",
      "Saving model to saved_models/attn_grid_4.epoch_3.ckpt.tar\n",
      "Decaying LR to 0.500000\n",
      "Epoch 4, loss: 53.947441, norm: 5.361520, elapsed: 1385.202334, lrn_rate: 0.500000\n",
      "Validation time: 0.418510 seconds\n",
      "Validation set metric: 6.787118\n",
      "Saving model to saved_models/attn_grid_4.epoch_4.ckpt.tar\n",
      "Decaying LR to 0.250000\n",
      "Epoch 5, loss: 43.542664, norm: 6.245208, elapsed: 1665.456568, lrn_rate: 0.250000\n",
      "Validation time: 0.356272 seconds\n",
      "Validation set metric: 6.295384\n",
      "Saving model to saved_models/attn_grid_4.epoch_5.ckpt.tar\n",
      "Decaying LR to 0.125000\n",
      "Epoch 6, loss: 51.070637, norm: 6.960669, elapsed: 1946.965244, lrn_rate: 0.125000\n",
      "Validation time: 0.351350 seconds\n",
      "Validation set metric: 6.081556\n",
      "Saving model to saved_models/attn_grid_4.epoch_6.ckpt.tar\n",
      "Decaying LR to 0.062500\n",
      "Epoch 7, loss: 48.500435, norm: 5.910114, elapsed: 2226.234590, lrn_rate: 0.062500\n",
      "Validation time: 0.360739 seconds\n",
      "Validation set metric: 6.005617\n",
      "Saving model to saved_models/attn_grid_4.epoch_7.ckpt.tar\n",
      "Decaying LR to 0.031250\n",
      "Epoch 8, loss: 44.064594, norm: 5.544730, elapsed: 2507.387086, lrn_rate: 0.031250\n",
      "Validation time: 0.355819 seconds\n",
      "Validation set metric: 5.950150\n",
      "Saving model to saved_models/attn_grid_4.epoch_8.ckpt.tar\n",
      "Decaying LR to 0.015625\n",
      "Epoch 9, loss: 43.022419, norm: 5.858527, elapsed: 2789.384343, lrn_rate: 0.015625\n",
      "Validation time: 0.392147 seconds\n",
      "Validation set metric: 5.929888\n",
      "Saving model to saved_models/attn_grid_4.epoch_9.ckpt.tar\n",
      "FINAL VAL PERF 5.929888163985206\n",
      "200 True 4 0.4\n",
      "Target padding token: 1\n",
      "Using CUDA...\n",
      "Target padding token: 1\n",
      "Using CUDA...\n",
      "Innitializing parameters status:  True\n",
      "Epoch 0, loss: 80.919846, norm: 4.582043, elapsed: 269.123326, lrn_rate: 1.000000\n",
      "Validation time: 0.385350 seconds\n",
      "Validation set metric: 19.629608\n",
      "Saving model to saved_models/attn_grid_5.epoch_0.ckpt.tar\n",
      "Epoch 1, loss: 83.846222, norm: 5.241071, elapsed: 542.191104, lrn_rate: 1.000000\n",
      "Validation time: 0.353693 seconds\n",
      "Validation set metric: 14.764272\n",
      "Saving model to saved_models/attn_grid_5.epoch_1.ckpt.tar\n",
      "Epoch 2, loss: 65.174332, norm: 4.778107, elapsed: 814.743778, lrn_rate: 1.000000\n",
      "Validation time: 0.353189 seconds\n",
      "Validation set metric: 11.886541\n",
      "Saving model to saved_models/attn_grid_5.epoch_2.ckpt.tar\n",
      "Epoch 3, loss: 60.981152, norm: 5.968712, elapsed: 1090.004043, lrn_rate: 1.000000\n",
      "Validation time: 0.353783 seconds\n",
      "Validation set metric: 9.335173\n",
      "Saving model to saved_models/attn_grid_5.epoch_3.ckpt.tar\n",
      "Decaying LR to 0.500000\n",
      "Epoch 4, loss: 61.191242, norm: 5.375038, elapsed: 1365.859091, lrn_rate: 0.500000\n",
      "Validation time: 0.386384 seconds\n",
      "Validation set metric: 7.989257\n",
      "Saving model to saved_models/attn_grid_5.epoch_4.ckpt.tar\n",
      "Decaying LR to 0.250000\n",
      "Epoch 5, loss: 50.051792, norm: 6.178527, elapsed: 1642.623610, lrn_rate: 0.250000\n",
      "Validation time: 0.354101 seconds\n",
      "Validation set metric: 7.398204\n",
      "Saving model to saved_models/attn_grid_5.epoch_5.ckpt.tar\n",
      "Decaying LR to 0.125000\n",
      "Epoch 6, loss: 59.307312, norm: 7.137314, elapsed: 1919.201460, lrn_rate: 0.125000\n",
      "Validation time: 0.354267 seconds\n",
      "Validation set metric: 7.162647\n",
      "Saving model to saved_models/attn_grid_5.epoch_6.ckpt.tar\n",
      "Decaying LR to 0.062500\n",
      "Epoch 7, loss: 56.147991, norm: 5.810665, elapsed: 2194.852220, lrn_rate: 0.062500\n",
      "Validation time: 0.388363 seconds\n",
      "Validation set metric: 7.045846\n",
      "Saving model to saved_models/attn_grid_5.epoch_7.ckpt.tar\n",
      "Decaying LR to 0.031250\n",
      "Epoch 8, loss: 51.828869, norm: 5.715759, elapsed: 2471.200237, lrn_rate: 0.031250\n",
      "Validation time: 0.400573 seconds\n",
      "Validation set metric: 6.985106\n",
      "Saving model to saved_models/attn_grid_5.epoch_8.ckpt.tar\n",
      "Decaying LR to 0.015625\n",
      "Epoch 9, loss: 50.278885, norm: 5.570264, elapsed: 2748.088653, lrn_rate: 0.015625\n",
      "Validation time: 0.391616 seconds\n",
      "Validation set metric: 6.955871\n",
      "Saving model to saved_models/attn_grid_5.epoch_9.ckpt.tar\n",
      "FINAL VAL PERF 6.955870959106877\n",
      "200 True 8 0.2\n",
      "Target padding token: 1\n",
      "Using CUDA...\n",
      "Target padding token: 1\n",
      "Using CUDA...\n",
      "Innitializing parameters status:  True\n",
      "Epoch 0, loss: 78.055939, norm: 4.399601, elapsed: 272.657089, lrn_rate: 1.000000\n",
      "Validation time: 0.354965 seconds\n",
      "Validation set metric: 18.165227\n",
      "Saving model to saved_models/attn_grid_6.epoch_0.ckpt.tar\n",
      "Epoch 1, loss: 78.824440, norm: 5.276427, elapsed: 550.013885, lrn_rate: 1.000000\n",
      "Validation time: 0.367056 seconds\n",
      "Validation set metric: 13.387901\n",
      "Saving model to saved_models/attn_grid_6.epoch_1.ckpt.tar\n",
      "Epoch 2, loss: 57.863720, norm: 4.895478, elapsed: 828.730086, lrn_rate: 1.000000\n",
      "Validation time: 0.352305 seconds\n",
      "Validation set metric: 8.991170\n",
      "Saving model to saved_models/attn_grid_6.epoch_2.ckpt.tar\n",
      "Epoch 3, loss: 54.028553, norm: 5.656609, elapsed: 1109.136795, lrn_rate: 1.000000\n",
      "Validation time: 0.361284 seconds\n",
      "Validation set metric: 7.579922\n",
      "Saving model to saved_models/attn_grid_6.epoch_3.ckpt.tar\n",
      "Epoch 4, loss: 55.050648, norm: 5.349475, elapsed: 1390.106972, lrn_rate: 1.000000\n",
      "Validation time: 0.353842 seconds\n",
      "Validation set metric: 6.919784\n",
      "Saving model to saved_models/attn_grid_6.epoch_4.ckpt.tar\n",
      "Epoch 5, loss: 45.343040, norm: 6.299519, elapsed: 1670.594209, lrn_rate: 1.000000\n",
      "Validation time: 0.356303 seconds\n",
      "Validation set metric: 6.427721\n",
      "Saving model to saved_models/attn_grid_6.epoch_5.ckpt.tar\n",
      "Epoch 6, loss: 52.707630, norm: 6.814468, elapsed: 1951.234881, lrn_rate: 1.000000\n",
      "Validation time: 0.348605 seconds\n",
      "Validation set metric: 6.243900\n",
      "Saving model to saved_models/attn_grid_6.epoch_6.ckpt.tar\n",
      "Epoch 7, loss: 50.012188, norm: 5.943955, elapsed: 2231.551357, lrn_rate: 1.000000\n",
      "Validation time: 0.418138 seconds\n",
      "Validation set metric: 5.947431\n",
      "Saving model to saved_models/attn_grid_6.epoch_7.ckpt.tar\n",
      "Decaying LR to 0.500000\n",
      "Epoch 8, loss: 43.016766, norm: 5.050098, elapsed: 2512.055833, lrn_rate: 0.500000\n",
      "Validation time: 0.363793 seconds\n",
      "Validation set metric: 5.485799\n",
      "Saving model to saved_models/attn_grid_6.epoch_8.ckpt.tar\n",
      "Decaying LR to 0.250000\n",
      "Epoch 9, loss: 40.185081, norm: 5.137766, elapsed: 2792.097615, lrn_rate: 0.250000\n",
      "Validation time: 0.365701 seconds\n",
      "Validation set metric: 5.315936\n",
      "Saving model to saved_models/attn_grid_6.epoch_9.ckpt.tar\n",
      "FINAL VAL PERF 5.315935501788608\n",
      "200 True 8 0.4\n",
      "Target padding token: 1\n",
      "Using CUDA...\n",
      "Target padding token: 1\n",
      "Using CUDA...\n",
      "Innitializing parameters status:  True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, loss: 80.696709, norm: 4.487724, elapsed: 268.828376, lrn_rate: 1.000000\n",
      "Validation time: 0.356540 seconds\n",
      "Validation set metric: 19.476453\n",
      "Saving model to saved_models/attn_grid_7.epoch_0.ckpt.tar\n",
      "Epoch 1, loss: 84.230835, norm: 5.347474, elapsed: 540.498028, lrn_rate: 1.000000\n",
      "Validation time: 0.353929 seconds\n",
      "Validation set metric: 14.954864\n",
      "Saving model to saved_models/attn_grid_7.epoch_1.ckpt.tar\n",
      "Epoch 2, loss: 64.211594, norm: 4.860816, elapsed: 813.037319, lrn_rate: 1.000000\n",
      "Validation time: 0.354883 seconds\n",
      "Validation set metric: 11.287956\n",
      "Saving model to saved_models/attn_grid_7.epoch_2.ckpt.tar\n",
      "Epoch 3, loss: 60.588867, norm: 6.156037, elapsed: 1087.799996, lrn_rate: 1.000000\n",
      "Validation time: 0.348687 seconds\n",
      "Validation set metric: 9.001583\n",
      "Saving model to saved_models/attn_grid_7.epoch_3.ckpt.tar\n",
      "Epoch 4, loss: 62.230396, norm: 5.461389, elapsed: 1363.474364, lrn_rate: 1.000000\n",
      "Validation time: 0.371995 seconds\n",
      "Validation set metric: 8.228699\n",
      "Saving model to saved_models/attn_grid_7.epoch_4.ckpt.tar\n",
      "Epoch 5, loss: 51.283405, norm: 6.145927, elapsed: 1638.798782, lrn_rate: 1.000000\n",
      "Validation time: 0.358811 seconds\n",
      "Validation set metric: 7.638402\n",
      "Saving model to saved_models/attn_grid_7.epoch_5.ckpt.tar\n",
      "Epoch 6, loss: 61.524147, norm: 7.029633, elapsed: 1915.414260, lrn_rate: 1.000000\n",
      "Validation time: 0.361229 seconds\n",
      "Validation set metric: 7.317560\n",
      "Saving model to saved_models/attn_grid_7.epoch_6.ckpt.tar\n",
      "Epoch 7, loss: 57.577705, norm: 5.748381, elapsed: 2190.865862, lrn_rate: 1.000000\n",
      "Validation time: 0.365750 seconds\n",
      "Validation set metric: 7.090227\n",
      "Saving model to saved_models/attn_grid_7.epoch_7.ckpt.tar\n",
      "Decaying LR to 0.500000\n",
      "Epoch 8, loss: 51.088554, norm: 6.523343, elapsed: 2467.822230, lrn_rate: 0.500000\n",
      "Validation time: 0.375150 seconds\n",
      "Validation set metric: 6.569260\n",
      "Saving model to saved_models/attn_grid_7.epoch_8.ckpt.tar\n",
      "Decaying LR to 0.250000\n",
      "Epoch 9, loss: 48.479004, norm: 5.179575, elapsed: 2744.953962, lrn_rate: 0.250000\n",
      "Validation time: 0.355020 seconds\n",
      "Validation set metric: 6.369815\n",
      "Saving model to saved_models/attn_grid_7.epoch_9.ckpt.tar\n",
      "FINAL VAL PERF 6.369814560538489\n",
      "400 False 4 0.2\n",
      "Target padding token: 1\n",
      "Using CUDA...\n",
      "Target padding token: 1\n",
      "Using CUDA...\n",
      "Innitializing parameters status:  True\n",
      "Epoch 0, loss: 76.013931, norm: 4.984907, elapsed: 441.719422, lrn_rate: 1.000000\n",
      "Validation time: 0.719462 seconds\n",
      "Validation set metric: 16.896606\n",
      "Saving model to saved_models/attn_grid_8.epoch_0.ckpt.tar\n",
      "Epoch 1, loss: 69.334068, norm: 6.838554, elapsed: 887.813493, lrn_rate: 1.000000\n",
      "Validation time: 0.741214 seconds\n",
      "Validation set metric: 9.312835\n",
      "Saving model to saved_models/attn_grid_8.epoch_1.ckpt.tar\n",
      "Epoch 2, loss: 52.829327, norm: 5.360114, elapsed: 1334.857489, lrn_rate: 1.000000\n",
      "Validation time: 0.843638 seconds\n",
      "Validation set metric: 7.313180\n",
      "Saving model to saved_models/attn_grid_8.epoch_2.ckpt.tar\n",
      "Epoch 3, loss: 49.375183, norm: 6.728409, elapsed: 1782.912957, lrn_rate: 1.000000\n",
      "Validation time: 0.675194 seconds\n",
      "Validation set metric: 6.458962\n",
      "Saving model to saved_models/attn_grid_8.epoch_3.ckpt.tar\n",
      "Decaying LR to 0.500000\n",
      "Epoch 4, loss: 47.607494, norm: 6.027325, elapsed: 2230.707132, lrn_rate: 0.500000\n",
      "Validation time: 0.793595 seconds\n",
      "Validation set metric: 5.666680\n",
      "Saving model to saved_models/attn_grid_8.epoch_4.ckpt.tar\n",
      "Decaying LR to 0.250000\n",
      "Epoch 5, loss: 37.086143, norm: 7.075492, elapsed: 2693.598260, lrn_rate: 0.250000\n",
      "Validation time: 0.874730 seconds\n",
      "Validation set metric: 5.397645\n",
      "Saving model to saved_models/attn_grid_8.epoch_5.ckpt.tar\n",
      "Decaying LR to 0.125000\n",
      "Epoch 6, loss: 43.877644, norm: 8.391942, elapsed: 3158.934554, lrn_rate: 0.125000\n",
      "Validation time: 0.780769 seconds\n",
      "Validation set metric: 5.279377\n",
      "Saving model to saved_models/attn_grid_8.epoch_6.ckpt.tar\n",
      "Decaying LR to 0.062500\n",
      "Epoch 7, loss: 41.404530, norm: 6.978399, elapsed: 3624.027682, lrn_rate: 0.062500\n",
      "Validation time: 0.796287 seconds\n",
      "Validation set metric: 5.264551\n",
      "Saving model to saved_models/attn_grid_8.epoch_7.ckpt.tar\n",
      "Decaying LR to 0.031250\n",
      "Epoch 8, loss: 36.868183, norm: 6.648023, elapsed: 4090.833524, lrn_rate: 0.031250\n",
      "Validation time: 0.626812 seconds\n",
      "Validation set metric: 5.249808\n",
      "Saving model to saved_models/attn_grid_8.epoch_8.ckpt.tar\n",
      "Decaying LR to 0.015625\n",
      "Epoch 9, loss: 35.882256, norm: 6.842906, elapsed: 4561.072343, lrn_rate: 0.015625\n",
      "Validation time: 0.765557 seconds\n",
      "Validation set metric: 5.237747\n",
      "Saving model to saved_models/attn_grid_8.epoch_9.ckpt.tar\n",
      "FINAL VAL PERF 5.2377473354136415\n",
      "400 False 4 0.4\n",
      "Target padding token: 1\n",
      "Using CUDA...\n",
      "Target padding token: 1\n",
      "Using CUDA...\n",
      "Innitializing parameters status:  True\n",
      "Epoch 0, loss: 76.563705, norm: 5.911415, elapsed: 455.875975, lrn_rate: 1.000000\n",
      "Validation time: 0.693812 seconds\n",
      "Validation set metric: 15.977037\n",
      "Saving model to saved_models/attn_grid_9.epoch_0.ckpt.tar\n",
      "Epoch 1, loss: 70.776237, norm: 7.351193, elapsed: 920.386000, lrn_rate: 1.000000\n",
      "Validation time: 0.736591 seconds\n",
      "Validation set metric: 9.430819\n",
      "Saving model to saved_models/attn_grid_9.epoch_1.ckpt.tar\n",
      "Epoch 2, loss: 55.107819, norm: 5.958332, elapsed: 1385.179906, lrn_rate: 1.000000\n",
      "Validation time: 0.738710 seconds\n",
      "Validation set metric: 7.601131\n",
      "Saving model to saved_models/attn_grid_9.epoch_2.ckpt.tar\n",
      "Epoch 3, loss: 52.549431, norm: 7.006198, elapsed: 1849.190932, lrn_rate: 1.000000\n",
      "Validation time: 0.835269 seconds\n",
      "Validation set metric: 6.708083\n",
      "Saving model to saved_models/attn_grid_9.epoch_3.ckpt.tar\n",
      "Decaying LR to 0.500000\n",
      "Epoch 4, loss: 51.415947, norm: 6.396795, elapsed: 2315.782051, lrn_rate: 0.500000\n",
      "Validation time: 0.918275 seconds\n",
      "Validation set metric: 5.962437\n",
      "Saving model to saved_models/attn_grid_9.epoch_4.ckpt.tar\n",
      "Decaying LR to 0.250000\n",
      "Epoch 5, loss: 41.581474, norm: 7.315292, elapsed: 2791.153168, lrn_rate: 0.250000\n",
      "Validation time: 0.727789 seconds\n",
      "Validation set metric: 5.626896\n",
      "Saving model to saved_models/attn_grid_9.epoch_5.ckpt.tar\n",
      "Decaying LR to 0.125000\n",
      "Epoch 6, loss: 48.155838, norm: 8.468993, elapsed: 3260.779359, lrn_rate: 0.125000\n",
      "Validation time: 0.670628 seconds\n",
      "Validation set metric: 5.468408\n",
      "Saving model to saved_models/attn_grid_9.epoch_6.ckpt.tar\n",
      "Decaying LR to 0.062500\n",
      "Epoch 7, loss: 46.264179, norm: 6.928561, elapsed: 3728.343514, lrn_rate: 0.062500\n",
      "Validation time: 0.709721 seconds\n",
      "Validation set metric: 5.421141\n",
      "Saving model to saved_models/attn_grid_9.epoch_7.ckpt.tar\n",
      "Decaying LR to 0.031250\n",
      "Epoch 8, loss: 42.016472, norm: 6.633673, elapsed: 4197.836678, lrn_rate: 0.031250\n",
      "Validation time: 0.742999 seconds\n",
      "Validation set metric: 5.398870\n",
      "Saving model to saved_models/attn_grid_9.epoch_8.ckpt.tar\n",
      "Decaying LR to 0.015625\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "for hs, tw, force, drop in it.product([200,400,600],[False, True],\n",
    "                                      [4,8],[0.2, 0.4]):\n",
    "    print(hs, tw, force, drop)\n",
    "    train_iter, val_iter = data.BucketIterator.splits((train, val), batch_size=32, device=-1,\n",
    "                                                      repeat=False, sort_key=lambda x: len(x.src))\n",
    "    bs_encoder = BaseEncoder(DE, hidden_size=hs, num_layers=4, word_features=hs, \n",
    "                             dropout=drop)\n",
    "    # TODO: decide whether to add dropout to output states of encoder!\n",
    "    at_decoder = AttnDecoder(EN, hidden_size=hs, num_layers=4, word_features=hs, dropout=drop,\n",
    "                             tie_weights=tw)\n",
    "    trainer = NMTTrainer([bs_encoder, at_decoder], DE, EN, lrn_rate=1.0, \n",
    "                         lrn_decay='adaptive', attention=True,\n",
    "                         clip_norm=5, lrn_decay_force=force)\n",
    "    evaluator = NMTEvaluator([bs_encoder, at_decoder], DE, EN, attention=True,\n",
    "                            record_attention=False)\n",
    "    trainer.train(train_iter, verbose=True, le=evaluator, val_iter=val_iter,\n",
    "                  save_model_fn='attn_grid_%d' % cnt, num_iter=10)\n",
    "    cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target padding token: 1\n",
      "Using CUDA...\n",
      "Validation time: 0.677992 seconds\n",
      "5.322171705651731\n",
      "Target padding token: 1\n",
      "Using CUDA...\n",
      "Innitializing parameters status:  False\n",
      "Epoch 0, loss: 39.508476, norm: 6.285943, elapsed: 426.947739, lrn_rate: 0.500000\n",
      "Validation time: 0.728775 seconds\n",
      "Validation set metric: 5.209565\n",
      "Saving model to saved_models/attn_med_1b.epoch_0.ckpt.tar\n",
      "Epoch 1, loss: 44.648376, norm: 7.371956, elapsed: 859.301896, lrn_rate: 0.500000\n",
      "Validation time: 0.614820 seconds\n",
      "Validation set metric: 5.190459\n",
      "Saving model to saved_models/attn_med_1b.epoch_1.ckpt.tar\n",
      "Decaying LR to 0.250000\n",
      "Epoch 2, loss: 38.062698, norm: 7.248244, elapsed: 1291.240771, lrn_rate: 0.250000\n",
      "Validation time: 0.749838 seconds\n",
      "Validation set metric: 5.085312\n",
      "Saving model to saved_models/attn_med_1b.epoch_2.ckpt.tar\n",
      "Decaying LR to 0.125000\n",
      "Epoch 3, loss: 32.695580, norm: 6.297701, elapsed: 1722.055861, lrn_rate: 0.125000\n",
      "Validation time: 0.788171 seconds\n",
      "Validation set metric: 5.076100\n",
      "Saving model to saved_models/attn_med_1b.epoch_3.ckpt.tar\n",
      "Decaying LR to 0.062500\n",
      "Epoch 4, loss: 35.089790, norm: 6.534322, elapsed: 2152.567385, lrn_rate: 0.062500\n",
      "Validation time: 0.662246 seconds\n",
      "Validation set metric: 5.091741\n",
      "Saving model to saved_models/attn_med_1b.epoch_4.ckpt.tar\n",
      "Decaying LR to 0.031250\n",
      "Epoch 5, loss: 31.575603, norm: 6.593545, elapsed: 2585.185066, lrn_rate: 0.031250\n",
      "Validation time: 0.663013 seconds\n",
      "Validation set metric: 5.105446\n",
      "Saving model to saved_models/attn_med_1b.epoch_5.ckpt.tar\n",
      "Decaying LR to 0.015625\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-bdd8cc10b2ad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m                      clip_norm=5, lrn_decay_force=2)\n\u001b[1;32m     15\u001b[0m trainer.train(train_iter, verbose=True, le=evaluator, val_iter=val_iter,\n\u001b[0;32m---> 16\u001b[0;31m               save_model_fn='attn_med_1b', init_parameters=False)\n\u001b[0m",
      "\u001b[0;32m~/cs287/hw3/helpers.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, torch_train_iter, le, val_iter, save_model_fn, init_parameters, **kwargs)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_iter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 494\u001b[0;31m                 \u001b[0mres_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'skip_iter'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cs287/hw3/helpers.py\u001b[0m in \u001b[0;36mtrain_batch\u001b[0;34m(self, batch, **kwargs)\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[0;31m# norms must be clipped after backward but before step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m         \u001b[0mnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_norms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m         \u001b[0mloss_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loss_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cs287/hw3/helpers.py\u001b[0m in \u001b[0;36mclip_norms\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;31m# Norm clipping: returns a float\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m             norm = nn.utils.clip_grad_norm(\n\u001b[0;32m--> 434\u001b[0;31m                 parameters, self.clip_norm)\n\u001b[0m\u001b[1;32m    435\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m             \u001b[0mnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/utils/clip_grad.py\u001b[0m in \u001b[0;36mclip_grad_norm\u001b[0;34m(parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mparam_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0mtotal_norm\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mparam_norm\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mtotal_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_norm\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load a model and continue training:\n",
    "ld_enc, ld_dec = load_checkpoint('saved_models/attn_med_1.epoch_8.ckpt.tar')\n",
    "ld_encoder = BaseEncoder(DE, hidden_size=400, num_layers=4, word_features=400, \n",
    "                         dropout=0.2)\n",
    "ld_decoder = AttnDecoder(EN, hidden_size=400, num_layers=4, word_features=400, \n",
    "                         dropout=0.2)\n",
    "set_parameters(ld_encoder, ld_enc)\n",
    "set_parameters(ld_decoder, ld_dec)\n",
    "evaluator = NMTEvaluator([ld_encoder, ld_decoder], DE, EN, attention=True,\n",
    "                        record_attention=False)\n",
    "print(evaluator.evaluate(val_iter))\n",
    "trainer = NMTTrainer([ld_encoder, ld_decoder], DE, EN, lrn_rate=0.5, \n",
    "                     lrn_decay='adaptive', attention=True,\n",
    "                     clip_norm=5, lrn_decay_force=2)\n",
    "trainer.train(train_iter, verbose=True, le=evaluator, val_iter=val_iter,\n",
    "              save_model_fn='attn_med_1b', init_parameters=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation time: 0.659822 seconds\n",
      "5.120132318658038\n",
      "Validation time: 0.872873 seconds\n",
      "6.3874894806368365\n"
     ]
    }
   ],
   "source": [
    "print(evaluator.evaluate(val_iter))\n",
    "print(evaluator.evaluate(test_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation time: 0.456792 seconds\n",
      "6.048956472651303\n",
      "3722\n",
      "Variable containing:\n",
      " 0.6505  0.1110  0.0659  0.1725\n",
      " 0.5297  0.1554  0.2806  0.0343\n",
      " 0.0089  0.4239  0.4734  0.0938\n",
      " 0.0180  0.0371  0.9122  0.0327\n",
      " 0.0822  0.0702  0.3141  0.5335\n",
      " 0.4222  0.2591  0.0997  0.2190\n",
      " 0.1612  0.3041  0.4915  0.0432\n",
      " 0.2962  0.4637  0.0781  0.1620\n",
      "[torch.cuda.FloatTensor of size 8x4 (GPU 0)]\n",
      "\n",
      "Source\n",
      "Ich hatte Angst –\n",
      "Target\n",
      "<s> And I was scared . </s> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "print(evaluator.evaluate(val_iter))\n",
    "print(len(train_iter))\n",
    "print(evaluator.attns_log[0][4])\n",
    "val_iter.init_epoch()\n",
    "batch = next(iter(val_iter))\n",
    "sent_inspect(batch, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using final MLP\n",
      "Target padding token: 1\n",
      "Source padding token: 1\n",
      "Using CUDA...\n",
      "Validation time: 2.021641 seconds\n",
      "4.293765398892552\n"
     ]
    }
   ],
   "source": [
    "# save_checkpoint(bs_encoder, at_decoder, filename='saved_models/attn_big_0.epoch_10.ckpt.tar')\n",
    "def load_validate_model(fn):\n",
    "    ld_enc, ld_dec = load_checkpoint(fn)\n",
    "    ld_encoder = BaseEncoder(DE, hidden_size=650, num_layers=4, word_features=650, \n",
    "                             bidirectional=True, dropout=0.35)\n",
    "    ld_decoder = AttnDecoder(EN, hidden_size=650, num_layers=4, word_features=650, dropout=0.35,\n",
    "                             tie_weights=False, enc_linear=650,\n",
    "                             enc_bidirectional=True)\n",
    "    set_parameters(ld_encoder, ld_enc)\n",
    "    set_parameters(ld_decoder, ld_dec)\n",
    "    evaluator = NMTEvaluator([ld_encoder, ld_decoder], DE, EN, attention=True,\n",
    "                            record_attention=True)\n",
    "    print(evaluator.evaluate(val_iter))\n",
    "    return evaluator\n",
    "evaluator=load_validate_model('saved_models/attn_med_5.epoch_10.ckpt.tar')\n",
    "# print(evaluator.evaluate(test_iter)) # KEYYY\n",
    "# evaluator.predict(pred_set, fn='predictions_big_2_100.txt',beam_size=200, ignore_eos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f90387b52e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# for ep in [0,1,2,6,10]:\n",
    "#     evaluator = load_validate_model('saved_models/attn_med_5.epoch_%d.ckpt.tar' % ep)\n",
    "#     attn_list = list(evaluator.attns_log[10][i][1] for i in range(4))\n",
    "#     evaluator.visualize_attn(*attn_list)\n",
    "#     plt.savefig('attn_plots/learning_%d.pdf' % ep)\n",
    "attn_list = list(evaluator.attns_log[17][i][1] for i in range(4))\n",
    "attn_list.append('attn_plots/attn_vis_1.png')\n",
    "print(len(attn_list))\n",
    "evaluator.visualize_attn(*attn_list)\n",
    "# plt.savefig('attn_plots/attn_vis_1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_iter.init_epoch()\n",
    "debug_iter = iter(test_iter)\n",
    "for i in range(10):\n",
    "    batch = next(debug_iter)\n",
    "debug_set = [batch.src.data[:, i] for i in range(batch.src.data.size(1))]\n",
    "debug_ans = [batch.trg.data[:, i] for i in range(batch.trg.data.size(1))]\n",
    "evaluator.predict(pred_set, fn='predictions_real.txt',beam_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation time: 1.352269 seconds\n",
      "4.868429206982359\n",
      "Validation time: 1.816732 seconds\n",
      "6.052567905074193\n"
     ]
    }
   ],
   "source": [
    "print(evaluator.evaluate(val_iter))\n",
    "print(evaluator.evaluate(test_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-inf\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.Tensor([-np.inf])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (<ipython-input-20-855e1c448395>, line 354)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-20-855e1c448395>\"\u001b[0;36m, line \u001b[0;32m354\u001b[0m\n\u001b[0;31m    keep_idx)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "# Class that NMT{Trainer/Evaluator} extends\n",
    "class NMTModelUser(object):\n",
    "    # Models is a list [Encoder, Decoder]\n",
    "    def __init__(self, models, TEXT_SRC, TEXT_TRG, mask_src=False,\n",
    "                 attention=False, reverse_enc_input=False, cuda=True):\n",
    "        self._TEXT_SRC = TEXT_SRC\n",
    "        self._TEXT_TRG = TEXT_TRG\n",
    "        self.trg_pad = TEXT_TRG.vocab.stoi['<pad>']\n",
    "        self.src_pad = TEXT_SRC.vocab.stoi['<pad>']\n",
    "        print('Target padding token: %d' % self.trg_pad)\n",
    "        print('Source padding token: %d' % self.src_pad)\n",
    "        self.models = models\n",
    "        self.mask_src = mask_src\n",
    "        self.use_attention = attention\n",
    "        self.record_attention = False\n",
    "        self.reverse_enc_input = reverse_enc_input\n",
    "        self.cuda = cuda and torch.cuda.is_available()\n",
    "        if self.cuda:\n",
    "            print('Using CUDA...')\n",
    "        else:\n",
    "            print('CUDA is unavailable...')\n",
    "\n",
    "    def get_src_and_trg(self, batch):\n",
    "        if self.reverse_enc_input:\n",
    "            src_data = torch.t(batch.src.data)\n",
    "            ind_rev = torch.LongTensor(np.arange(src_data.size(1) - 1, -1, -1))\n",
    "            src = torch.index_select(torch.t(batch.src.data), dim=1,\n",
    "                                     index=ind_rev)\n",
    "            src = src.contiguous()\n",
    "        else:\n",
    "            src = torch.t(batch.src.data).contiguous()\n",
    "        trg = torch.t(batch.trg.data)\n",
    "        # Have to shift the target so we don't predict the word \n",
    "        # we see (this is ok since sentences in trg all begin \n",
    "        # with <s>)\n",
    "        trg_feat = trg[:, :-1].contiguous()\n",
    "        trg_lab = trg[:, 1:].contiguous()\n",
    "        return (src, trg_feat, trg_lab)\n",
    "\n",
    "    def zeros_hidden(self, batch_sz, model_num):\n",
    "        num_directions = 2 if self.models[model_num].bidirectional else 1\n",
    "        return torch.zeros(self.models[model_num].num_layers * num_directions, \n",
    "                           batch_sz,\n",
    "                           self.models[model_num].hidden_size)\n",
    "\n",
    "    # Ok to have self.prev_hidden apply to encoder then decoder since\n",
    "    # encoder all ends before decoder starts\n",
    "    def prepare_hidden(self, batch_sz, zero_out=True, model_num=0):\n",
    "        if (not self.prev_hidden is None) and (not zero_out):\n",
    "            pre_hidden = self.prev_hidden\n",
    "        else:\n",
    "            pre_hidden = (self.zeros_hidden(batch_sz, model_num) \\\n",
    "                          for i in range(2))\n",
    "        if self.cuda:\n",
    "            pre_hidden = tuple(t.cuda() for t in pre_hidden)\n",
    "        return tuple(autograd.Variable(t) for t in pre_hidden)\n",
    "\n",
    "    # kwargs can contain zero_out, model_num for prepare_hidden\n",
    "    def prepare_model_inputs(self, batch, **kwargs):\n",
    "        if self.cuda:\n",
    "            src, trg_feat, trg_lab = \\\n",
    "                tuple(t.cuda() for t in self.get_src_and_trg(batch))\n",
    "        else:\n",
    "            src, trg_feat, trg_lab = self.get_src_and_trg(batch)\n",
    "\n",
    "        # TODO: can comment this out (assuming it passes\n",
    "        # -- just is checking batch-sz)\n",
    "        assert batch.src.size(1) == batch.trg.size(1)\n",
    "        var_hidden = self.prepare_hidden(batch.src.size(1), **kwargs)\n",
    "\n",
    "        var_src = autograd.Variable(src)\n",
    "        var_trg_feat = autograd.Variable(trg_feat)\n",
    "        var_trg_lab = autograd.Variable(trg_lab)\n",
    "\n",
    "        return (var_src, var_trg_feat, var_trg_lab, var_hidden)\n",
    "\n",
    "    def init_epoch(self):\n",
    "        self.prev_hidden = None\n",
    "        self.debug_cnt = 0\n",
    "        \n",
    "    def debug_model_output(self, var_src, var_trg, dec_output,\n",
    "                           num_samp=10):\n",
    "        print('DEBUG CNT: %d' % self.debug_cnt)\n",
    "        print(var_src.size(), var_trg.size(), dec_output.size())\n",
    "        self.debug_cnt += 1\n",
    "        if self.debug_cnt > 10:\n",
    "            return\n",
    "        src = var_src.data\n",
    "        trg = var_trg.data\n",
    "        _, pred = torch.topk(dec_output, k=1, dim=2)\n",
    "        pred = pred.squeeze().data\n",
    "        print(pred.size()) # should be [batch_sz, sent_len]\n",
    "        for i in range(num_samp):\n",
    "            print('=== SAMPLE %d ===' % i)\n",
    "            print('-- SRC --')\n",
    "            print(' '.join(self._TEXT_SRC.vocab.itos[src[i,j]] \\\n",
    "                                           for j in range(src.size(1))))\n",
    "            print('-- REAL TRG --')\n",
    "            print(' '.join(self._TEXT_TRG.vocab.itos[trg[i,j]] \\\n",
    "                                           for j in range(trg.size(1))))\n",
    "            print('-- PRED TRG --')\n",
    "            print(' '.join(self._TEXT_TRG.vocab.itos[pred[i,j]] \\\n",
    "                                           for j in range(pred.size(1))))\n",
    "\n",
    "\n",
    "    def set_enc_prev_hidden(self, enc_hidden):\n",
    "        # Each element of hidden is [F(x_T), B(x_1)]; we want to use\n",
    "        # the first\n",
    "        if self.models[1].enc_directions == 2:\n",
    "            assert self.use_attention\n",
    "            # This is the first hidden B(x_1) of the backwards layer\n",
    "            self.prev_hidden = tuple(h[self.models[0].num_layers:,:,:] for \\\n",
    "                                     h in enc_hidden)\n",
    "        else:\n",
    "            self.prev_hidden = enc_hidden\n",
    "\n",
    "    def generate_attn_mask(self, var_src):\n",
    "        if not self.mask_src:\n",
    "            return None\n",
    "        # Using broadcasting \n",
    "        pad_mask = torch.eq(var_src, self.src_pad).type(torch.FloatTensor)\n",
    "        pad_mask = pad_mask.cuda() if self.cuda else pad_mask\n",
    "        return pad_mask\n",
    "        \n",
    "    def run_model(self, batch, mode='mean'):\n",
    "        # var_src, var_trg are [batch_sz, sent_len]\n",
    "        var_src, var_trg_feat, var_trg_lab, var_hidden = \\\n",
    "            self.prepare_model_inputs(\n",
    "            batch, zero_out=True, model_num=0)\n",
    "\n",
    "        # For attention, will use enc_output (not otherwise)\n",
    "        enc_output, enc_hidden = self.models[0](var_src, var_hidden)\n",
    "        self.set_enc_prev_hidden(enc_hidden)\n",
    "            \n",
    "        if self.use_attention:\n",
    "            pad_mask = self.generate_attn_mask(var_src)\n",
    "            dec_output, dec_hidden, dec_attn = self.models[1](\n",
    "                var_trg_feat, self.prev_hidden, enc_output, pad_mask)\n",
    "            if self.record_attention:\n",
    "                _, pred = torch.topk(dec_output, k=1, dim=2)\n",
    "                self.attns_log.append((dec_attn, var_src, pred.squeeze(),\n",
    "                                       var_trg_lab))\n",
    "        else:\n",
    "            # Using real words as input. Use prev_hidden both to\n",
    "            # initialize hidden state (the first time) and as context\n",
    "            # vector\n",
    "            dec_output, dec_hidden = self.models[1](\n",
    "                var_trg_feat, self.prev_hidden, enc_hidden)\n",
    "            \n",
    "        # TEMPORARY\n",
    "        # self.debug_model_output(var_src, var_trg_lab, dec_output)\n",
    "        self.prev_hidden = dec_hidden\n",
    "        loss = self.nll_loss(dec_output, var_trg_lab, mode=mode)\n",
    "        return loss\n",
    "\n",
    "    # Assume log_probs is [batch_sz, sent_len, V], output is\n",
    "    # [batch_sz, sent_len]\n",
    "    def nll_loss(self, log_probs, output, mode='mean', **kwargs):\n",
    "        batch_sz = log_probs.size(0)\n",
    "        sl_type = torch.cuda.FloatTensor if self.cuda else \\\n",
    "            torch.FloatTensor\n",
    "        sent_len = torch.sum((output != self.trg_pad).type(sl_type)) / batch_sz\n",
    "        # sent_len = sent_len.data[0]\n",
    "        # sent_len = log_probs.size(1)\n",
    "        # print(sent_len, log_probs.size())\n",
    "        log_probs_rshp = log_probs.view(-1, log_probs.size(2))\n",
    "        output_rshp = output.view(-1)\n",
    "        if mode == 'mean':\n",
    "            # Sum over all words in sent, mean over sentences; \n",
    "            # make sure to ignore padding\n",
    "            return F.nll_loss(log_probs_rshp, output_rshp, \n",
    "                              ignore_index=self.trg_pad, \n",
    "                              **kwargs) * sent_len\n",
    "        elif mode == 'sum':\n",
    "            # Sum over all sentences and words in them\n",
    "            return F.nll_loss(log_probs_rshp, output_rshp,\n",
    "                              ignore_index=self.trg_pad,\n",
    "                              size_average=False)\n",
    "        else:\n",
    "            raise ValueError('Invalid mode field: %s' % mode)\n",
    "            \n",
    "class NMTEvaluator(NMTModelUser):\n",
    "    def __init__(self, models, TEXT_SRC, TEXT_TRG, record_attention=False,\n",
    "                 visualize_freq=None, **kwargs):\n",
    "        super(NMTEvaluator, self).__init__(models, TEXT_SRC, TEXT_TRG,\n",
    "                                           **kwargs)\n",
    "        # Perhaps overwrite record_attention\n",
    "        self.record_attention = record_attention\n",
    "        self.visualize_freq = visualize_freq\n",
    "        \n",
    "    def init_epoch(self):\n",
    "        super(NMTEvaluator, self).init_epoch()\n",
    "        self.attns_log = list()\n",
    "        \n",
    "    def visualize_attn(self, dec_attn_smpl, var_src_smpl, pred_smpl,\n",
    "                       var_trg_lab=None, save_fn=None):\n",
    "        # dec_attn_smpl is [src_len, pred_len], var_src_smpl is [src_len],\n",
    "        # pred_smpl is [pred_len]\n",
    "        attn = dec_attn_smpl.cpu().data.numpy()\n",
    "        src_words = np.array(list(map(lambda x: self._TEXT_SRC.vocab.itos[x], \n",
    "                                      var_src_smpl.cpu().data.numpy())))\n",
    "        pred_words = np.array(list(map(lambda x: self._TEXT_TRG.vocab.itos[x], \n",
    "                                       pred_smpl.cpu().data.numpy())))\n",
    "        if not var_trg_lab is None:\n",
    "            trg_cpu = var_trg_lab.cpu().data.numpy()\n",
    "            trg_words = np.array(list(map(lambda x : self._TEXT_TRG.vocab.itos[x],\n",
    "                                         trg_cpu)))\n",
    "            pred_words = np.array(['%s (%s)' % (pred_words[i], trg_words[i]) for \\\n",
    "                                   i in range(pred_words.shape[0])])\n",
    "            pad_idx = np.where(trg_words == '<pad>')[0]\n",
    "            if len(pad_idx):\n",
    "                clip_len = pad_idx[0]\n",
    "                trg_words = trg_words[:clip_len]\n",
    "                pred_words = pred_words[:clip_len]\n",
    "                attn = attn[:clip_len, :]\n",
    "        \n",
    "        fig, ax = plt.subplots()\n",
    "        ax.imshow(attn, cmap='gray')\n",
    "        plt.xticks(range(len(src_words)),src_words, rotation='vertical')\n",
    "        plt.yticks(range(len(pred_words)),pred_words)\n",
    "        ax.xaxis.tick_top()\n",
    "        if not save_fn is None:\n",
    "            plt.savefig(save_fn)\n",
    "        plt.show()\n",
    "\n",
    "    def evaluate(self, test_iter, num_iter=None):\n",
    "        start_time = time.time()\n",
    "        for model in self.models:\n",
    "            model.eval()\n",
    "        nll_sum = 0\n",
    "        nll_cnt = 0\n",
    "\n",
    "        self.init_epoch()\n",
    "        test_iter.init_epoch()\n",
    "        for i,batch in enumerate(test_iter):\n",
    "            nll_cnt += batch.trg.data.numel()\n",
    "            loss = self.run_model(batch, mode='sum')\n",
    "            # TODO: make sure loss just has 1 element!\n",
    "            nll_sum += loss.data[0]\n",
    "            \n",
    "            if self.visualize_freq and i % self.visualize_freq == 0:\n",
    "                sample = self.attns_log[-1]\n",
    "                self.visualize_attn(sample[0][0], sample[1][0], sample[2][0])\n",
    "            if not num_iter is None and i > num_iter:\n",
    "                break\n",
    "                        \n",
    "        # Wrap the model.eval(), just in case\n",
    "        for model in self.models:\n",
    "            model.train()\n",
    "        \n",
    "        print('Validation time: %f seconds' % (time.time() - start_time))\n",
    "        return np.exp(nll_sum / nll_cnt)\n",
    "    \n",
    "    # Performs beam search\n",
    "    def run_model_predict(self, sent, ref_beam, ref_voc,\n",
    "                          beam_size=100, pred_len=3, pred_num=None,\n",
    "                          ignore_eos=False, translate_mode=False):\n",
    "        if pred_num is None:\n",
    "            pred_num = beam_size\n",
    "        \n",
    "        # [sent_len]\n",
    "        sent_tsr = torch.LongTensor(sent)\n",
    "        if self.reverse_enc_input:\n",
    "            ind_rev = torch.LongTensor(np.arange(sent_tsr.size(0) - 1, -1, -1))\n",
    "            sent_tsr = torch.index_select(sent_tsr, dim=0,\n",
    "                                          index=ind_rev)\n",
    "        if self.cuda:\n",
    "            sent_tsr = sent_tsr.cuda()\n",
    "        var_src = autograd.Variable(sent_tsr.view(1, -1).expand(beam_size, -1))\n",
    "        var_hidden = self.prepare_hidden(beam_size, zero_out=True)\n",
    "        \n",
    "        # For attention, will use enc_output (not otherwise)\n",
    "        enc_output, enc_hidden = self.models[0](var_src, var_hidden)\n",
    "        self.set_enc_prev_hidden(enc_hidden)\n",
    "        \n",
    "        # Make sure to start with SOS token\n",
    "        sos_token = self._TEXT_TRG.vocab.stoi['<s>']\n",
    "        self.cur_beams = (sos_token * torch.ones(beam_size, 1)).type(torch.LongTensor)\n",
    "        self.cur_beam_vals = torch.zeros(beam_size, 1).type(torch.FloatTensor)\n",
    "        if self.cuda:\n",
    "            self.cur_beams = self.cur_beams.cuda()\n",
    "            self.cur_beam_vals = self.cur_beam_vals.cuda()\n",
    "        self.cur_beams = autograd.Variable(self.cur_beams)\n",
    "        self.cur_beam_vals = autograd.Variable(self.cur_beam_vals)\n",
    "\n",
    "        if translate_mode:\n",
    "            final_preds = list()\n",
    "\n",
    "        for i in range(pred_len):\n",
    "            if translate_mode:\n",
    "                (ref_beam, ref_voc) = self.create_beam_arrays(self.cur_beams.size(0))\n",
    "\n",
    "            cur_sent = self.cur_beams[:, i:i+1]\n",
    "            if self.use_attention:\n",
    "                pad_mask = self.generate_attn_mask(var_src)\n",
    "\n",
    "                dec_output, dec_hidden, dec_attn = self.models[1](\n",
    "                    cur_sent, self.prev_hidden, enc_output, pad_mask)\n",
    "                if self.record_attention:\n",
    "                    _, pred = torch.topk(dec_output, k=1, dim=2)\n",
    "                    self.attns_log.append((dec_attn, var_src, pred.squeeze(),\n",
    "                                           None))\n",
    "            else:\n",
    "                # Using real words as input. Use prev_hidden both to\n",
    "                # initialize hidden state (the first time) and as context\n",
    "                # vector\n",
    "                dec_output, dec_hidden = self.models[1](\n",
    "                    cur_sent, self.prev_hidden, enc_hidden)\n",
    "            self.prev_hidden = dec_hidden\n",
    "            \n",
    "            # dec_output is [batch_sz, sent_len=1, V]\n",
    "            # print(dec_output.size())\n",
    "            # Using broadcasting:\n",
    "            dec_output = dec_output.squeeze()\n",
    "\n",
    "            # Deal with EOS tokens:\n",
    "            if ignore_eos:\n",
    "                eos_token = self._TEXT_TRG.vocab.stoi['</s>']\n",
    "                dec_output[:, eos_token] = -np.inf\n",
    "\n",
    "            dec_output = dec_output + self.cur_beam_vals\n",
    "            if i == 0:\n",
    "                # All start words were the same, so need to restrict \n",
    "                # to the first row\n",
    "                dec_output = dec_output[0, :]\n",
    "            else:\n",
    "                dec_output = dec_output.view(-1)\n",
    "                \n",
    "            topk_dec, topk_inds = torch.topk(dec_output, k=beam_size)\n",
    "            chosen_prev_inds = torch.index_select(ref_beam, dim=0, index=topk_inds)\n",
    "            chosen_prevs = torch.index_select(self.cur_beams, dim=0,\n",
    "                                              index=chosen_prev_inds)\n",
    "            # Important to update hidden to reflect which prev \n",
    "            # sents we choose\n",
    "            self.prev_hidden = tuple(torch.index_select(\n",
    "                    self.prev_hidden[j], dim=1, index=chosen_prev_inds) \\\n",
    "                                     for j in range(len(self.prev_hidden)))\n",
    "            \n",
    "            # Update self.cur_beam_vals: [beam_sz, 1] \n",
    "            # (we already added on prev cur_beam_vals above)\n",
    "            self.cur_beam_vals = topk_dec.view(-1, 1)\n",
    "            # print('cur_beam_vals', self.cur_beam_vals)\n",
    "\n",
    "            # [batch_sz=beam_sz, 1]\n",
    "            chosen_nexts = torch.index_select(ref_voc, dim=0, index=topk_inds).view(-1, 1)\n",
    "            # print('chosen_nexts', chosen_nexts)\n",
    "            self.cur_beams = torch.cat((chosen_prevs, chosen_nexts), dim=1)\n",
    "\n",
    "            # Hugely inefficient, but whatever\n",
    "            if translate_mode:\n",
    "                keep_sents = list()\n",
    "                eos_token = self._TEXT_TRG.vocab.stoi['</s>']\n",
    "                for i in self.cur_beams.size(0):\n",
    "                    if self.cur_beams[i,-1].data[0] == eos_token:\n",
    "                        final_preds.append(self.cur_beams[i,:])\n",
    "                    else:\n",
    "                        keep_sents.append(i)\n",
    "                    keep_idx = autograd.Variable(torch.LongTensor(keep_sents))\n",
    "                    keep_idx = keep_idx.cuda() if self.cuda else keep_idx\n",
    "\n",
    "                    # Now reselect\n",
    "                    self.cur_beams = torch.index_select(self.cur_beams, 0,\n",
    "                                                        keep_idx)\n",
    "                    self.cur_beam_vals = torch.index_select(self.cur_beam_vals, 0,\n",
    "                                                            keep_idx)\n",
    "                    self.prev_hidden = tuple(torch.index_select(\n",
    "                        self.prev_hidden[j], 1, keep_idx) \\\n",
    "                                             for j in range(len(self.prev_hidden)))\n",
    "                    print(self.cur_beams)\n",
    "                    print(self.cur_beam_vals)\n",
    "                    print(self.prev_hidden[0].size())\n",
    "\n",
    "                if len(keep_sents) > 100:\n",
    "                    return keep_sents\n",
    "            # print('cur_beams', self.cur_beams)\n",
    "\n",
    "        if translate_mode:\n",
    "            return keep_sents, self.cur_beams\n",
    "        return self.cur_beams\n",
    "    \n",
    "    @staticmethod\n",
    "    def escape(l):\n",
    "        return l.replace(\"\\\"\", \"<quote>\").replace(\",\", \"<comma>\")\n",
    "\n",
    "    def create_ref_arrays(self, beam_size):\n",
    "        # Create reference idx for expanding beams and vocab\n",
    "        trg_vocab_sz = len(self._TEXT_TRG.vocab)\n",
    "        ref_beam = torch.LongTensor(np.arange(beam_size)).view(-1, 1).expand(-1, trg_vocab_sz)\n",
    "        ref_beam = ref_beam.contiguous().view(-1)\n",
    "        ref_beam = ref_beam.cuda() if self.cuda else ref_beam\n",
    "        ref_beam = autograd.Variable(ref_beam)\n",
    "        \n",
    "        ref_voc = torch.LongTensor(np.arange(trg_vocab_sz)).view(1, -1).expand(beam_size, -1)\n",
    "        ref_voc = ref_voc.contiguous().view(-1)\n",
    "        ref_voc = ref_voc.cuda() if self.cuda else ref_voc\n",
    "        ref_voc = autograd.Variable(ref_voc)\n",
    "        return (ref_beam, ref_voc)\n",
    "        \n",
    "    def predict(self, test_set, fn='predictions.txt', num_cands=100, pred_len=3,\n",
    "                beam_size=100, ignore_eos=False, translate_mode=False):\n",
    "        start_time = time.time()\n",
    "        for model in self.models:\n",
    "            model.eval()\n",
    "\n",
    "        (ref_beam, ref_voc) = self.create_ref_arrays(beam_size)\n",
    "            \n",
    "        self.init_epoch()\n",
    "        predictions = list()\n",
    "        for i,sent in enumerate(test_set):\n",
    "            # [pred_num, pred_len] tensor\n",
    "            best_translations = self.run_model_predict(sent, ref_beam=ref_beam,\n",
    "                                                       ref_voc=ref_voc,\n",
    "                                                       pred_len=pred_len,\n",
    "                                                       beam_size=beam_size,\n",
    "                                                       ignore_eos=ignore_eos,\n",
    "                                                       translate_mode=translate_mode)\n",
    "            predictions.append(best_translations)\n",
    "            # if i > 10:\n",
    "            #     break\n",
    "            \n",
    "        print('Writing predictions to %s...' % fn)\n",
    "        with open(fn, 'w') as fout:\n",
    "            print('id,word', file=fout)\n",
    "            for i,preds in enumerate(predictions):\n",
    "                # We can traverse the beam in order since topk \n",
    "                # sorts its output\n",
    "                cands = list()\n",
    "                for j in range(num_cands):\n",
    "                    # Ignore SOS\n",
    "                    words = [self._TEXT_TRG.vocab.itos[preds[j,k].data[0]] for k in range(1, pred_len + 1)]\n",
    "                    sent = '|'.join(self.escape(l) for l in words)\n",
    "                    cands.append(sent)\n",
    "                print('%d,%s' % (i+1, ' '.join(cands)), file=fout)\n",
    "        print('Computing predictions took %f seconds' % (time.time() - start_time))\n",
    "        \n",
    "        # Wrap model.eavl\n",
    "        for model in self.models:\n",
    "            model.train()\n",
    "            \n",
    "\n",
    "    \n",
    "class NMTTrainer(NMTModelUser):\n",
    "    def __init__(self, models, TEXT_SRC, TEXT_TRG, lrn_rate=0.1,\n",
    "                 optimizer=optim.SGD, lrn_decay='none',\n",
    "                 lrn_decay_force=np.inf,\n",
    "                 lrn_decay_rate=0.1,\n",
    "                 clip_norm=10, **kwargs):\n",
    "        super(NMTTrainer, self).__init__(models, TEXT_SRC, TEXT_TRG, **kwargs)\n",
    "\n",
    "        self.base_lrn_rate = lrn_rate\n",
    "        self.optimizer_type = optimizer\n",
    "        self.init_optimizers()\n",
    "\n",
    "        # Do learning rate decay:\n",
    "        self.lr_decay_opt = lrn_decay\n",
    "        self.lr_decay_force = lrn_decay_force\n",
    "        if self.lr_decay_opt == 'none' or self.lr_decay_opt == 'adaptive':\n",
    "            self.lambda_lr = lambda i : 1\n",
    "        elif self.lr_decay_opt == 'invlin':\n",
    "            decay_rate = lrn_decay_rate\n",
    "            self.lambda_lr = lambda i : 1 / (1 + (i-6) * decay_rate) if i > 6 else 1\n",
    "        else:\n",
    "            raise ValueError('Invalid learning rate decay option: %s' \\\n",
    "                             % self.lr_decay_opt)\n",
    "        self.schedulers = [optim.lr_scheduler.LambdaLR(optimizer,\n",
    "            self.lambda_lr) for optimizer in self.optimizers]\n",
    "\n",
    "        self.clip_norm = clip_norm\n",
    "        self.init_lists()\n",
    "        if self.cuda:\n",
    "            for model in self.models:\n",
    "                model.cuda()\n",
    "                \n",
    "    def init_optimizers(self):\n",
    "        self.optimizers = [self.optimizer_type(filter(lambda p : p.requires_grad,\n",
    "                                                      model.parameters()),\n",
    "                                               lr = self.base_lrn_rate) for \\\n",
    "                           model in self.models]\n",
    "    def init_lists(self):\n",
    "        self.training_losses = list()\n",
    "        self.training_norms = list()\n",
    "        self.val_perfs = list()\n",
    "\n",
    "    def get_loss_data(self, loss):\n",
    "        if self.cuda:\n",
    "            return loss.data.cpu().numpy()[0]\n",
    "        else:\n",
    "            return loss.data.numpy()[0]\n",
    "\n",
    "    def make_recordings(self, loss, norm):\n",
    "        self.training_norms.append(norm)\n",
    "        self.training_losses.append(loss)\n",
    "\n",
    "    def clip_norms(self):\n",
    "        # Clip grad norm after backward but before step\n",
    "        if self.clip_norm > 0:\n",
    "            parameters = tuple()\n",
    "            for model in self.models:\n",
    "                parameters += tuple(model.parameters())\n",
    "                \n",
    "            # Norm clipping: returns a float\n",
    "            norm = nn.utils.clip_grad_norm(\n",
    "                parameters, self.clip_norm)\n",
    "        else:\n",
    "            norm = -1\n",
    "        return norm\n",
    "\n",
    "    def train_batch(self, batch, **kwargs):\n",
    "        for model in self.models:\n",
    "            model.zero_grad()\n",
    "            \n",
    "        loss = self.run_model(batch)\n",
    "        loss.backward()\n",
    "\n",
    "        # norms must be clipped after backward but before step\n",
    "        norm = self.clip_norms()\n",
    "\n",
    "        loss_data = self.get_loss_data(loss)\n",
    "        # print('TEMP: ', loss_data, norm)\n",
    "        if kwargs.get('verbose', False):\n",
    "            self.make_recordings(loss_data, norm)\n",
    "\n",
    "        for optimizer in self.optimizers:\n",
    "            optimizer.step()\n",
    "\n",
    "        # Return loss and norm (before gradient step)\n",
    "        return loss_data, norm\n",
    "\n",
    "    def init_parameters(self):\n",
    "        for model in self.models:\n",
    "            for p in model.parameters():\n",
    "                p.data.uniform_(-0.05, 0.05)\n",
    "\n",
    "    def train(self, torch_train_iter, le=None, val_iter=None,\n",
    "              save_model_fn=None, init_parameters=True, **kwargs):\n",
    "        self.init_lists()\n",
    "        start_time = time.time()\n",
    "        print(\"Innitializing parameters status: \", init_parameters)\n",
    "        if init_parameters:\n",
    "            self.init_parameters()\n",
    "\n",
    "        torch_train_iter.init_epoch()\n",
    "        for epoch in range(kwargs.get('num_iter', 100)):\n",
    "            self.init_epoch()\n",
    "            for model in self.models:\n",
    "                model.train()\n",
    "                \n",
    "            # Learning rate decay, if any\n",
    "            if self.lr_decay_opt == 'adaptive':\n",
    "                if (epoch > 2 and self.val_perfs[-1] > self.val_perfs[-2]) or \\\n",
    "                   (epoch >= self.lr_decay_force):\n",
    "                    self.base_lrn_rate = self.base_lrn_rate / 2\n",
    "                    self.init_optimizers() # Looks at self.base_lrn_rate\n",
    "                    print('Decaying LR to %f' % self.base_lrn_rate)\n",
    "            else:\n",
    "                for scheduler in self.schedulers:\n",
    "                    scheduler.step()\n",
    "\n",
    "            # TODO: LR decay\n",
    "            train_iter = iter(torch_train_iter)\n",
    "\n",
    "            for batch in train_iter:\n",
    "                res_loss, res_norm = self.train_batch(batch, **kwargs)\n",
    "\n",
    "            if epoch % kwargs.get('skip_iter', 1) == 0:\n",
    "                if not kwargs.get('verbose', False):\n",
    "                    self.make_recordings(res_loss, res_norm)\n",
    "\n",
    "            print('Epoch %d, loss: %f, norm: %f, elapsed: %f, lrn_rate: %f' \\\n",
    "                  % (epoch, np.mean(self.training_losses[-10:]),\n",
    "                     np.mean(self.training_norms[-10:]),\n",
    "                     time.time() - start_time,\n",
    "                     self.base_lrn_rate)) #  * self.lambda_lr(epoch)))\n",
    "                    \n",
    "            \n",
    "            if (not le is None) and (not val_iter is None):\n",
    "                self.val_perfs.append(le.evaluate(val_iter))\n",
    "                print('Validation set metric: %f' % \\\n",
    "                      self.val_perfs[-1])\n",
    "\n",
    "            if not save_model_fn is None:\n",
    "                pathname = 'saved_models/' + save_model_fn + \\\n",
    "                           '.epoch_%d.ckpt.tar' % epoch\n",
    "                print('Saving model to %s' % pathname)\n",
    "                save_checkpoint(self.models[0], self.models[1],\n",
    "                           pathname)\n",
    "\n",
    "        if len(self.val_perfs) >= 1:\n",
    "            print('FINAL VAL PERF', self.val_perfs[-1])\n",
    "            return self.val_perfs[-1]\n",
    "        return -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EmbeddingsLM(nn.Module):\n",
    "    def __init__(self, TEXT, dropout=0.0, max_embed_norm=None, word_features=1000):\n",
    "        super(EmbeddingsLM, self).__init__()\n",
    "        # Initialize dropout\n",
    "        self.dropout_prob = dropout\n",
    "        self.dropout = nn.Dropout(self.dropout_prob)\n",
    "        \n",
    "        # V is size of vocab, D is dim of embedding\n",
    "        self.V = len(TEXT.vocab)\n",
    "        self.D = word_features\n",
    "        self.embeddings = nn.Embedding(self.V, self.D, max_norm=max_embed_norm)\n",
    "\n",
    "class BaseEncoder(EmbeddingsLM):\n",
    "    def __init__(self, TEXT, hidden_size=1000, num_layers=4,\n",
    "                 bidirectional=False, **kwargs):\n",
    "        super(BaseEncoder, self).__init__(TEXT, **kwargs)\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional\n",
    "        self.lstm = nn.LSTM(input_size=self.D, hidden_size=self.hidden_size,\n",
    "                            num_layers=self.num_layers,\n",
    "                            dropout=self.dropout_prob, batch_first=True,\n",
    "                            bidirectional=self.bidirectional)\n",
    "\n",
    "        \n",
    "    def forward(self, input_tsr, hidden):\n",
    "        # [batch_sz, sent_len, D]:\n",
    "        embedded_tsr = self.embeddings(input_tsr)\n",
    "\n",
    "        # XXX\n",
    "        embedded_tsr = self.dropout(embedded_tsr)\n",
    "\n",
    "        # output is [batch, sent_len, hidden_size * num_directions]\n",
    "        output, hidden = self.lstm(embedded_tsr, hidden)\n",
    "\n",
    "        # TODO: this is experimental XXX: should be careful here since\n",
    "        # the weighted sum of outputs (i.e. context) is already being\n",
    "        # dropout'ed in the context part of the decoder (but not for\n",
    "        # attn right now)\n",
    "        # output = self.dropout(output)\n",
    "        \n",
    "        # TODO: perhaps add dropout to output\n",
    "        return output, hidden\n",
    "\n",
    "class BaseDecoder(BaseEncoder):\n",
    "    def __init__(self, TEXT, num_context=1, enc_bidirectional=False, **kwargs):\n",
    "        super(BaseDecoder, self).__init__(TEXT, **kwargs)\n",
    "        # V is the size of the vocab, which is what we're predicting\n",
    "        # (it's also used as input through the embedding)\n",
    "        self.num_context = num_context\n",
    "        self.enc_directions = 2 if enc_bidirectional else 1\n",
    "        # For now assume that encoder and decoder have same hidden size\n",
    "        blowup = self.num_context * self.num_layers * self.enc_directions + 1\n",
    "        self.out_linear = nn.Linear(\n",
    "            blowup * self.hidden_size, self.V)\n",
    "\n",
    "    # Context is a tuple (h_T, c_T) of hidden and cell states from\n",
    "    # last time step of encoder\n",
    "    def forward(self, input_tsr, hidden, context):\n",
    "        # [batch_sz, sent_len, D] : note that sent_len may be 1 if we\n",
    "        # feed in each word at a time!\n",
    "        embedding = self.embeddings(input_tsr)\n",
    "        embedding = F.relu(embedding)\n",
    "        output, hidden = self.lstm(embedding, hidden)\n",
    "\n",
    "        if self.num_context:\n",
    "            # We get lucky that hidden is stored as (h,c), \n",
    "            # so hidden (not cell) first\n",
    "            context_tsr = torch.cat(context[:self.num_context])\n",
    "            batch_sz = context_tsr.size(1)\n",
    "            sent_len = output.size(1)\n",
    "            # [batch_sz, 1, hidden_size * num_context]\n",
    "            context_tsr = context_tsr.permute(1,0,2).contiguous().view(batch_sz, 1, -1)\n",
    "            context_tsr = context_tsr.expand(-1, sent_len, -1)\n",
    "            # [batch_sz, sent_len, hidden_sz * (num_context + 1)]\n",
    "            output = torch.cat((output, context_tsr), dim=2)\n",
    "\n",
    "        # output is now [batch, sent_len, V]:\n",
    "        output = self.out_linear(output)\n",
    "        output = F.log_softmax(output, dim=2)\n",
    "        return output, hidden\n",
    "\n",
    "class AttnDecoder(BaseEncoder):\n",
    "    def __init__(self, TEXT, enc_bidirectional=False, tie_weights=False,\n",
    "                 enc_linear=0, **kwargs):\n",
    "        super(AttnDecoder, self).__init__(TEXT, **kwargs)\n",
    "        print('Using final MLP')\n",
    "        self.enc_directions = 2 if enc_bidirectional else 1\n",
    "        # XXX\n",
    "        blowup = self.enc_directions # one for our output, one or two for context\n",
    "        self.out_linear_dec = nn.Linear(self.hidden_size, self.V)\n",
    "        self.out_linear_contxt = nn.Linear(blowup * self.hidden_size, self.V)\n",
    "        # self.mlp_linear = nn.Linear(self.hidden_size, self.V)\n",
    "\n",
    "        self.enc_linear = enc_linear\n",
    "        if self.enc_linear > 0:\n",
    "            self.attn_linear = nn.Linear(self.enc_directions * self.enc_linear,\n",
    "                                         self.hidden_size)\n",
    "\n",
    "        \n",
    "        if tie_weights:\n",
    "            if self.hidden_size != self.D:\n",
    "                raise ValueError('For tied weights, hidden_size must equal num embeddings!')\n",
    "            self.out_linear_dec.weight = self.embeddings.weight\n",
    "        \n",
    "    def forward(self, input_tsr, hidden, enc_output, mask_inds=None):\n",
    "        # [batch_sz, sent_len, D]:\n",
    "        embedding = self.embeddings(input_tsr)\n",
    "\n",
    "        # XXX\n",
    "        # embedding = F.relu(embedding)\n",
    "        embedding = self.dropout(embedding)\n",
    "        \n",
    "        dec_output, hidden = self.lstm(embedding, hidden)\n",
    "        \n",
    "        # Now do attention: enc_output is [batch_sz, sent_len_src, hidden_sz],\n",
    "        # and dec_output is [batch_sz, sent_len_trg, hidden_sz]\n",
    "        \n",
    "        # Normally do linear layer after dropout\n",
    "        if self.enc_linear > 0:\n",
    "            # print(enc_output.size())\n",
    "            # print(self.enc_directions * self.enc_linear,\n",
    "            #                              self.hidden_size)\n",
    "            enc_output_lin = self.attn_linear(enc_output)\n",
    "        else:\n",
    "            enc_output_lin = enc_output\n",
    "\n",
    "        # enc_output_perm is [batch_sz, hidden_sz, sent_len_src]\n",
    "        enc_output_perm = enc_output_lin.permute(0, 2, 1)\n",
    "        \n",
    "        # should be [batch_sz, sent_len_trg, sent_len_src]\n",
    "        # Note that decoder hidden state for output pos t is compouted \n",
    "        # using hidden state of the last layer (i.e. enc_output) at pos t\n",
    "        # as opposed to t-1, as in Bahdanau\n",
    "        dot_products = torch.bmm(dec_output, enc_output_perm)\n",
    "\n",
    "        # mask_inds is [batch_sz, sent_len_src]\n",
    "        if not mask_inds is None:\n",
    "            # np.inf gives nans...\n",
    "            # Using braodcasting\n",
    "            mask_inds = autograd.Variable(torch.Tensor([np.inf])) * mask_inds\n",
    "            # remove nans\n",
    "            mask_inds[mask_inds != mask_inds] = 0\n",
    "            dot_products = dot_products - torch.unsqueeze(mask_inds, 1)\n",
    "        \n",
    "        # This is the attn distribution, [batch_sz, sent_len_trg, sent_len_src]\n",
    "        dot_products_sftmx = F.softmax(dot_products, dim=2)\n",
    "\n",
    "        \n",
    "        # [batch_sz, sent_len_trg, hidden_sz]\n",
    "        context = torch.bmm(dot_products_sftmx, enc_output)\n",
    "\n",
    "        # XXX\n",
    "        output_1 = self.out_linear_dec(self.dropout(dec_output))\n",
    "        output_2 = self.out_linear_contxt(self.dropout(context))\n",
    "        output = output_1 + output_2\n",
    "        # output = self.mlp_linear(self.dropout(F.tanh(output)))\n",
    "        output = F.log_softmax(output, dim=2)\n",
    "        \n",
    "        # [batch_sz, sent_len_trg, hidden_sz * 2]\n",
    "        # output = torch.cat((dec_output, context), dim=2)\n",
    "        # output = self.dropout(output)\n",
    "        # output = self.out_linear(output)\n",
    "        # output = F.log_softmax(output, dim=2)\n",
    "        return output, hidden, dot_products_sftmx      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "a = np.array(['a', 'b','c'])\n",
    "idx = np.where(a == 'd')[0]\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## PROBABLY NOT RELEVANT STUFF BELOW HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bs_decoder_new = AttnDecoder(EN, hidden_size=500, num_layers=4, word_features=500, dropout=0.2)\n",
    "old_params = list(bs_decoder.parameters())\n",
    "for i,p in enumerate(bs_decoder_new.parameters()):\n",
    "    p.data = old_params[i].data\n",
    "print(list(bs_decoder_new.parameters())[0])\n",
    "print(list(bs_decoder.parameters())[0])\n",
    "bs_decoder_new.lstm.flatten_parameters()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
