{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text text processing library\n",
    "import torchtext\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "from torchtext.vocab import Vectors\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import itertools as it\n",
    "# from models import *\n",
    "# from helpers import *\n",
    "# import main\n",
    "import matplotlib.pyplot as plt\n",
    "import spacy\n",
    "import time\n",
    "MAX_LEN = 20\n",
    "MIN_FREQ = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_de = spacy.load('de')\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "def tokenize_de(text):\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "BOS_WORD = '<s>'\n",
    "EOS_WORD = '</s>'\n",
    "DE = data.Field(tokenize=tokenize_de)\n",
    "\n",
    "# only target needs BOS/EOS:\n",
    "EN = data.Field(tokenize=tokenize_en, init_token = BOS_WORD, eos_token = EOS_WORD) \n",
    "\n",
    "train, val, test = datasets.IWSLT.splits(exts=('.de', '.en'), fields=(DE, EN), \n",
    "                                         filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and \n",
    "                                         len(vars(x)['trg']) <= MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DE.build_vocab(train.src, min_freq=MIN_FREQ)\n",
    "EN.build_vocab(train.trg, min_freq=MIN_FREQ)\n",
    "\n",
    "train_iter, val_iter = data.BucketIterator.splits((train, val), batch_size=32, device=-1,\n",
    "                                                  repeat=False, sort_key=lambda x: len(x.src))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch = next(iter(train_iter))\n",
    "idx = 0\n",
    "print(\"Source\")\n",
    "print(' '.join([DE.vocab.itos[w] for w in batch.src.data[:,idx]]))\n",
    "print(\"Target\")\n",
    "print(' '.join([EN.vocab.itos[w] for w in batch.trg.data[:,idx]]))\n",
    "print(DE.vocab.stoi['pad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is unavailable...\n",
      "Validation time: 1.440321 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1661.6927327180153"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = NMTEvaluator([bs_encoder, bs_decoder], DE, EN)\n",
    "evaluator.evaluate(val_iter, num_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target padding token: 1\n",
      "Using CUDA...\n",
      "Target padding token: 1\n",
      "Using CUDA...\n",
      "Epoch 0, loss: 67.997849, norm: 5.400293, elapsed: 123.604213, lrn_rate: 0.700000\n",
      "Validation time: 0.317650 seconds\n",
      "Validation set metric: 13.252261\n",
      "Epoch 1, loss: 58.891731, norm: 4.741750, elapsed: 247.950029, lrn_rate: 0.700000\n",
      "Validation time: 0.287907 seconds\n",
      "Validation set metric: 10.227173\n",
      "Epoch 2, loss: 54.438667, norm: 5.911691, elapsed: 363.922499, lrn_rate: 0.700000\n",
      "Validation time: 0.308646 seconds\n",
      "Validation set metric: 8.875916\n",
      "Epoch 3, loss: 56.392853, norm: 5.608681, elapsed: 481.610786, lrn_rate: 0.700000\n",
      "Validation time: 0.292938 seconds\n",
      "Validation set metric: 8.219257\n",
      "Epoch 4, loss: 52.174797, norm: 6.666979, elapsed: 607.977410, lrn_rate: 0.700000\n",
      "Validation time: 0.305438 seconds\n",
      "Validation set metric: 7.723540\n",
      "Epoch 5, loss: 50.490479, norm: 5.672861, elapsed: 730.866575, lrn_rate: 0.700000\n",
      "Validation time: 0.309030 seconds\n",
      "Validation set metric: 7.469425\n",
      "Epoch 6, loss: 43.117153, norm: 6.355931, elapsed: 848.808041, lrn_rate: 0.700000\n",
      "Validation time: 0.296658 seconds\n",
      "Validation set metric: 7.455178\n",
      "Epoch 7, loss: 40.964428, norm: 5.712735, elapsed: 966.767438, lrn_rate: 0.700000\n",
      "Validation time: 0.278699 seconds\n",
      "Validation set metric: 7.366994\n",
      "Epoch 8, loss: 40.610348, norm: 5.894077, elapsed: 1090.680003, lrn_rate: 0.700000\n",
      "Validation time: 0.325475 seconds\n",
      "Validation set metric: 7.428063\n",
      "Epoch 9, loss: 41.748569, norm: 6.091435, elapsed: 1218.343647, lrn_rate: 0.700000\n",
      "Validation time: 0.288487 seconds\n",
      "Validation set metric: 7.457728\n",
      "Epoch 10, loss: 43.941078, norm: 6.451295, elapsed: 1342.408243, lrn_rate: 0.700000\n",
      "Validation time: 0.291022 seconds\n",
      "Validation set metric: 7.358460\n",
      "Epoch 11, loss: 42.125629, norm: 6.400049, elapsed: 1463.451678, lrn_rate: 0.700000\n",
      "Validation time: 0.285880 seconds\n",
      "Validation set metric: 7.518932\n",
      "Epoch 12, loss: 43.427292, norm: 7.839340, elapsed: 1597.451478, lrn_rate: 0.700000\n",
      "Validation time: 0.288470 seconds\n",
      "Validation set metric: 7.684168\n",
      "Epoch 13, loss: 33.677013, norm: 6.218863, elapsed: 1723.121654, lrn_rate: 0.700000\n",
      "Validation time: 0.290298 seconds\n",
      "Validation set metric: 7.781574\n",
      "Epoch 14, loss: 43.973030, norm: 8.058181, elapsed: 1843.536870, lrn_rate: 0.700000\n",
      "Validation time: 0.299995 seconds\n",
      "Validation set metric: 7.739259\n",
      "Epoch 15, loss: 42.954937, norm: 7.699135, elapsed: 1968.437994, lrn_rate: 0.700000\n",
      "Validation time: 0.296476 seconds\n",
      "Validation set metric: 8.150277\n",
      "Epoch 16, loss: 41.622684, norm: 7.059343, elapsed: 2085.902933, lrn_rate: 0.700000\n",
      "Validation time: 0.291396 seconds\n",
      "Validation set metric: 7.872363\n",
      "Epoch 17, loss: 41.027870, norm: 8.266948, elapsed: 2216.169501, lrn_rate: 0.700000\n",
      "Validation time: 0.304219 seconds\n",
      "Validation set metric: 8.310570\n",
      "Epoch 18, loss: 43.648212, norm: 8.218669, elapsed: 2339.030693, lrn_rate: 0.700000\n",
      "Validation time: 0.298340 seconds\n",
      "Validation set metric: 8.320399\n",
      "Epoch 19, loss: 36.515739, norm: 6.654588, elapsed: 2460.450201, lrn_rate: 0.700000\n",
      "Validation time: 0.287820 seconds\n",
      "Validation set metric: 8.239900\n",
      "Epoch 20, loss: 39.824272, norm: 6.833759, elapsed: 2576.579586, lrn_rate: 0.700000\n",
      "Validation time: 0.275383 seconds\n",
      "Validation set metric: 8.378230\n",
      "Epoch 21, loss: 40.853951, norm: 7.017832, elapsed: 2710.320789, lrn_rate: 0.700000\n",
      "Validation time: 0.349160 seconds\n",
      "Validation set metric: 8.482706\n"
     ]
    }
   ],
   "source": [
    "train_iter, val_iter = data.BucketIterator.splits((train, val), batch_size=32, device=-1,\n",
    "                                                  repeat=False, sort_key=lambda x: len(x.src))\n",
    "bs_encoder = BaseEncoder(DE, hidden_size=100, num_layers=2, word_features=100)\n",
    "bs_decoder = BaseDecoder(EN, hidden_size=100, num_layers=2, word_features=100)\n",
    "trainer = NMTTrainer([bs_encoder, bs_decoder], DE, EN, lrn_rate=0.7)\n",
    "evaluator = NMTEvaluator([bs_encoder, bs_decoder], DE, EN)\n",
    "trainer.train(train_iter, verbose=True, le=evaluator, val_iter=val_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingsLM(nn.Module):\n",
    "    def __init__(self, TEXT, **kwargs):\n",
    "        super(EmbeddingsLM, self).__init__()\n",
    "        # Initialize dropout\n",
    "        self.dropout_prob = kwargs.get('dropout', 0.0)\n",
    "        self.dropout = nn.Dropout(self.dropout_prob)\n",
    "        \n",
    "        # V is size of vocab, D is dim of embedding\n",
    "        self.V = len(TEXT.vocab)\n",
    "        max_embed_norm = kwargs.get('max_embed_norm', None)\n",
    "        self.D = kwargs.get('word_features', 1000)\n",
    "        self.embeddings = nn.Embedding(self.V, self.D, max_norm=max_embed_norm)\n",
    "\n",
    "class BaseEncoder(EmbeddingsLM):\n",
    "    def __init__(self, TEXT, **kwargs):\n",
    "        super(BaseEncoder, self).__init__(TEXT, **kwargs)\n",
    "        self.hidden_size = kwargs.get('hidden_size', 1000)\n",
    "        self.num_layers = kwargs.get('num_layers', 4)\n",
    "        self.lstm = nn.LSTM(input_size=self.D, hidden_size=self.hidden_size,\n",
    "                            num_layers=self.num_layers,\n",
    "                            dropout=self.dropout_prob, batch_first=True)\n",
    "        \n",
    "    def forward(self, input_tsr, hidden):\n",
    "        # [batch_sz, sent_len, D]:\n",
    "        embedded_tsr = self.embeddings(input_tsr)\n",
    "\n",
    "        # output is [batch, sent_len, hidden_size]\n",
    "        output, hidden = self.lstm(embedded_tsr, hidden)\n",
    "        \n",
    "        # TODO: perhaps add dropout to output\n",
    "        return output, hidden\n",
    "\n",
    "class BaseDecoder(BaseEncoder):\n",
    "    def __init__(self, TEXT, **kwargs):\n",
    "        super(BaseDecoder, self).__init__(TEXT, **kwargs)\n",
    "        # V is the size of the vocab, which is what we're predicting\n",
    "        # (it's also used as input through the embedding)\n",
    "        self.num_context = kwargs.get('num_context', 1)\n",
    "        # For now assume that encoder and decoder have same hidden size\n",
    "        blowup = self.num_context * self.num_layers + 1\n",
    "        self.out_linear = nn.Linear(blowup * self.hidden_size, self.V)\n",
    "\n",
    "    # Context is a tuple (h_T, c_T) of hidden and cell states from\n",
    "    # last time step of encoder\n",
    "    def forward(self, input_tsr, hidden, context):\n",
    "        # [batch_sz, sent_len, D] : note that sent_len may be 1 if we\n",
    "        # feed in each word at a time!\n",
    "        embedding = self.embeddings(input_tsr)\n",
    "        embedding = F.relu(embedding)\n",
    "        output, hidden = self.lstm(embedding, hidden)\n",
    "\n",
    "        if self.num_context:\n",
    "            # We get lucky that hidden is stored as (h,c), \n",
    "            # so hidden (not cell) first\n",
    "            context_tsr = torch.cat(context[:self.num_context])\n",
    "            batch_sz = context_tsr.size(1)\n",
    "            sent_len = output.size(1)\n",
    "            # [batch_sz, 1, hidden_size * num_context]\n",
    "            context_tsr = context_tsr.permute(1,0,2).contiguous().view(batch_sz, 1, -1)\n",
    "            context_tsr = context_tsr.expand(-1, sent_len, -1)\n",
    "            # [batch_sz, sent_len, hidden_sz * (num_context + 1)]\n",
    "            output = torch.cat((output, context_tsr), dim=2)\n",
    "\n",
    "        # output is now [batch, sent_len, V]:\n",
    "        output = self.out_linear(output)\n",
    "        output = F.log_softmax(output, dim=2)\n",
    "        return output, hidden\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMTModelUser(object):\n",
    "    # Models is a list [Encoder, Decoder]\n",
    "    def __init__(self, models, TEXT_SRC, TEXT_TRG, **kwargs):\n",
    "        self._TEXT_SRC = TEXT_SRC\n",
    "        self._TEXT_TRG = TEXT_TRG\n",
    "        self.trg_pad = TEXT_TRG.vocab.stoi['<pad>']\n",
    "        print('Target padding token: %d' % self.trg_pad)\n",
    "        self.models = models\n",
    "        self.use_attention = kwargs.get('attention', False)        \n",
    "        self.cuda = kwargs.get('cuda', True) and \\\n",
    "                    torch.cuda.is_available()\n",
    "        if self.cuda:\n",
    "            print('Using CUDA...')\n",
    "        else:\n",
    "            print('CUDA is unavailable...')\n",
    "\n",
    "    def get_src_and_trg(self, batch):\n",
    "        src = torch.t(batch.src.data).contiguous()\n",
    "        trg = torch.t(batch.trg.data)\n",
    "        # Have to shift the target so we don't predict the word \n",
    "        # we see (this is ok since sentences in trg all begin \n",
    "        # with <s>)\n",
    "        trg_feat = trg[:, :-1].contiguous()\n",
    "        trg_lab = trg[:, 1:].contiguous()\n",
    "        return (src, trg_feat, trg_lab)\n",
    "\n",
    "    def zeros_hidden(self, batch_sz, model_num):\n",
    "        return torch.zeros(self.models[model_num].num_layers, batch_sz,\n",
    "                           self.models[model_num].hidden_size)\n",
    "\n",
    "    # Ok to have self.prev_hidden apply to encoder then decoder since\n",
    "    # encoder all ends before decoder starts\n",
    "    def prepare_hidden(self, batch_sz, zero_out=True, model_num=0):\n",
    "        if (not self.prev_hidden is None) and (not zero_out):\n",
    "            pre_hidden = self.prev_hidden\n",
    "        else:\n",
    "            pre_hidden = (self.zeros_hidden(batch_sz, model_num) \\\n",
    "                          for i in range(2))\n",
    "        if self.cuda:\n",
    "            pre_hidden = tuple(t.cuda() for t in pre_hidden)\n",
    "        return tuple(autograd.Variable(t) for t in pre_hidden)\n",
    "\n",
    "    # kwargs can contain zero_out, model_num for prepare_hidden\n",
    "    def prepare_model_inputs(self, batch, **kwargs):\n",
    "        if self.cuda:\n",
    "            src, trg_feat, trg_lab = \\\n",
    "                tuple(t.cuda() for t in self.get_src_and_trg(batch))\n",
    "        else:\n",
    "            src, trg_feat, trg_lab = self.get_src_and_trg(batch)\n",
    "\n",
    "        # TODO: can comment this out (assuming it passes\n",
    "        # -- just is checking batch-sz)\n",
    "        assert batch.src.size(1) == batch.trg.size(1)\n",
    "        var_hidden = self.prepare_hidden(batch.src.size(1), **kwargs)\n",
    "\n",
    "        var_src = autograd.Variable(src)\n",
    "        var_trg_feat = autograd.Variable(trg_feat)\n",
    "        var_trg_lab = autograd.Variable(trg_lab)\n",
    "\n",
    "        return (var_src, var_trg_feat, var_trg_lab, var_hidden)\n",
    "\n",
    "    def init_epoch(self):\n",
    "        self.prev_hidden = None\n",
    "        \n",
    "    def run_model(self, batch, mode='mean'):\n",
    "        # var_src, var_trg are [batch_sz, sent_len]\n",
    "        var_src, var_trg_feat, var_trg_lab, var_hidden = \\\n",
    "            self.prepare_model_inputs(\n",
    "            batch, zero_out=True, model_num=0)\n",
    "\n",
    "        # For attention, will use enc_output (not otherwise)\n",
    "        enc_output, enc_hidden = self.models[0](var_src, var_hidden)\n",
    "        self.prev_hidden = enc_hidden\n",
    "        if self.use_attention:\n",
    "            raise NotImplementedError('Attention not yet implemented!')\n",
    "        else:\n",
    "            # Using real words as input. Use prev_hidden both to\n",
    "            # initialize hidden state (the first time) and as context\n",
    "            # vector\n",
    "            dec_output, dec_hidden = self.models[1](\n",
    "                var_trg_feat, self.prev_hidden, enc_hidden)\n",
    "            self.prev_hidden = dec_hidden\n",
    "\n",
    "            loss = self.nll_loss(dec_output, var_trg_lab, mode=mode)\n",
    "        return loss\n",
    "\n",
    "    # Assume log_probs is [batch_sz, sent_len, V], output is\n",
    "    # [batch_sz, sent_len]\n",
    "    def nll_loss(self, log_probs, output, mode='mean', **kwargs):\n",
    "        sent_len = log_probs.size(1)\n",
    "        log_probs_rshp = log_probs.view(-1, log_probs.size(2))\n",
    "        output_rshp = output.view(-1)\n",
    "        if mode == 'mean':\n",
    "            # Sum over all words in sent, mean over sentences; \n",
    "            # make sure to ignore padding\n",
    "            return F.nll_loss(log_probs_rshp, output_rshp, \n",
    "                              ignore_index=self.trg_pad, \n",
    "                              **kwargs) * sent_len\n",
    "        elif mode == 'sum':\n",
    "            # Sum over all sentences and words in them\n",
    "            return F.nll_loss(log_probs_rshp, output_rshp,\n",
    "                              ignore_index=self.trg_pad,\n",
    "                              size_average=False)\n",
    "        else:\n",
    "            raise ValueError('Invalid mode field: %s' % mode)\n",
    "                \n",
    "\n",
    "class NMTEvaluator(NMTModelUser):\n",
    "    def __init__(self, models, TEXT_SRC, TEXT_TRG, **kwargs):\n",
    "        super(NMTEvaluator, self).__init__(models, TEXT_SRC, TEXT_TRG,\n",
    "                                           **kwargs)\n",
    "\n",
    "    def evaluate(self, test_iter, num_iter=None):\n",
    "        start_time = time.time()\n",
    "        for model in self.models:\n",
    "            model.eval()\n",
    "        nll_sum = 0\n",
    "        nll_cnt = 0\n",
    "\n",
    "        self.init_epoch()\n",
    "        for i,batch in enumerate(test_iter):\n",
    "            nll_cnt += batch.trg.data.numel()\n",
    "            loss = self.run_model(batch, mode='sum')\n",
    "            # TODO: make sure loss just has 1 element!\n",
    "            nll_sum += loss.data[0]\n",
    "            \n",
    "            if not num_iter is None and i > num_iter:\n",
    "                break\n",
    "                        \n",
    "        # Wrap the model.eval(), just in case\n",
    "        for model in self.models:\n",
    "            model.train()\n",
    "        \n",
    "        print('Validation time: %f seconds' % (time.time() - start_time))\n",
    "        return np.exp(nll_sum / nll_cnt)\n",
    "\n",
    "    \n",
    "class NMTTrainer(NMTModelUser):\n",
    "    def __init__(self, models, TEXT_SRC, TEXT_TRG, **kwargs):\n",
    "        super(NMTTrainer, self).__init__(models, TEXT_SRC, TEXT_TRG, **kwargs)\n",
    "\n",
    "        self.base_lrn_rate = kwargs.get('lrn_rate', 0.1)\n",
    "        self.optimizer_type = kwargs.get('optimizer', optim.SGD)\n",
    "        self.optimizers = [self.optimizer_type(filter(lambda p : p.requires_grad,\n",
    "                                                      model.parameters()),\n",
    "                                               lr = self.base_lrn_rate) for \\\n",
    "                           model in self.models]\n",
    "\n",
    "        self.lr_decay_opt = kwargs.get('lrn_decay', 'none')\n",
    "        # TODO: setup for lr decay\n",
    "\n",
    "        self.clip_norm = kwargs.get('clip_norm', 10)\n",
    "        self.init_lists()\n",
    "        if self.cuda:\n",
    "            for model in self.models:\n",
    "                model.cuda()\n",
    "\n",
    "    def init_lists(self):\n",
    "        self.training_losses = list()\n",
    "        self.training_norms = list()\n",
    "        self.val_perfs = list()\n",
    "\n",
    "    def get_loss_data(self, loss):\n",
    "        if self.cuda:\n",
    "            return loss.data.cpu().numpy()[0]\n",
    "        else:\n",
    "            return loss.data.numpy()[0]\n",
    "\n",
    "    def make_recordings(self, loss, norm):\n",
    "        self.training_norms.append(norm)\n",
    "        self.training_losses.append(loss)\n",
    "\n",
    "    def clip_norms(self):\n",
    "        # Clip grad norm after backward but before step\n",
    "        if self.clip_norm > 0:\n",
    "            parameters = tuple()\n",
    "            for model in self.models:\n",
    "                parameters += tuple(model.parameters())\n",
    "                \n",
    "            # Norm clipping: returns a float\n",
    "            norm = nn.utils.clip_grad_norm(\n",
    "                parameters, self.clip_norm)\n",
    "        else:\n",
    "            norm = -1\n",
    "        return norm\n",
    "\n",
    "    def train_batch(self, batch, **kwargs):\n",
    "        for model in self.models:\n",
    "            model.zero_grad()\n",
    "            \n",
    "        loss = self.run_model(batch)\n",
    "        loss.backward()\n",
    "\n",
    "        # norms must be clipped after backward but before step\n",
    "        norm = self.clip_norms()\n",
    "\n",
    "        loss_data = self.get_loss_data(loss)\n",
    "        # print('TEMP: ', loss_data, norm)\n",
    "        if kwargs.get('verbose', False):\n",
    "            self.make_recordings(loss_data, norm)\n",
    "\n",
    "        for optimizer in self.optimizers:\n",
    "            optimizer.step()\n",
    "\n",
    "        # Return loss and norm (before gradient step)\n",
    "        return loss_data, norm\n",
    "\n",
    "    def init_parameters(self):\n",
    "        for model in self.models:\n",
    "            for p in model.parameters():\n",
    "                p.data.uniform_(-0.05, 0.05)\n",
    "\n",
    "    def train(self, torch_train_iter, le=None, val_iter=None, **kwargs):\n",
    "        self.init_lists()\n",
    "        start_time = time.time()\n",
    "        self.init_parameters()\n",
    "        torch_train_iter.init_epoch()\n",
    "        for epoch in range(kwargs.get('num_iter', 100)):\n",
    "            self.init_epoch()\n",
    "            for model in self.models:\n",
    "                model.train()\n",
    "\n",
    "            # TODO: LR decay\n",
    "            train_iter = iter(torch_train_iter)\n",
    "\n",
    "            for batch in train_iter:\n",
    "                res_loss, res_norm = self.train_batch(batch, **kwargs)\n",
    "\n",
    "            if epoch % kwargs.get('skip_iter', 1) == 0:\n",
    "                if not kwargs.get('verbose', False):\n",
    "                    self.make_recordings(res_loss, res_norm)\n",
    "\n",
    "            print('Epoch %d, loss: %f, norm: %f, elapsed: %f, lrn_rate: %f' \\\n",
    "                  % (epoch, np.mean(self.training_losses[-10:]),\n",
    "                     np.mean(self.training_norms[-10:]),\n",
    "                     time.time() - start_time,\n",
    "                     self.base_lrn_rate)) #  * self.lambda_lr(epoch)))\n",
    "                    \n",
    "            \n",
    "            if (not le is None) and (not val_iter is None):\n",
    "                self.val_perfs.append(le.evaluate(val_iter))\n",
    "                print('Validation set metric: %f' % \\\n",
    "                      self.val_perfs[-1])\n",
    "\n",
    "        if len(self.val_perfs) >= 1:\n",
    "            print('FINAL VAL PERF', self.val_perfs[-1])\n",
    "            return self.val_perfs[-1]\n",
    "        return -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = (1,2)\n",
    "a[:1] + (0,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
