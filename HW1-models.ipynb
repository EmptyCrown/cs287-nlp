{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.vocab import Vectors, GloVe\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from hw1.models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Our input $x$\n",
    "TEXT = torchtext.data.Field()\n",
    "    \n",
    "# Our labels $y$\n",
    "LABEL = torchtext.data.Field(sequential=False)\n",
    "\n",
    "\n",
    "train, val, test = torchtext.datasets.SST.splits(\n",
    "    TEXT, LABEL,\n",
    "    filter_pred=lambda ex: ex.label != 'neutral')\n",
    "\n",
    "# Build vocab\n",
    "TEXT.build_vocab(train)\n",
    "LABEL.build_vocab(train)\n",
    "\n",
    "train_iter, val_iter, test_iter = torchtext.data.BucketIterator.splits(\n",
    "(train, val, test), batch_size=10, device=-1)\n",
    "\n",
    "# Build the vocabulary with word embeddings\n",
    "url = 'https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.simple.vec'\n",
    "TEXT.vocab.load_vectors(vectors=Vectors('wiki.simple.vec', url=url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      " 0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "-0.0671 -0.0058 -0.0793  ...   0.0308  0.0540  0.0882\n",
      "-0.2525 -0.0505 -0.1246  ...   0.0678  0.0826  0.3026\n",
      "          ...             â‹±             ...          \n",
      " 0.1253 -0.1663  0.2053  ...   0.0221 -0.5225 -0.0705\n",
      "-0.0021  0.0008 -0.0020  ...  -0.0006  0.0027 -0.0001\n",
      " 0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
      "[torch.FloatTensor of size 16284x300]\n",
      "\n",
      "Parameter containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      "1.00000e-02 *\n",
      "  3.3217  4.4757 -3.9592 -4.9604  3.6914  2.3955  0.5458  1.0182  0.4738  4.0487\n",
      "  0.0547 -2.2727 -3.4778 -1.7274  0.5824  1.8603 -4.9241  0.4656  4.5720  0.5508\n",
      " -2.7608  4.3582  4.9227 -1.1431  1.2861  1.0787 -4.8951  5.1399 -5.3310  4.2848\n",
      "\n",
      "Columns 10 to 19 \n",
      "1.00000e-02 *\n",
      "  4.0068 -5.3642  3.7567 -0.4841 -4.6548  4.6807  3.2525 -0.3577 -2.7947 -4.8811\n",
      "  4.2396  1.5764 -2.6887 -2.0928 -1.0368  2.1105 -4.0316  3.7971  1.4540  2.9874\n",
      " -0.7472 -1.6380  2.9901  3.2048 -1.0400 -4.9838 -3.7554  1.2413  5.4105  1.0407\n",
      "\n",
      "Columns 20 to 29 \n",
      "1.00000e-02 *\n",
      "  0.0763 -0.3284 -1.2104  3.1305 -5.3224  1.3390  2.0884 -2.8186  4.7332 -2.7459\n",
      "  5.1232  1.1442  0.7730 -4.2292  2.1229 -2.4171 -0.2915 -5.6139  0.3275  1.2643\n",
      "  5.2082  3.2540 -4.2654 -2.0538 -1.2002  5.0484  3.7396  0.9332 -0.4907  1.7126\n",
      "\n",
      "Columns 30 to 39 \n",
      "1.00000e-02 *\n",
      "  3.8238 -0.4306 -4.0468 -5.3402 -2.9594 -0.7510  4.1420 -2.4935 -0.7242  5.3829\n",
      " -1.5905  5.4476  5.6696  3.7976 -1.8656  0.6869  4.9153 -2.5359  3.5091 -2.8633\n",
      " -0.8703  1.6193  1.0976 -3.6332 -5.0878  5.2939 -5.5774  4.1435 -3.7399 -0.4999\n",
      "\n",
      "Columns 40 to 49 \n",
      "1.00000e-02 *\n",
      " -2.6037 -2.8280 -0.3210  1.8911 -1.3273  1.0293 -3.4636 -1.2589  0.1076 -1.2026\n",
      " -0.9168  1.6498  3.4897 -4.7481  2.4873  0.2226 -3.5997  3.1526 -1.3421 -3.1180\n",
      " -5.4840 -1.7013 -0.2473 -5.2783  2.8525 -0.4201  1.2654 -4.6138 -4.7881  4.2896\n",
      "\n",
      "Columns 50 to 59 \n",
      "1.00000e-02 *\n",
      " -3.6705 -1.9344  3.0823  1.2876 -3.5045  4.5772  1.6838 -4.3091 -4.2581  3.8206\n",
      "  1.9190  4.4359  4.3140  0.2135 -2.8611 -5.5954  2.7717  3.8054  0.4575 -2.0299\n",
      " -2.6547  3.7476  1.4855 -3.0695  5.1501 -1.6606 -3.0182 -3.8412  5.5131  3.6917\n",
      "\n",
      "Columns 60 to 69 \n",
      "1.00000e-02 *\n",
      "  3.1016  2.7873 -1.0153  0.9281  4.4549  0.0979 -4.1346  1.9410  4.3850  3.2060\n",
      " -2.1786 -2.7316  4.1320  1.6686  0.0747  1.9890  4.9986  4.2873  5.0028  4.5843\n",
      "  0.5024 -3.6589 -3.7557  2.3142  4.6491 -1.2852  2.6439 -5.1742 -5.4214  3.9220\n",
      "\n",
      "Columns 70 to 79 \n",
      "1.00000e-02 *\n",
      " -2.3605  3.7475  4.3362 -0.7123  3.4416  5.5054 -4.2306 -0.9956 -4.9519 -4.5216\n",
      "  3.0653  2.6843  2.3663  0.0620  2.8040 -0.2768 -1.0607 -4.7869  5.1000  4.4733\n",
      " -3.8801  0.9853  5.4023 -1.1035  4.7310  2.9812 -1.7779 -3.1936 -4.0748 -1.3573\n",
      "\n",
      "Columns 80 to 89 \n",
      "1.00000e-02 *\n",
      "  4.4916  2.3259  2.8854  3.6132 -2.7211  1.6534 -4.5776 -0.2569 -0.3376  0.7052\n",
      " -4.6029  3.9821  4.4892  1.6807  5.6441 -3.1737 -2.8689  4.8339  1.7205  3.6618\n",
      "  1.0194 -5.1166 -3.4052 -2.2545 -2.7608  0.2049  2.2786  1.3762  5.3015 -2.5522\n",
      "\n",
      "Columns 90 to 99 \n",
      "1.00000e-02 *\n",
      "  3.7023  2.2861 -0.3007 -4.4091  3.1512  3.3149  5.2014  4.2785 -3.6312 -0.6405\n",
      " -2.8067  1.8325 -5.5117  0.5774  1.3586  0.7862 -4.4733 -1.0013 -0.4848 -5.3813\n",
      " -1.7224  0.8255  1.7220 -1.0910  0.5659  2.1985  5.4383 -5.1991  3.6544  2.3832\n",
      "\n",
      "Columns 100 to 109 \n",
      "1.00000e-02 *\n",
      " -0.9055 -0.8476 -1.2796  2.1844  0.8732  0.4097  0.9813  4.8755 -0.6712  5.7218\n",
      " -3.2291  0.9256 -4.1352 -2.0206  2.4876  1.4649  4.4416  0.8695 -5.5426  3.5068\n",
      "  1.2606  3.0848 -3.6716  1.2275 -0.3866 -5.4967 -0.4117 -2.2052  2.9120 -5.2663\n",
      "\n",
      "Columns 110 to 119 \n",
      "1.00000e-02 *\n",
      " -4.6554 -5.1709 -0.1268 -5.7581  3.9427 -5.0598  3.2719 -3.7184  5.1833 -1.4664\n",
      " -5.7253 -2.0032  3.9503 -4.0793 -3.6864  0.0803  1.5287  2.7483  2.1834  2.2797\n",
      "  4.0159  0.0971 -0.1229  2.4200  0.8899 -1.2637 -0.9027 -2.0768  3.4200  5.6570\n",
      "\n",
      "Columns 120 to 129 \n",
      "1.00000e-02 *\n",
      " -0.2629 -2.8838  2.0488 -3.4687  4.3538 -4.0095 -2.3667 -3.0865  2.1592  4.2059\n",
      " -0.4398 -0.6933 -0.1304 -2.2111 -5.3896 -2.9515  4.7458  1.4958  5.1088  2.4177\n",
      " -0.8700  1.9309 -4.4802 -2.7088  2.6511 -3.9336  3.6821  2.0496  2.8140 -4.3836\n",
      "\n",
      "Columns 130 to 139 \n",
      "1.00000e-02 *\n",
      " -1.5031  3.6848 -2.9007  2.7413  3.4819  1.7682  5.6640  0.3354 -5.0756 -2.3833\n",
      " -1.5714 -4.9956 -4.0675 -4.3250 -2.5988  1.8115  0.9645 -4.3540 -5.0752  1.6297\n",
      "  3.8848 -3.8868 -1.2823  1.5166 -2.8334  4.3596 -0.7376 -2.1581  1.1877 -4.8739\n",
      "\n",
      "Columns 140 to 149 \n",
      "1.00000e-02 *\n",
      " -1.0444 -3.3382 -3.6200  1.1502  0.3124 -2.1784 -0.7624  3.0404 -4.3310 -2.3304\n",
      "  3.7627 -0.2167 -0.7626 -2.0879  0.2070 -0.0248 -0.7809  3.8289  2.6297  2.3477\n",
      " -5.3664  2.4665  4.0825  4.1546 -5.4946  0.0558  0.4848 -2.3472  5.4350 -1.5973\n",
      "\n",
      "Columns 150 to 159 \n",
      "1.00000e-02 *\n",
      "  1.6841  0.2751 -2.9073  5.0311  4.1750 -3.4968 -2.7744  4.0596 -1.2543 -3.7881\n",
      " -2.9389  3.7190 -4.6358  5.3959  5.7027  3.3102 -1.1375 -3.5320 -3.2520 -3.3438\n",
      " -5.7619  4.3458 -3.4805  0.6216 -5.3986  3.8037  1.5510 -2.0775 -5.0120 -3.5922\n",
      "\n",
      "Columns 160 to 169 \n",
      "1.00000e-02 *\n",
      " -3.2760 -1.6650  1.9725 -2.0143  3.1369 -4.8498  1.3150 -1.6754  5.0238  5.0616\n",
      "  5.3817 -2.7124  1.4008 -4.7193  4.9147  3.8829 -4.7843 -0.5430  3.4159 -5.3672\n",
      "  1.5723  2.6225  0.4124  0.2438 -4.7956 -4.7904  4.2433 -1.0477  1.4667 -2.2137\n",
      "\n",
      "Columns 170 to 179 \n",
      "1.00000e-02 *\n",
      " -5.7055  2.1063 -0.8797  4.4708 -1.6770  3.3409 -0.1216  2.5535 -3.3459  1.2610\n",
      "  3.6062 -1.3018 -0.3556  3.0945 -5.1866 -0.5705  0.5821 -0.8520 -5.1923 -5.0689\n",
      " -2.8683 -5.3723 -0.3554 -0.1036  2.7073  5.3951 -3.5396  0.8533 -1.7342 -4.1071\n",
      "\n",
      "Columns 180 to 189 \n",
      "1.00000e-02 *\n",
      " -1.6514  0.5440  5.4972  4.2609 -2.9373 -4.7953  5.7523 -1.1741  0.1797 -1.7081\n",
      " -1.9831  1.4283 -3.2951  5.2734 -3.8731 -2.0998 -4.0670 -0.7749 -0.3862 -0.6622\n",
      " -0.3836  2.5628  4.4592 -1.4272 -4.1698 -4.6369 -2.2299 -0.7882 -4.6761  5.0223\n",
      "\n",
      "Columns 190 to 199 \n",
      "1.00000e-02 *\n",
      "  1.7962 -4.8469  3.0041  1.4320 -0.4716 -4.8505  5.1255  3.6905 -2.4802 -4.4153\n",
      " -2.3442 -1.3359 -4.0668 -2.8374  0.3127 -3.5159  4.1884  2.2917  4.5309  3.4816\n",
      " -1.7316 -5.3504  3.5711 -5.7463  3.1266 -1.7624  3.6729 -1.0341  0.5972  1.9352\n",
      "\n",
      "Columns 200 to 209 \n",
      "1.00000e-02 *\n",
      " -2.8278 -1.9430  5.7490  2.1397 -2.3768 -0.6001  0.3238 -1.3097 -0.4051 -5.4430\n",
      "  1.1947 -0.0188 -3.1735  0.1018  5.0355 -2.3514 -5.6109 -1.3455  4.7763 -3.3089\n",
      " -2.6714  3.6768  3.1311 -0.1834  3.3359 -2.3005 -5.2375 -4.6405  0.4957  1.1020\n",
      "\n",
      "Columns 210 to 219 \n",
      "1.00000e-02 *\n",
      "  0.1052 -2.7415 -0.6656 -0.1988 -4.2067  0.9732 -5.6233 -4.3708 -3.1854  1.9079\n",
      " -3.3613 -0.8582  5.3808  0.2586  2.1568 -1.4591  4.0938  5.0502 -0.7752  3.8943\n",
      " -5.1172 -5.7632 -2.9483  2.7552 -1.8954 -0.8414 -0.5180 -5.7068  0.6818  1.0191\n",
      "\n",
      "Columns 220 to 229 \n",
      "1.00000e-02 *\n",
      " -4.2989 -2.3565  4.7277 -0.1960 -0.3764 -5.6676 -4.3855  3.3564 -4.3326  0.5908\n",
      "  5.3321  5.0517  3.6035  4.4889 -1.5442 -2.3599  2.0847  0.3818 -3.1105 -5.7207\n",
      " -2.8818 -4.4479 -1.6415  1.5188 -0.1828  3.6696  2.5837 -2.9342 -0.8288 -1.2751\n",
      "\n",
      "Columns 230 to 239 \n",
      "1.00000e-02 *\n",
      "  4.9560  1.2633 -2.0577 -2.4860 -0.5946 -0.7742  3.3994  2.2346  2.9272  4.0297\n",
      "  5.7186  0.8834 -3.9935 -5.3115  0.3326 -5.2229  2.6899  4.0456  4.9545 -0.1772\n",
      " -1.8570 -1.2619  0.7023  1.2672  2.2716  4.1218 -4.4705  5.6959  0.6371  2.1361\n",
      "\n",
      "Columns 240 to 249 \n",
      "1.00000e-02 *\n",
      " -0.6046  0.0502  1.4676  3.3439 -5.4202  2.3258  4.2658  3.8564  2.5737  0.3609\n",
      "  5.2182 -5.6455  0.1955 -1.9707  0.3083  3.8929  2.0474  0.9702  5.3021  3.6764\n",
      " -2.4423  4.8714  5.4385  5.5585  3.6020  4.4645  1.9460 -4.7692  2.5105  2.1970\n",
      "\n",
      "Columns 250 to 259 \n",
      "1.00000e-02 *\n",
      " -4.7762 -3.7641  1.4223 -4.7191  2.3967  0.3756  4.3720 -1.0602  0.8049 -2.4124\n",
      " -0.9931  5.0875 -2.8144 -2.9493 -2.9392 -5.2949  3.9253  3.9923 -1.1931 -0.2271\n",
      " -1.7803 -0.4880  4.7451 -2.8377  3.6120  3.4663  4.9519  2.4341  5.2668 -1.0863\n",
      "\n",
      "Columns 260 to 269 \n",
      "1.00000e-02 *\n",
      " -4.3617 -1.4128 -0.1860  1.4852  4.3970 -0.4630 -2.8828  1.0908  3.3329 -5.0648\n",
      "  5.3638  3.8509 -4.8603 -1.5991  3.6645 -5.5531 -5.2347 -2.3308  0.3475  0.6577\n",
      "  3.9693  5.0637  5.2024  4.5736  0.9989  2.0692  4.0159  3.5467 -2.7589 -0.7659\n",
      "\n",
      "Columns 270 to 279 \n",
      "1.00000e-02 *\n",
      " -5.5890  2.5714 -2.2055  3.9208 -5.1043  5.5723  0.0921  0.1314  0.2334  2.7039\n",
      "  4.4516 -3.7489 -1.1249 -5.5748  0.9872 -2.2215 -4.3598 -3.2107 -5.1292 -2.1587\n",
      " -5.4746  1.9833 -1.3375  3.4766 -3.6164  5.4931 -2.1022 -3.2617  0.5789 -2.1402\n",
      "\n",
      "Columns 280 to 289 \n",
      "1.00000e-02 *\n",
      " -4.7641 -2.7142  2.5571  1.8914 -2.2289  2.0533 -2.4726 -3.1222  3.1758  4.1615\n",
      " -1.2684 -1.5816 -2.7386 -2.4261  4.3806  2.6011  3.5687 -3.2986 -1.0540 -3.9663\n",
      "  0.8255  2.7947 -0.7117  1.0040 -5.4105  1.3003  4.5645 -5.6724  3.6913 -4.9535\n",
      "\n",
      "Columns 290 to 299 \n",
      "1.00000e-02 *\n",
      " -1.5291 -4.1200 -1.6709  5.0079 -2.4637  3.8741  4.7698  1.0916  0.7951 -0.5869\n",
      " -5.7272  0.4970  1.7950 -4.8704 -0.5361 -0.2754  2.6696 -5.2221  0.1135 -2.4889\n",
      "  0.7948  4.6313  0.5414 -2.9703  4.5174  2.5059 -0.6405  3.5009 -1.0402 -3.3707\n",
      "[torch.FloatTensor of size 3x300]\n",
      "\n",
      "Parameter containing:\n",
      "1.00000e-02 *\n",
      "  3.1602\n",
      " -5.4427\n",
      " -1.9598\n",
      "[torch.FloatTensor of size 3]\n",
      "\n",
      "Variable containing:\n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "-0.0671 -0.0058 -0.0793  0.0804 -0.0157 -0.0569  0.0550  0.0325 -0.0141 -0.0193\n",
      "-0.2525 -0.0505 -0.1246  0.0505 -0.1886 -0.1022  0.0137  0.0662 -0.1940 -0.1981\n",
      "-0.0461  0.0396 -0.1733  0.1620 -0.0376 -0.0607  0.0707  0.0110 -0.1545 -0.1324\n",
      "-0.2471 -0.1494 -0.2899  0.2738  0.0042 -0.1182  0.0525  0.0540 -0.1392 -0.1127\n",
      "-0.0085 -0.0252  0.0044  0.1332 -0.1478  0.0025 -0.0520 -0.2059 -0.0907 -0.0784\n",
      "-0.0083 -0.2947 -0.0342  0.0341 -0.0405 -0.2484 -0.0039  0.0805 -0.0365  0.0122\n",
      "-0.1255 -0.0116 -0.0599 -0.1351  0.0042 -0.0030 -0.1033  0.0700 -0.2586 -0.0829\n",
      "-0.0132  0.1339 -0.0326  0.0690  0.0907  0.2216  0.0115 -0.0304  0.0160  0.1215\n",
      " 0.1180 -0.0336  0.0676 -0.1603  0.0631  0.2075 -0.1991  0.0508 -0.0806  0.0330\n",
      "[torch.FloatTensor of size 10x10]\n",
      "\n",
      "Iteration 0, loss: 1.125124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100, loss: 1.694317\n",
      "Iteration 200, loss: 0.437575\n",
      "Iteration 300, loss: 1.140245\n",
      "Iteration 400, loss: 0.465758\n",
      "Iteration 500, loss: 0.449803\n",
      "Iteration 600, loss: 1.286211\n",
      "Iteration 700, loss: 0.642169\n",
      "Iteration 800, loss: 0.848213\n",
      "Iteration 900, loss: 0.715434\n",
      "Variable containing:\n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000  0.0000\n",
      "-0.0968 -0.0231 -0.0775  0.1214 -0.0481 -0.0954  0.0662  0.0168 -0.0572 -0.0308\n",
      "-0.4070 -0.0705 -0.2034  0.1931 -0.2440 -0.1833  0.0393  0.1389 -0.2937 -0.2520\n",
      "-0.1767  0.0833 -0.3064  0.3803 -0.0618 -0.1790  0.1391  0.0540 -0.2477 -0.1820\n",
      "-0.3424 -0.0573 -0.4077  0.4390 -0.0444 -0.2406  0.0932  0.0659 -0.1966 -0.1798\n",
      "-0.1071 -0.0269 -0.1442  0.2953 -0.1712 -0.1028  0.0245 -0.1460 -0.1107 -0.0994\n",
      "-0.1101 -0.2306 -0.1144  0.1689 -0.0556 -0.2764  0.0735  0.1391 -0.0789 -0.0109\n",
      "-0.2102 -0.0145 -0.1591  0.0353 -0.0475 -0.1037 -0.0157  0.1019 -0.3215 -0.1522\n",
      "-0.1239  0.1271 -0.1349  0.2067  0.0580  0.1208  0.0411 -0.0122 -0.0505  0.0446\n",
      " 0.0594 -0.0268  0.0007 -0.0263  0.0535  0.1183 -0.1530  0.0456 -0.1263  0.0007\n",
      "[torch.FloatTensor of size 10x10]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = CBOW(TEXT, LABEL)\n",
    "for p in model.parameters():\n",
    "    if p.requires_grad:\n",
    "        print(p)\n",
    "print(model.embeddings.weight[:10,:10])\n",
    "trainer = TextTrainer(TEXT, LABEL, model)\n",
    "trainer.train(train_iter, num_iter=1000, skip_iter=100)\n",
    "print(model.embeddings.weight[:10,:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, TEXT, LABEL):\n",
    "        super(CBOW, self).__init__()\n",
    "        # Embeddings vectors (should be trainable); [V, d]\n",
    "        # TODO: is default for requires_grad True?\n",
    "        self.embeddings = nn.EmbeddingBag(TEXT.vocab.vectors.size()[0],\n",
    "                                          TEXT.vocab.vectors.size()[1],\n",
    "                                          mode='sum')\n",
    "        self.embeddings.weight = nn.Parameter(TEXT.vocab.vectors, requires_grad=True)\n",
    "        \n",
    "        # Linear layer\n",
    "        self.linear = nn.Linear(TEXT.vocab.vectors.size()[1], len(LABEL.vocab))\n",
    "        \n",
    "    # Here bow is [len-of-sentence, N] -- it is an integer matrix\n",
    "    def forward(self, bow):\n",
    "        bow_features = self.embeddings(bow)\n",
    "        return F.log_softmax(self.linear(bow_features), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogisticRegression2(nn.Module):\n",
    "    def __init__(self, TEXT, LABEL):\n",
    "        super(LogisticRegression2, self).__init__()\n",
    "        # Embeddings vectors (should be trainable); [V, d]\n",
    "        # TODO: is default for requires_grad True?\n",
    "        self.embeddings = nn.EmbeddingBag(len(TEXT.vocab),\n",
    "                                          len(TEXT.vocab),\n",
    "                                          mode='sum')\n",
    "        self.embeddings.weight = nn.Parameter(torch.eye(len(TEXT.vocab)),\n",
    "                                              requires_grad=False)\n",
    "        # Linear layer\n",
    "        self.linear = nn.Linear(len(TEXT.vocab), len(LABEL.vocab))\n",
    "        \n",
    "    # Here bow is [len-of-sentence, N] -- it is an integer matrix\n",
    "    def forward(self, bow):\n",
    "        bow_features = self.embeddings(bow)\n",
    "        return F.log_softmax(self.linear(bow_features), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
