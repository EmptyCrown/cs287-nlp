{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text text processing library\n",
    "import torchtext\n",
    "from torchtext.vocab import Vectors\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import itertools as it\n",
    "from models import *\n",
    "from helpers import *\n",
    "debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Our input $x$\n",
    "TEXT = torchtext.data.Field()\n",
    "\n",
    "# Data distributed with the assignment\n",
    "train, val, test = torchtext.datasets.LanguageModelingDataset.splits(\n",
    "    path=\".\", \n",
    "    train=\"train.txt\", validation=\"valid.txt\", test=\"valid.txt\", text_field=TEXT)\n",
    "\n",
    "TEXT.build_vocab(train)\n",
    "if debug:\n",
    "    TEXT.build_vocab(train, max_size=10)\n",
    "\n",
    "train_iter, val_iter, test_iter = torchtext.data.BPTTIterator.splits(\n",
    "    (train, val, test), batch_size=10, device=-1, bptt_len=32, repeat=False)\n",
    "\n",
    "# Build the vocabulary with word embeddings\n",
    "url = 'https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.simple.vec'\n",
    "TEXT.vocab.load_vectors(vectors=Vectors('wiki.simple.vec', url=url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10])\n",
      "<unk> <unk> <unk> <unk> <eos> <unk> <unk> the <unk> and <unk> <unk> of <unk> <unk> the <unk> <unk> <unk> <unk> a <unk> to <unk> <unk> <unk> <unk> <eos> <unk> <unk> a <unk>\n",
      "<unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> the <unk> 's <unk> <unk> <eos> the <unk> <unk> <unk> in the <unk> <unk> <unk> <unk> <unk> <unk> N <unk>\n"
     ]
    }
   ],
   "source": [
    "it = iter(test_iter)\n",
    "batch = next(it)\n",
    "print(batch.text.size())\n",
    "# print(batch.text[:,3])\n",
    "print(' '.join([TEXT.vocab.itos[i] for i in batch.text[:,4].data]))\n",
    "print(' '.join([TEXT.vocab.itos[i] for i in batch.text[:,5].data]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torchtext.vocab import Vectors\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import itertools as it\n",
    "import time\n",
    "\n",
    "# Debugging functions\n",
    "\n",
    "# batch is [batch_size, sent_len], is a tensor\n",
    "def inspect_batch(batch, TEXT):\n",
    "    for i in range(batch.size(0)):\n",
    "        print(' '.join([TEXT.vocab.itos[j] for j in batch[i,:]]))\n",
    "\n",
    "# Class that Lang{Trainer/Evaluator} extends\n",
    "class LangModelUser(object):\n",
    "    def __init__(self, model, TEXT, use_hidden=False,\n",
    "                 **kwargs):\n",
    "        self._TEXT = TEXT\n",
    "        self.use_hidden = use_hidden\n",
    "        # Amount by which to shift label in a batch (this is a bit\n",
    "        # wasteful since we're therefore wasting the first\n",
    "        # self.shift_label words of a batch, but oh well...); TODO:\n",
    "        # this is problematic for validation/testing...\n",
    "        if self.use_hidden:\n",
    "            self.shift_label = 1\n",
    "        else:\n",
    "            self.shift_label = 0\n",
    "        self.model = model\n",
    "        self.cuda = kwargs.get('cuda', True) and \\\n",
    "            torch.cuda.is_available()\n",
    "        if self.cuda:\n",
    "            print('Using CUDA for evaluation...')\n",
    "        else:\n",
    "            print('CUDA is unavailable...')\n",
    "            \n",
    "    # Here batch is output from a RNN/NNLM/Trigram model:\n",
    "    # [..., size_vocab], and output are the real words: [...]\n",
    "    @staticmethod\n",
    "    def loss_nll(batch, output, mode='mean'):\n",
    "        # [batch_size * sent_len, size_vocab]\n",
    "        vocab_len = output.size()[-1]\n",
    "        output = output.view(-1, vocab_len)\n",
    "        sent_len = batch.size(1)\n",
    "        # [batch_size * sent_len]\n",
    "        batch = batch.view(-1, 1)\n",
    "        batch_probs = -1 * torch.gather(output, 1, \n",
    "                                        batch) #.type(torch.LongTensor))\n",
    "        if mode == 'mean':\n",
    "            return torch.mean(batch_probs) * sent_len\n",
    "        else:\n",
    "            return torch.sum(batch_probs)\n",
    "        return\n",
    "    \n",
    "    @staticmethod\n",
    "    def loss_perplexity(*args):\n",
    "        return torch.exp(self.loss_nll(*args))\n",
    "\n",
    "    # Ignore the last self.shift_label words in each sentence\n",
    "    def get_feature(self, batch):\n",
    "        batch_transpose = torch.t(batch.text.data)\n",
    "        if self.shift_label > 0:\n",
    "            return batch_transpose[:, :-self.shift_label].contiguous()\n",
    "        else:\n",
    "            return batch_transpose.contiguous()\n",
    "\n",
    "    # Ignore the first self.shift_label words in each sentence\n",
    "    def get_label(self, batch):\n",
    "        batch_transpose = torch.t(batch.text.data)\n",
    "        return batch_transpose[:, self.shift_label:].contiguous()\n",
    "\n",
    "    # We haven't yet transposed batch, so this should work (and\n",
    "    # batch_first does not apply to hidden layers in lstm, for some\n",
    "    # reason)\n",
    "    def prepare_hidden(self, batch):\n",
    "        return torch.zeros(self.model.num_layers, batch.text.size(1),\n",
    "                           self.model.hidden_dim)\n",
    "\n",
    "    def prepare_model_inputs(self, batch):\n",
    "        # TODO: this might break trigram stuff (easy to fix)...\n",
    "        if self.cuda:\n",
    "            # [batch_size, sent_len]                \n",
    "            feature, label = self.get_feature(batch).cuda(), \\\n",
    "                             self.get_label(batch).cuda()\n",
    "            if self.use_hidden:\n",
    "                # [batch_sz, num_layers, hidden_dim]\n",
    "                hidden = (self.prepare_hidden(batch).cuda(),\n",
    "                          self.prepare_hidden(batch).cuda())\n",
    "        else:\n",
    "            feature, label = self.get_feature(batch), \\\n",
    "                             self.get_label(batch)\n",
    "            if self.use_hidden:\n",
    "                hidden = (self.prepare_hidden(batch),\n",
    "                          self.prepare_hidden(batch))\n",
    "\n",
    "        # print('FEATURE BATCH')\n",
    "        # inspect_batch(feature, self._TEXT)\n",
    "        # print('LABEL BATCH')\n",
    "        # inspect_batch(label, self._TEXT)\n",
    "        \n",
    "        var_feature = autograd.Variable(feature)\n",
    "        var_label = autograd.Variable(label)\n",
    "        if self.use_hidden:\n",
    "            var_hidden = (autograd.Variable(hidden[0]),\n",
    "                          autograd.Variable(hidden[1]))\n",
    "            return ([var_feature, var_hidden], var_label)\n",
    "        else:\n",
    "            return ([var_feature], var_label)\n",
    "\n",
    "    def process_model_output(self, log_probs):\n",
    "        # print(log_probs.data)\n",
    "        print(log_probs.data.size())\n",
    "        return \"<>\"\n",
    "\n",
    "        \n",
    "            \n",
    "\n",
    "# use_hidden is special: only intended for RNN models!\n",
    "class LangEvaluator(LangModelUser):\n",
    "    def __init__(self, model, TEXT, use_hidden=False,\n",
    "                 **kwargs):\n",
    "        super(LangEvaluator, self).__init__(model, TEXT, use_hidden=use_hidden,\n",
    "                                            **kwargs)\n",
    "        self.eval_metric = kwargs.get('evalmetric', 'perplexity')\n",
    "        \n",
    "    def evaluate(self, test_iter, num_iter=None, produce_predictions=False):\n",
    "        start_time = time.time()\n",
    "        self.model.eval() # In case we have dropout\n",
    "        sum_nll = 0\n",
    "        cnt_nll = 0\n",
    "\n",
    "        predictions = []\n",
    "        for i,batch in enumerate(test_iter):\n",
    "            # Model output: [batch_size, sent_len, size_vocab]; these\n",
    "            # aren't actually probabilities if the model is a Trigram,\n",
    "            # but this doesn't matter.\n",
    "            \n",
    "            var_feature_arr, var_label = self.prepare_model_inputs(batch)\n",
    "            \n",
    "            if self.use_hidden:\n",
    "                log_probs, _ = self.model(*var_feature_arr)\n",
    "            else:\n",
    "                log_probs = self.model(*var_feature_arr)\n",
    "\n",
    "            # Get predictions for test data\n",
    "            if produce_predictions:\n",
    "                predictions.append(self.process_model_output(log_probs))\n",
    "                \n",
    "            # This is the true feature (might have hidden at 1)            \n",
    "            cnt_nll += var_label.data.size()[0] * \\\n",
    "                       var_label.data.size()[1]\n",
    "            sum_nll += self.loss_nll(var_label,\n",
    "                                     log_probs, mode='sum').data[0]\n",
    "            if not num_iter is None and i > num_iter:\n",
    "                break\n",
    "\n",
    "        if produce_predictions:\n",
    "            print('Writing test predictions to predictions.txt...')\n",
    "            with open(\"predictions.txt\", \"w\") as fout: \n",
    "                print(\"id,word\", file=fout)\n",
    "                for i,l in enumerate(predictions, 1):\n",
    "                    print(\"%d,%s\"%(i, \" \".join(l)), file=fout)\n",
    "\n",
    "        self.model.train() # Wrap model.eval()\n",
    "        print('Validation time: %f seconds' % (time.time() - start_time))\n",
    "        return np.exp(sum_nll / cnt_nll)\n",
    "\n",
    "# Option use_hidden is special: only intended for LSTM/RNN usage!\n",
    "class LangTrainer(LangModelUser):\n",
    "    def __init__(self, TEXT, model, use_hidden=False,\n",
    "                 **kwargs):\n",
    "        super(LangTrainer, self).__init__(model, TEXT, use_hidden=use_hidden,\n",
    "                                          **kwargs)\n",
    "        # Settings:\n",
    "        self.base_lrn_rate = kwargs.get('lrn_rate', 0.1)\n",
    "        optimizer = kwargs.get('optimizer', optim.SGD)\n",
    "        self._optimizer = optimizer(filter(lambda p : p.requires_grad,\n",
    "                                           model.parameters()),\n",
    "                                    lr=self.base_lrn_rate)\n",
    "\n",
    "        # Do learning rate decay:\n",
    "        lr_decay_opt = kwargs.get('lrn_decay', 'none')\n",
    "        if lr_decay_opt == 'none':\n",
    "            self.lambda_lr = lambda i : 1\n",
    "        elif lr_decay_opt == 'invlin':\n",
    "            decay_rate = kwargs.get('lrn_decay_rate', 0.1)\n",
    "            self.lambda_lr = lambda i : 1 / (1 + i * decay_rate)\n",
    "        else:\n",
    "            raise ValueError('Invalid learning rate decay option: %s' \\\n",
    "                             % lr_decay_opt)\n",
    "        self.scheduler = optim.lr_scheduler.LambdaLR(self._optimizer,\n",
    "                                                     self.lambda_lr)\n",
    "            \n",
    "        self.clip_norm = kwargs.get('clip_norm', 5)\n",
    "            \n",
    "        # TODO: implement validation thing for early stopping\n",
    "        self.training_losses = list()\n",
    "        self.training_norms = list()\n",
    "        self.val_perfs = list()\n",
    "        if self.cuda:\n",
    "            self.model.cuda()\n",
    "    \n",
    "    \n",
    "    # We are doing a slightly funky thing of taking a \n",
    "    # variable's data and then making a new \n",
    "    # variable...this is kinda unnecessary and ugly\n",
    "    def make_loss(self, batch):\n",
    "        var_feature_arr, var_label = self.prepare_model_inputs(batch)\n",
    "        if self.use_hidden:\n",
    "            log_probs, _ = self.model(*var_feature_arr)\n",
    "        else:\n",
    "            log_probs = self.model(*var_feature_arr)\n",
    "        loss = self.loss_nll(var_label, log_probs)\n",
    "        return loss\n",
    "\n",
    "    # le is a LangEvaluator, and if supplied must have a val_iter\n",
    "    # along with it\n",
    "    def train(self, torch_train_iter, le=None, val_iter=None, test_iter=None,\n",
    "              **kwargs):\n",
    "        start_time = time.time()\n",
    "        retain_graph = kwargs.get('retain_graph', False)\n",
    "        for p in self.model.parameters():\n",
    "            p.data.uniform_(-0.1, 0.1)\n",
    "        for epoch in range(kwargs.get('num_iter', 100)):\n",
    "            self.model.train()\n",
    "            # Learning rate decay, if any\n",
    "            self.scheduler.step()\n",
    "            torch_train_iter.init_epoch()\n",
    "            train_iter = iter(torch_train_iter)\n",
    "\n",
    "            for batch in train_iter:\n",
    "                self.model.zero_grad()\n",
    "                loss = self.make_loss(batch)\n",
    "                    \n",
    "                # Do gradient updates\n",
    "                loss.backward(retain_graph=retain_graph)\n",
    "\n",
    "                # Clip grad norm after backward but before step\n",
    "                if self.clip_norm > 0:\n",
    "                    # Norm clipping: returns a float\n",
    "                    norm = nn.utils.clip_grad_norm(\n",
    "                        self.model.parameters(), self.clip_norm)\n",
    "                    self.training_norms.append(norm)\n",
    "                else:\n",
    "                    self.training_norms.append(-1)\n",
    "\n",
    "                self._optimizer.step()\n",
    "\n",
    "            # Logging, early stopping\n",
    "            if epoch % kwargs.get('skip_iter', 1) == 0:\n",
    "                # Update training_losses\n",
    "                if self.cuda:\n",
    "                    self.training_losses.append(loss.data.cpu().numpy()[0])\n",
    "                else:\n",
    "                    self.training_losses.append(loss.data.numpy()[0])\n",
    "\n",
    "                # Logging\n",
    "                print('Epoch %d, loss: %f, norm: %f, elapsed: %f, lrn_rate: %f' \\\n",
    "                      % (epoch, np.mean(self.training_losses[-10:]),\n",
    "                         self.training_norms[-1],\n",
    "                         time.time() - start_time,\n",
    "                         self.base_lrn_rate * self.lambda_lr(epoch)))\n",
    "                if (not le is None) and (not val_iter is None):\n",
    "                    self.val_perfs.append(le.evaluate(val_iter))\n",
    "                    print('Validation set metric: %f' % \\\n",
    "                          self.val_perfs[-1])\n",
    "                    # We've stopped improving (basically), so stop training\n",
    "                    if len(self.val_perfs) > 2 and \\\n",
    "                       self.val_perfs[-1] > self.val_perfs[-2] - 100: #TODO: Change back to 0.1\n",
    "                        break\n",
    "\n",
    "        if kwargs.get('produce_predictions',False):\n",
    "            if (not le is None) and (not test_iter is None):\n",
    "                \n",
    "                print('Test set metric: %f' % \\\n",
    "                    le.evaluate(test_iter, produce_predictions=kwargs.get('produce_predictions',False)))\n",
    "\n",
    "        if len(self.val_perfs) > 1:\n",
    "            print('FINAL VALID PERF', self.val_perfs[-1])\n",
    "            return self.val_perfs[-1]\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perplexity goals:\n",
    "count: 120-200\n",
    "feedforward: 100-150\n",
    "recurrent: below 100 (between 80-100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA for evaluation...\n",
      "Using CUDA for evaluation...\n"
     ]
    }
   ],
   "source": [
    "train_iter, _, _  = torchtext.data.BPTTIterator.splits(\n",
    "    (train, val, test), batch_size=10, device=-1, bptt_len=32, repeat=False)\n",
    "lstmlm = LSTMLM2(TEXT)\n",
    "trainer = LangTrainer(TEXT, lstmlm, use_hidden=True, num_iter=10)\n",
    "le = LangEvaluator(TEXT, lstmlm, use_hidden=True)\n",
    "trainer.train(train_iter, le=le, val_iter=val_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IGNORE STUFF BELOW HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # NOT USED!\n",
    "    # Here arr is a 1-D numpy array; this returns \n",
    "    # groups of n consecutive words (with overlapping)\n",
    "    def get_ngrams(self, arr, n=3):\n",
    "        len_ngrams = arr.shape[0] - n + 1\n",
    "        ngram_inds = np.tile(np.reshape(np.arange(len_ngrams), [len_ngrams, 1]), [1, n]) + \\\n",
    "                    np.tile(np.reshape(np.arange(n), [1, n]), [len_ngrams, 1])\n",
    "        return np.take(arr, ngram_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 5, 8]\n"
     ]
    }
   ],
   "source": [
    "a = [1,2,3]\n",
    "b = [2 * x + i for i,x in enumerate(a)]\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10001, 300])\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.vectors.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py36)",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
