{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Text text processing library\n",
    "import torchtext\n",
    "from torchtext.vocab import Vectors\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our input $x$\n",
    "TEXT = torchtext.data.Field()\n",
    "\n",
    "# Data distributed with the assignment\n",
    "train, val, test = torchtext.datasets.LanguageModelingDataset.splits(\n",
    "    path=\".\", \n",
    "    train=\"train.txt\", validation=\"valid.txt\", test=\"valid.txt\", text_field=TEXT)\n",
    "\n",
    "TEXT.build_vocab(train)\n",
    "if debug:\n",
    "    TEXT.build_vocab(train, max_size=1000)\n",
    "\n",
    "train_iter, val_iter, test_iter = torchtext.data.BPTTIterator.splits(\n",
    "    (train, val, test), batch_size=10, device=-1, bptt_len=32, repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10])\n",
      "the government can ensure the same flow of resources and reduce the current deficit <eos> predictably guarantees outstanding have risen by $ N billion since N while direct loans outstanding have fallen\n",
      "comes across as a <unk> executive mr. phillips has a <unk> <unk> <eos> during time off mr. roman tends to his garden mr. phillips <unk> to a <unk> for among other things\n"
     ]
    }
   ],
   "source": [
    "it = iter(train_iter)\n",
    "batch = next(it)\n",
    "print(batch.text.size())\n",
    "# print(batch.text[:,3])\n",
    "print(' '.join([TEXT.vocab.itos[i] for i in batch.text[:,5].data]))\n",
    "print(' '.join([TEXT.vocab.itos[i] for i in batch.text[:,6].data]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perplexity goals:\n",
    "count: 120-200\n",
    "feedforward: 100-150\n",
    "recurrent: below 100 (between 80-100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Iteration 1000\n",
      "Iteration 2000\n"
     ]
    }
   ],
   "source": [
    "tgram = Trigram(TEXT)\n",
    "tgram.train_counts(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trigram(nn.Module):\n",
    "    def __init__(self, TEXT, args=None):\n",
    "        super(Trigram, self).__init__()\n",
    "        self._TEXT = TEXT\n",
    "        self._text_vocab_len = len(TEXT.vocab)\n",
    "        \n",
    "        # Use dictionaries since we don't want to have to \n",
    "        # store the vast majority of bi/tri gram counts \n",
    "        # which are 0.\n",
    "        self.cnts = [dict(), dict(), dict()]\n",
    "        \n",
    "    def set_alpha(self, *args):\n",
    "        self.alphas = args\n",
    "        if len(self.alphas) < 3:\n",
    "            assert len(self.alphas) == 2\n",
    "            self.alphas[2] = 1 - sum(self.alphas)\n",
    "        \n",
    "    def train_counts(self, train_iter):\n",
    "        num_iter = len(train_iter)\n",
    "        train_iter = iter(train_iter)\n",
    "        for i in range(num_iter):\n",
    "            batch = next(train_iter)\n",
    "            if i % 1000 == 0:\n",
    "                print('Iteration %d' % i)\n",
    "            self.update_trigram_cnts(torch.t(batch.text).data.numpy())\n",
    "            \n",
    "    # Batch is a torch tensor of size [size_batch, sentence_len]; \n",
    "    # this returns the probability vectors for each of the words\n",
    "    # TODO: havven't checked yet!\n",
    "    def forward(self, batch):\n",
    "        ret_arr = torch.zeros(batch.size()[0], self._text_vocab_len)\n",
    "        for i in batch.size()[0]:\n",
    "            for n in range(0,3):\n",
    "                key = tuple(batch[i, -n:])\n",
    "                if key in self.cnts[n]:\n",
    "                    ret_arr[i,:] += self.alphas[n] * self.cnts[n][key]\n",
    "        return ret_arr\n",
    "                \n",
    "    # Batch is an np array of size [batch_size, bptt_len]\n",
    "    def update_trigram_cnts(self, batch):\n",
    "        # We don't glue rows together since they may be shuffled \n",
    "        # (this is all kind of silly since ideally we'd just do \n",
    "        # this in one big 'sentence', but perhaps we want a 'fair \n",
    "        # comparison'...)\n",
    "        for j in range(batch.shape[0]):\n",
    "            for n in range(0,3):\n",
    "                for k in range(batch.shape[1] - n):\n",
    "                    dict_key = tuple(batch[j, k:k+n])\n",
    "                    if not dict_key in self.cnts[n]:\n",
    "                        self.cnts[n][dict_key] = np.zeros(self._text_vocab_len)\n",
    "                    # Here's where we increment the ocunt\n",
    "                    self.cnts[n][dict_key][batch[j, k+n]] += 1\n",
    "                    \n",
    "    # NOT USED!\n",
    "    # Here arr is a 1-D numpy array; this returns \n",
    "    # groups of n consecutive words (with overlapping)\n",
    "    def get_ngrams(self, arr, n=3):\n",
    "        len_ngrams = arr.shape[0] - n + 1\n",
    "        ngram_inds = np.tile(np.reshape(np.arange(len_ngrams), [len_ngrams, 1]), [1, n]) + \\\n",
    "                    np.tile(np.reshape(np.arange(n), [1, n]), [len_ngrams, 1])\n",
    "        return np.take(arr, ngram_inds)\n",
    "    \n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10001\n"
     ]
    }
   ],
   "source": [
    "print(len(tgram.cnts[0][()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3527539"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
