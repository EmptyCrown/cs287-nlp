{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Text text processing library\n",
    "import torchtext\n",
    "from torchtext.vocab import Vectors\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import itertools as it\n",
    "from models import *\n",
    "from helpers import *\n",
    "debug = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Our input $x$\n",
    "TEXT = torchtext.data.Field()\n",
    "\n",
    "# Data distributed with the assignment\n",
    "train, val, test = torchtext.datasets.LanguageModelingDataset.splits(\n",
    "    path=\".\", \n",
    "    train=\"train.txt\", validation=\"valid.txt\", test=\"valid.txt\", text_field=TEXT)\n",
    "\n",
    "TEXT.build_vocab(train)\n",
    "if debug:\n",
    "    TEXT.build_vocab(train, max_size=100)\n",
    "\n",
    "train_iter, val_iter, test_iter = torchtext.data.BPTTIterator.splits(\n",
    "    (train, val, test), batch_size=10, device=-1, bptt_len=32, repeat=False)\n",
    "\n",
    "# Build the vocabulary with word embeddings\n",
    "url = 'https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.simple.vec'\n",
    "TEXT.vocab.load_vectors(vectors=Vectors('wiki.simple.vec', url=url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10])\n",
      "<unk> some <unk> <unk> <eos> last year the <unk> and <unk> <unk> of <unk> co. the <unk> <unk> <unk> <unk> a <unk> to its <unk> <unk> <unk> <eos> <unk> <unk> a <unk>\n",
      "more <unk> than <unk> <unk> <unk> said <unk> <unk> <unk> <unk> <unk> the <unk> 's <unk> <unk> <eos> the <unk> <unk> <unk> in the <unk> <unk> <unk> <unk> <unk> $ N billion\n"
     ]
    }
   ],
   "source": [
    "it = iter(test_iter)\n",
    "batch = next(it)\n",
    "print(batch.text.size())\n",
    "# print(batch.text[:,3])\n",
    "print(' '.join([TEXT.vocab.itos[i] for i in batch.text[:,4].data]))\n",
    "print(' '.join([TEXT.vocab.itos[i] for i in batch.text[:,5].data]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perplexity goals:\n",
    "count: 120-200\n",
    "feedforward: 100-150\n",
    "recurrent: below 100 (between 80-100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EmbeddingsLM(nn.Module):\n",
    "    def __init__(self, TEXT, **kwargs):\n",
    "        super(EmbeddingsLM, self).__init__()\n",
    "        # Initialize dropout\n",
    "        self.dropout = nn.Dropout(kwargs.get('dropout', 0.5))\n",
    "        \n",
    "        # V is size of vocab, D is dim of embedding\n",
    "        self.V = TEXT.vocab.vectors.size()[0]\n",
    "        max_embed_norm = kwargs.get('max_embed_norm', 10)\n",
    "        if kwargs.get('pretrain_embeddings', True):\n",
    "            self.D = TEXT.vocab.vectors.size()[1]\n",
    "            self.embeddings = nn.Embedding(self.V, self.D, max_norm=max_embed_norm)\n",
    "            self.embeddings.weight = nn.Parameter(\n",
    "                TEXT.vocab.vectors, requires_grad= \\\n",
    "                kwargs.get('train_embeddings', True))\n",
    "        else:\n",
    "            self.D = kwargs.get('word_features', 100)\n",
    "            self.embeddings = nn.Embedding(self.V, self.D, max_norm=max_embed_norm)\n",
    "        \n",
    "\n",
    "class NNLM(EmbeddingsLM):\n",
    "    def __init__(self, TEXT, **kwargs):\n",
    "        # sets up self.embeddings, self.D, self.V, self.dropout\n",
    "        super(NNLM, self).__init__(TEXT, **kwargs)\n",
    "\n",
    "        # Save parameters:\n",
    "        self.activation = kwargs.get('activation', F.tanh)\n",
    "                \n",
    "        in_channels = 1\n",
    "        out_channels = kwargs.get('hidden_size', 100)\n",
    "        self.kernel_sizes_inner = [kwargs.get('kern_size_inner', 5)] \n",
    "        self.kernel_size_direct = kwargs.get('kern_size_direct', -1)\n",
    "\n",
    "        # List of convolutional layers\n",
    "        self.convs_inner = nn.ModuleList(\n",
    "            [nn.Conv2d(in_channels, out_channels, (K, self.D),\n",
    "                       padding=(K, 0)) for K in self.kernel_sizes_inner])\n",
    "        if self.kernel_size_direct > 0:\n",
    "            # Bias is already in self.linear, so don't put another here\n",
    "            self.conv_direct = nn.Conv2d(\n",
    "                in_channels, self.V, (self.kernel_size_direct, self.D),\n",
    "                padding=(self.kernel_size_direct,0), bias=False)\n",
    "\n",
    "        \n",
    "        self.linear = nn.Linear(len(self.kernel_sizes_inner) * out_channels,\n",
    "                                self.V)\n",
    "    \n",
    "    # x is [batch_sz, sent_len]: words are encoded as integers (indices)\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x) # [btch_sz, sent_len, D]\n",
    "        x = x.unsqueeze(1) # [btch_sz, in_channels, sent_len, D]\n",
    "        # [btch_sz, out_channels, sent_len] * len(kerns)\n",
    "        x = [self.activation(conv(x)).squeeze(3)\\\n",
    "             [:,:,:-(self.kernel_sizes_inner[i]+1)] for \\\n",
    "             i,conv in enumerate(self.convs_inner)]\n",
    "        # [btch_sz, out_channels * len(kerns), sent_len]\n",
    "        x = torch.cat(x, 1)\n",
    "        # [btch_sz, sent_len, out_channels * len(kerns)]\n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        x = self.dropout(x) # Bengio et al. doesn't mention dropout \n",
    "        # (it hadn't been 'discovered')\n",
    "        \n",
    "        # [btch_sz, sent_len, V]\n",
    "        x = self.linear(x) # has a bias term\n",
    "        \n",
    "        if self.kernel_size_direct > 0:\n",
    "            # [btch_sz, V, sent_len]\n",
    "            y = self.conv_direct(x)[:,:,:-(self.kernel_size_direct+1)]\n",
    "            # [btch_sz, sent_len, V]\n",
    "            y = y.permute(0, 2, 1)\n",
    "            x = x + y # '+' should be overloaded\n",
    "            \n",
    "        return F.log_softmax(x, dim=2)        \n",
    "\n",
    "class LSTMLM2(EmbeddingsLM):\n",
    "    def __init__(self, TEXT, **kwargs):\n",
    "        # sets up self.D, self.V, self.embeddings, self.dropout\n",
    "        super(LSTMLM2, self).__init__(TEXT, **kwargs)\n",
    "        \n",
    "        # Save parameters:\n",
    "        self.hidden_dim = kwargs.get('hidden_dim', 650)\n",
    "        self.num_layers = kwargs.get('num_layers', 1)\n",
    "        \n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        # TODO: Make sure LSTM does dropout the right way on the inner parameters\n",
    "        self.lstm = nn.LSTM(self.D, self.hidden_dim,\n",
    "                            num_layers=self.num_layers,\n",
    "                            dropout=kwargs.get('dropout', 0.5),\n",
    "                            batch_first=True)\n",
    "        \n",
    "        # The linear layer that maps from hidden state space to label space\n",
    "        self.linear = nn.Linear(self.hidden_dim, self.V)\n",
    "\n",
    "    # hidden should be [batch_sz, num_layers, hidden_dim]\n",
    "    def forward(self, x, hidden):\n",
    "        sent_len = x.size(1)\n",
    "        btch_sz = x.size(0)\n",
    "        x = self.embeddings(x) # [btch_sz, sent_len, D]\n",
    "\n",
    "        # hidden_out is [batch_sz, num_layers, hidden_dim]\n",
    "        lstm_out, hidden_out = self.lstm(x, hidden)\n",
    "\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "        # lstm_out is [batch_sz, sent_len, hidden]\n",
    "        pred = self.linear(lstm_out)\n",
    "        return F.log_softmax(pred, dim=2), hidden_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is unavailable...\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651908>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651a90>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651b38>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651080>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651908>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651a90>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651b38>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651080>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651908>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651a90>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651b38>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651080>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651908>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651a90>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651b38>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651080>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651908>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651a90>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651b38>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651080>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651908>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651a90>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651b38>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651080>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651908>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651a90>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651b38>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651080>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651908>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651a90>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651b38>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651080>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651908>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651a90>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651b38>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651080>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651a58>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651a90>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651908>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651080>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651a58>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651a90>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651908>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651080>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651a58>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651a90>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651908>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651080>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651a58>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651a90>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651908>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651080>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651a58>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651a90>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651908>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651080>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651a58>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651a90>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651908>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651080>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651a58>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651a90>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651908>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651080>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651a58>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651a90>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651908>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651080>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651a58>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651a90>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651908>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651080>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651a58>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651a90>\n",
      "<torchtext.data.batch.Batch object at 0x102ff0a90>\n",
      "<torchtext.data.batch.Batch object at 0x10a651908>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-58917fd6f193>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel_lstmlm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTMLM2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTEXT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLangTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTEXT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_lstmlm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_hidden\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/jesse/Documents/cs287-nlp/hw2/helpers.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, torch_train_iter, le, val_iter, **kwargs)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m                 \u001b[0;31m# Do gradient updates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py36/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py36/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 99\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_iter, _, _  = torchtext.data.BPTTIterator.splits(\n",
    "    (train, val, test), batch_size=10, device=-1, bptt_len=32, repeat=False)\n",
    "model_lstmlm = LSTMLM2(TEXT)\n",
    "trainer = LangTrainer(TEXT, model_lstmlm, use_hidden=True, num_iter=10)\n",
    "trainer.train(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, m1: [10000 x 500], m2: [50 x 10001] at /Users/soumith/code/builder/wheel/pytorch-src/torch/lib/TH/generic/THTensorMath.c:1416",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-dc4dd26590cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     (train, val, test), batch_size=1, device=-1, bptt_len=10000, repeat=False)\n\u001b[1;32m      3\u001b[0m \u001b[0mle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLangEvaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_lstmlm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTEXT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'perplexity'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/jesse/Documents/cs287-nlp/hw2/helpers.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, test_iter, num_iter, use_variable)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mvar_feature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mvar_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mlog_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_feature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0mcnt_nll\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mvar_feature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                        \u001b[0mvar_feature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-92b2ef637487>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     34\u001b[0m         lstm_out, self.hidden = self.lstm(\n\u001b[1;32m     35\u001b[0m             x.view(sent_len, btch_sz, -1), self.hidden)\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbtch_sz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py36/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py36/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m    835\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 837\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    838\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py36/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0msizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py36/lib/python3.6/site-packages/torch/functional.py\u001b[0m in \u001b[0;36mmatmul\u001b[0;34m(tensor1, tensor2, out)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_contiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, m1: [10000 x 500], m2: [50 x 10001] at /Users/soumith/code/builder/wheel/pytorch-src/torch/lib/TH/generic/THTensorMath.c:1416"
     ]
    }
   ],
   "source": [
    "_, val_iter, _  = torchtext.data.BPTTIterator.splits(\n",
    "    (train, val, test), batch_size=1, device=-1, bptt_len=10000, repeat=False)\n",
    "le = LangEvaluator(model_lstmlm, TEXT, evalmetric='perplexity')\n",
    "print(le.evaluate(val_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IGNORE STUFF BELOW HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # NOT USED!\n",
    "    # Here arr is a 1-D numpy array; this returns \n",
    "    # groups of n consecutive words (with overlapping)\n",
    "    def get_ngrams(self, arr, n=3):\n",
    "        len_ngrams = arr.shape[0] - n + 1\n",
    "        ngram_inds = np.tile(np.reshape(np.arange(len_ngrams), [len_ngrams, 1]), [1, n]) + \\\n",
    "                    np.tile(np.reshape(np.arange(n), [1, n]), [len_ngrams, 1])\n",
    "        return np.take(arr, ngram_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 5, 8]\n"
     ]
    }
   ],
   "source": [
    "a = [1,2,3]\n",
    "b = [2 * x + i for i,x in enumerate(a)]\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10001, 300])\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.vectors.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
