{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text text processing library\n",
    "import torchtext\n",
    "from torchtext.vocab import Vectors\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import itertools as it\n",
    "from models import *\n",
    "from helpers import *\n",
    "import main\n",
    "import matplotlib.pyplot as plt\n",
    "debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our input $x$\n",
    "TEXT = torchtext.data.Field()\n",
    "\n",
    "# Data distributed with the assignment\n",
    "train, val, test = torchtext.datasets.LanguageModelingDataset.splits(\n",
    "    path=\".\", \n",
    "    train=\"train.txt\", validation=\"valid.txt\", test=\"valid.txt\", text_field=TEXT)\n",
    "\n",
    "TEXT.build_vocab(train)\n",
    "if debug:\n",
    "    TEXT.build_vocab(train, max_size=1000)\n",
    "\n",
    "train_iter, val_iter, test_iter = torchtext.data.BPTTIterator.splits(\n",
    "    (train, val, test), batch_size=10, device=-1, bptt_len=32, repeat=False)\n",
    "\n",
    "# Build the vocabulary with word embeddings\n",
    "url = 'https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.simple.vec'\n",
    "TEXT.vocab.load_vectors(vectors=Vectors('wiki.simple.vec', url=url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memotec mlx nahb punts rake regatta rubens sim snack-food ssangyong swapo wachter <eos> pierre <unk> N years old will join\n",
      "in part because of buy programs generated by stock-index arbitrage a form of program trading involving futures contracts <eos> but interest <unk> as the day wore on and investors looked ahead to\n",
      "recorders and personal computers and then sell them at a huge <unk> <eos> the going rate for a small personal computer that costs about $ N in the west is anywhere from\n",
      "comes across as a <unk> executive mr. phillips has a <unk> <unk> <eos> during time off mr. roman tends to his garden mr. phillips <unk> to a <unk> for among other things\n",
      "with notes they 're <unk> distinct <eos> dean witter reynolds inc. lost its second recent arbitration case involving a former <unk> executive <eos> a new york stock exchange arbitration panel ordered dean\n",
      "EOB\n",
      "32\n",
      "the board as a nonexecutive director nov. N <eos> mr. <unk> is chairman of <unk> n.v. the dutch publishing group <eos> rudolph <unk> N years old and former chairman of consolidated gold\n",
      "the release later this week of two important economic reports <eos> the first is wednesday 's survey of purchasing managers considered a good indicator of how the nation 's manufacturing sector fared\n",
      "N to N rubles <eos> even a pack of N western cigarettes can fetch N rubles or more <eos> with more than N billion rubles in savings accounts and little to spend\n",
      "fast cars and planes <eos> industry executives say that although the two executives used to clash more frequently the wpp takeover brought them closer together <eos> i 'm the guy who made\n",
      "witter to pay $ N in back bonuses to william kelly the company 's former head of high-yield high-risk junk-bond trading and sales <eos> it also awarded $ N in back bonuses\n",
      "EOB\n"
     ]
    }
   ],
   "source": [
    "train_iter, val_iter, test_iter = torchtext.data.BPTTIterator.splits(\n",
    "    (train, val, test), batch_size=5, device=-1, bptt_len=32, repeat=False, shuffle=True)\n",
    "def inspect_batch(batch):\n",
    "    for i in range(batch.text.data.size(1)):\n",
    "        print(' '.join([TEXT.vocab.itos[i] for i in batch.text[:,i].data]))\n",
    "        # print(' '.join([TEXT.vocab.itos[i] for i in batch.text[:,5].data]))\n",
    "it = iter(train_iter)\n",
    "for i in range(2):\n",
    "    batch = next(it)\n",
    "    print(batch.text.data.size(0))\n",
    "    inspect_batch(batch)\n",
    "    print(\"EOB\")\n",
    "# print(batch.text[:,3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perplexity goals:\n",
    "count: 120-200\n",
    "feedforward: 100-150\n",
    "recurrent: below 100 (between 80-100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n"
     ]
    }
   ],
   "source": [
    "train_iter, _, _  = torchtext.data.BPTTIterator.splits(\n",
    "    (train, val, test), batch_size=1, device=-1, bptt_len=10000, repeat=False)\n",
    "tgram = Trigram(TEXT)\n",
    "tgram.train_counts(train_iter)\n",
    "tgram.set_alpha(0.25, 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "0.3 0.5 191.339042201\n"
     ]
    }
   ],
   "source": [
    "_, val_iter, _  = torchtext.data.BPTTIterator.splits(\n",
    "    (train, val, test), batch_size=1, device=-1, bptt_len=10000, repeat=False)\n",
    "for (a1, a2) in [(0.3, 0.5)]: #it.product(np.arange(0.1, 1, 0.1), repeat=2):\n",
    "    if a1 + a2 >= 1:\n",
    "        continue\n",
    "    tgram.set_alpha(a1, a2)\n",
    "    le = LangEvaluator(tgram, TEXT, evalmetric='perplexity')\n",
    "    print(a1, a2, le.evaluate(val_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.Size([10001, 300]), torch.Size([60, 1, 6, 300]), torch.Size([60]), torch.Size([10001, 1, 6, 300]), torch.Size([10001]), torch.Size([10001, 60]), torch.Size([10001])]\n"
     ]
    }
   ],
   "source": [
    "params_train = list(filter(lambda p : p.requires_grad, model_nnlm.parameters()))\n",
    "print([p.size() for p in params_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss: 9.210577, norm: 0.000000\n",
      "Iteration 10, loss: 9.138780, norm: 0.000000\n",
      "Iteration 20, loss: 9.048756, norm: 0.000000\n",
      "Iteration 30, loss: 8.949280, norm: 0.000000\n",
      "Iteration 40, loss: 8.841352, norm: 0.000000\n",
      "Iteration 50, loss: 8.572618, norm: 0.000000\n",
      "Iteration 60, loss: 8.503356, norm: 0.000000\n",
      "Iteration 70, loss: 8.215203, norm: 0.000000\n",
      "Iteration 80, loss: 7.986099, norm: 0.000000\n",
      "Iteration 90, loss: 7.916728, norm: 0.000000\n"
     ]
    }
   ],
   "source": [
    "train_iter, _, _  = torchtext.data.BPTTIterator.splits(\n",
    "    (train, val, test), batch_size=10, device=-1, bptt_len=32, repeat=False)\n",
    "model_nnlm = NNLM(TEXT)\n",
    "trainer = LangTrainer(TEXT, model_nnlm)\n",
    "trainer.train(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA for evaluation...\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 25])\n",
      "Epoch 0, loss: 170.719833, norm: 2.964386, elapsed: 53.980587, lrn_rate: 1.000000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_iter, _, _  = torchtext.data.BPTTIterator.splits(\n",
    "    (train, val, test), batch_size=128, device=-1, bptt_len=36, repeat=False)\n",
    "model_lstm = LSTMLM2(TEXT)\n",
    "trainer = LangTrainer(TEXT, model_lstm, use_hidden=True, lrn_rate=1.0)\n",
    "trainer.train(train_iter, num_iter=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA for evaluation...\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 35])\n",
      "torch.Size([128, 1]) torch.Size([128, 25])\n",
      "Validation time: 20.875873 seconds\n",
      "543.5701575796053\n",
      "torch.Size([128, 1]) torch.Size([10, 31])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "inconsistent tensor sizes at /pytorch/torch/lib/TH/generic/THTensorMath.c:2864",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-6be0fd2c849c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# print(le.evaluate(train_iter))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/cs287/hw2/helpers.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, test_iter, num_iter)\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;31m# but this doesn't matter.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0;31m# If self.prev_hidden is None, hidden will be 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m             \u001b[0mvar_feature_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_model_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_hidden\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cs287/hw2/helpers.py\u001b[0m in \u001b[0;36mprepare_model_inputs\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0;31m# [batch_size, sent_len]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_and_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m             \u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_and_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cs287/hw2/helpers.py\u001b[0m in \u001b[0;36mget_feature_and_label\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     80\u001b[0m                                   batch_transpose[:,:-self.shift_label]),\n\u001b[1;32m     81\u001b[0m                                  dim=1)\n\u001b[0;32m---> 82\u001b[0;31m                 \u001b[0mlab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_transpose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprev_feature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_transpose\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshift_label\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: inconsistent tensor sizes at /pytorch/torch/lib/TH/generic/THTensorMath.c:2864"
     ]
    }
   ],
   "source": [
    "le = LangEvaluator(model_lstm, TEXT, use_hidden=True)\n",
    "# print(le.evaluate(train_iter))\n",
    "print(le.evaluate(train_iter))\n",
    "print(le.evaluate(test_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Argument: m_word_features, Value: 100\n",
      "Argument: m_hidden_size, Value: 100\n",
      "Argument: m_dropout, Value: 0.25\n",
      "Argument: m_kern_size_inner, Value: 4\n",
      "Argument: m_kern_size_direct, Value: -1\n",
      "Argument: m_pretrain_embeddings, Value: False\n",
      "Argument: m_num_layers, Value: 1\n",
      "Argument: t_lrn_decay_rate, Value: 0.1\n",
      "Argument: t_retain_graph, Value: False\n",
      "Argument: t_lrn_rate, Value: 0.001\n",
      "Argument: t_optimizer, Value: adam\n",
      "Argument: t_lrn_decay, Value: invlin\n",
      "Argument: t_clip_norm, Value: -1\n",
      "Using CUDA for evaluation...\n",
      "Using CUDA for evaluation...\n",
      "Argument: tt_produce_predictions, Value: True\n",
      "Argument: tt_num_iter, Value: 100000\n",
      "Argument: tt_skip_iter, Value: 1\n",
      "Epoch 0, loss: 5477.533203, norm: -1.000000, elapsed: 10.869075, lrn_rate: 0.001000\n",
      "Validation time: 0.343404 seconds\n",
      "Validation set metric: 292.296586\n",
      "Epoch 1, loss: 5138.680664, norm: -1.000000, elapsed: 21.949557, lrn_rate: 0.000909\n",
      "Validation time: 0.295537 seconds\n",
      "Validation set metric: 224.433073\n",
      "Epoch 2, loss: 4948.553711, norm: -1.000000, elapsed: 33.071324, lrn_rate: 0.000833\n",
      "Validation time: 0.304502 seconds\n",
      "Validation set metric: 199.059565\n",
      "Epoch 3, loss: 4815.454102, norm: -1.000000, elapsed: 44.181146, lrn_rate: 0.000769\n",
      "Validation time: 0.307767 seconds\n",
      "Validation set metric: 185.463994\n",
      "Epoch 4, loss: 4718.419922, norm: -1.000000, elapsed: 55.209731, lrn_rate: 0.000714\n",
      "Validation time: 0.295222 seconds\n",
      "Validation set metric: 177.617737\n",
      "Epoch 5, loss: 4649.497070, norm: -1.000000, elapsed: 66.216637, lrn_rate: 0.000667\n",
      "Validation time: 0.299345 seconds\n",
      "Validation set metric: 172.458044\n",
      "Epoch 6, loss: 4583.991211, norm: -1.000000, elapsed: 77.369116, lrn_rate: 0.000625\n",
      "Validation time: 0.301249 seconds\n",
      "Validation set metric: 168.871985\n",
      "Epoch 7, loss: 4523.082031, norm: -1.000000, elapsed: 88.554230, lrn_rate: 0.000588\n",
      "Validation time: 0.300974 seconds\n",
      "Validation set metric: 166.232695\n",
      "Epoch 8, loss: 4477.510742, norm: -1.000000, elapsed: 99.704235, lrn_rate: 0.000556\n",
      "Validation time: 0.306620 seconds\n",
      "Validation set metric: 164.342246\n",
      "Epoch 9, loss: 4433.789062, norm: -1.000000, elapsed: 110.802964, lrn_rate: 0.000526\n",
      "Validation time: 0.302623 seconds\n",
      "Validation set metric: 163.077690\n",
      "Epoch 10, loss: 4383.145020, norm: -1.000000, elapsed: 121.928449, lrn_rate: 0.000500\n",
      "Validation time: 0.304660 seconds\n",
      "Validation set metric: 162.155253\n",
      "Epoch 11, loss: 4354.120117, norm: -1.000000, elapsed: 133.060224, lrn_rate: 0.000476\n",
      "Validation time: 0.309991 seconds\n",
      "Validation set metric: 161.397143\n",
      "Epoch 12, loss: 4325.317383, norm: -1.000000, elapsed: 144.143391, lrn_rate: 0.000455\n",
      "Validation time: 0.306692 seconds\n",
      "Validation set metric: 160.920216\n",
      "Epoch 13, loss: 4301.815918, norm: -1.000000, elapsed: 155.286826, lrn_rate: 0.000435\n",
      "Validation time: 0.310886 seconds\n",
      "Validation set metric: 160.703233\n",
      "Epoch 14, loss: 4270.301758, norm: -1.000000, elapsed: 166.501344, lrn_rate: 0.000417\n",
      "Validation time: 0.296936 seconds\n",
      "Validation set metric: 160.468904\n",
      "Epoch 15, loss: 4259.118164, norm: -1.000000, elapsed: 177.599205, lrn_rate: 0.000400\n",
      "Validation time: 0.304060 seconds\n",
      "Validation set metric: 160.446459\n",
      "Epoch 16, loss: 4230.734863, norm: -1.000000, elapsed: 188.880915, lrn_rate: 0.000385\n",
      "Validation time: 0.302599 seconds\n",
      "Validation set metric: 160.439856\n",
      "Epoch 17, loss: 4215.826660, norm: -1.000000, elapsed: 200.017678, lrn_rate: 0.000370\n",
      "Validation time: 0.302525 seconds\n",
      "Validation set metric: 160.516323\n",
      "Epoch 18, loss: 4185.464355, norm: -1.000000, elapsed: 211.083409, lrn_rate: 0.000357\n",
      "Validation time: 0.304330 seconds\n",
      "Validation set metric: 160.718442\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-4f8e6487ec02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLangEvaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTEXT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_hidden\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     trainer.train(train_iter, le=le, val_iter=val_iter, verbose=True,\n\u001b[0;32m---> 20\u001b[0;31m                    **main.prepare_kwargs(args, 'tt'))\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cs287/hw2/helpers.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, torch_train_iter, le, val_iter, test_set, **kwargs)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0;31m# Do gradient updates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m                 \u001b[0;31m# Clip grad norm after backward but before step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 99\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_str = ['--tt_num_iter=100000',\n",
    "             '--early_stop', '--batch_sz=1',\n",
    "             '--bptt_len=1000', '--t_lrn_rate=0.001',\n",
    "             '--t_lrn_decay=invlin', '--t_lrn_decay_rate=0.1', '--m_word_features=100 ',\n",
    "             '--t_optimizer=adam',  '--m_kern_size_inner=4', '--m_dropout=0.25',\n",
    "             '--tt_produce_predictions']\n",
    "args = main.parse_input(input_str)\n",
    "# main.train_network(args.network, args, TEXT, (train, val, test))\n",
    "model = NNLM(TEXT, **main.prepare_kwargs(args, 'm'))\n",
    "trainer = LangTrainer(TEXT, model, use_hidden=False,\n",
    "                        **main.prepare_kwargs(args, 't'))\n",
    "\n",
    "train_iter, val_iter, _ = torchtext.data.BPTTIterator.splits(\n",
    "    (train, val, test), batch_size=args.batch_sz, device=-1,\n",
    "    bptt_len=args.bptt_len, repeat=False)\n",
    "    \n",
    "if args.early_stop:\n",
    "    le = LangEvaluator(model, TEXT, use_hidden=False)\n",
    "    trainer.train(train_iter, le=le, val_iter=val_iter, verbose=True,\n",
    "                   **main.prepare_kwargs(args, 'tt'))\n",
    "else:\n",
    "    trainer.train(train_iter, verbose=True, **main.prepare_kwargs(args, 'tt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Argument: m_tie_weights, Value: True\n",
      "Argument: m_pretrain_embeddings, Value: False\n",
      "Argument: m_dropout, Value: 0.5\n",
      "Argument: m_kern_size_direct, Value: -1\n",
      "Argument: m_num_layers, Value: 2\n",
      "Argument: m_hidden_size, Value: 500\n",
      "Argument: m_word_features, Value: 500\n",
      "Argument: m_kern_size_inner, Value: 5\n",
      "Argument: t_lrn_decay, Value: invlin\n",
      "Argument: t_lrn_decay_rate, Value: 0.5\n",
      "Argument: t_retain_graph, Value: False\n",
      "Argument: t_clip_norm, Value: 5\n",
      "Argument: t_optimizer, Value: sgd\n",
      "Argument: t_lrn_rate, Value: 1.0\n",
      "Using CUDA for evaluation...\n",
      "Using CUDA for evaluation...\n",
      "Argument: tt_skip_iter, Value: 1\n",
      "Argument: tt_produce_predictions, Value: True\n",
      "Argument: tt_num_iter, Value: 100\n",
      "Epoch 0, loss: 224.925583, norm: 3.221404, elapsed: 42.138913, lrn_rate: 1.000000\n",
      "Validation time: 1.201099 seconds\n",
      "Validation set metric: 497.713983\n",
      "Epoch 1, loss: 204.006485, norm: 4.304972, elapsed: 85.941018, lrn_rate: 1.000000\n",
      "Validation time: 1.314237 seconds\n",
      "Validation set metric: 281.238881\n",
      "Epoch 2, loss: 192.837616, norm: 3.974151, elapsed: 130.722953, lrn_rate: 1.000000\n",
      "Validation time: 1.416803 seconds\n",
      "Validation set metric: 202.635096\n",
      "Epoch 3, loss: 185.989853, norm: 4.594647, elapsed: 178.075079, lrn_rate: 1.000000\n",
      "Validation time: 1.421873 seconds\n",
      "Validation set metric: 179.470034\n",
      "Epoch 4, loss: 180.735641, norm: 4.232871, elapsed: 222.865049, lrn_rate: 1.000000\n",
      "Validation time: 1.458095 seconds\n",
      "Validation set metric: 150.478268\n",
      "Epoch 5, loss: 176.700714, norm: 4.347734, elapsed: 267.908256, lrn_rate: 1.000000\n",
      "Validation time: 1.372388 seconds\n",
      "Validation set metric: 135.562875\n",
      "Epoch 6, loss: 173.250397, norm: 4.710929, elapsed: 313.232163, lrn_rate: 1.000000\n",
      "Validation time: 1.278206 seconds\n",
      "Validation set metric: 130.166470\n",
      "Epoch 7, loss: 168.064301, norm: 4.367847, elapsed: 358.731511, lrn_rate: 0.666667\n",
      "Validation time: 1.324373 seconds\n",
      "Validation set metric: 116.689090\n",
      "Epoch 8, loss: 165.277008, norm: 4.465798, elapsed: 403.806904, lrn_rate: 0.500000\n",
      "Validation time: 1.344954 seconds\n",
      "Validation set metric: 111.618034\n",
      "Epoch 9, loss: 163.193878, norm: 4.553750, elapsed: 448.331953, lrn_rate: 0.400000\n",
      "Validation time: 1.329701 seconds\n",
      "Validation set metric: 108.325846\n",
      "Epoch 10, loss: 161.932663, norm: 4.671812, elapsed: 493.342309, lrn_rate: 0.333333\n",
      "Validation time: 1.366446 seconds\n",
      "Validation set metric: 105.878099\n",
      "Epoch 11, loss: 160.642334, norm: 4.757834, elapsed: 537.539949, lrn_rate: 0.285714\n",
      "Validation time: 1.445634 seconds\n",
      "Validation set metric: 103.643273\n",
      "Epoch 12, loss: 159.699814, norm: 4.822469, elapsed: 582.161028, lrn_rate: 0.250000\n",
      "Validation time: 1.501861 seconds\n",
      "Validation set metric: 102.331841\n",
      "Epoch 13, loss: 158.779312, norm: 4.840461, elapsed: 626.965226, lrn_rate: 0.222222\n",
      "Validation time: 1.398217 seconds\n",
      "Validation set metric: 101.044916\n",
      "Epoch 14, loss: 158.346420, norm: 4.927886, elapsed: 671.904253, lrn_rate: 0.200000\n",
      "Validation time: 1.164044 seconds\n",
      "Validation set metric: 99.799877\n",
      "Epoch 15, loss: 157.791168, norm: 5.021664, elapsed: 716.690873, lrn_rate: 0.181818\n",
      "Validation time: 1.356087 seconds\n",
      "Validation set metric: 99.053957\n",
      "Epoch 16, loss: 157.075623, norm: 5.038051, elapsed: 762.023034, lrn_rate: 0.166667\n",
      "Validation time: 1.289471 seconds\n",
      "Validation set metric: 98.312428\n",
      "Epoch 17, loss: 156.749466, norm: 5.094970, elapsed: 807.390570, lrn_rate: 0.153846\n",
      "Validation time: 1.345717 seconds\n",
      "Validation set metric: 97.580759\n",
      "Epoch 18, loss: 156.052124, norm: 5.156087, elapsed: 851.558246, lrn_rate: 0.142857\n",
      "Validation time: 1.487656 seconds\n",
      "Validation set metric: 96.929694\n",
      "Epoch 19, loss: 155.714203, norm: 5.119403, elapsed: 896.571366, lrn_rate: 0.133333\n",
      "Validation time: 1.499669 seconds\n",
      "Validation set metric: 96.459033\n",
      "Epoch 20, loss: 155.590530, norm: 5.183068, elapsed: 941.646659, lrn_rate: 0.125000\n",
      "Validation time: 1.506127 seconds\n",
      "Validation set metric: 96.046953\n",
      "Epoch 21, loss: 155.234924, norm: 5.255326, elapsed: 987.142218, lrn_rate: 0.117647\n",
      "Validation time: 1.271688 seconds\n",
      "Validation set metric: 95.624550\n",
      "Epoch 22, loss: 154.837097, norm: 5.271178, elapsed: 1031.711511, lrn_rate: 0.111111\n",
      "Validation time: 1.332160 seconds\n",
      "Validation set metric: 95.169352\n",
      "Epoch 23, loss: 154.654861, norm: 5.290339, elapsed: 1076.707108, lrn_rate: 0.105263\n",
      "Validation time: 1.366821 seconds\n",
      "Validation set metric: 94.866300\n",
      "Epoch 24, loss: 154.231903, norm: 5.284136, elapsed: 1121.119551, lrn_rate: 0.100000\n",
      "Validation time: 1.404987 seconds\n",
      "Validation set metric: 94.525848\n",
      "Epoch 25, loss: 154.165253, norm: 5.320870, elapsed: 1166.356958, lrn_rate: 0.095238\n",
      "Validation time: 1.431275 seconds\n",
      "Validation set metric: 94.270896\n",
      "Epoch 26, loss: 153.897110, norm: 5.368620, elapsed: 1211.560424, lrn_rate: 0.090909\n",
      "Validation time: 1.475629 seconds\n",
      "Validation set metric: 94.013908\n",
      "Epoch 27, loss: 153.617416, norm: 5.404219, elapsed: 1256.852359, lrn_rate: 0.086957\n",
      "Validation time: 1.312067 seconds\n",
      "Validation set metric: 93.695666\n",
      "Epoch 28, loss: 153.237000, norm: 5.434409, elapsed: 1301.130390, lrn_rate: 0.083333\n",
      "Validation time: 1.376568 seconds\n",
      "Validation set metric: 93.469631\n"
     ]
    }
   ],
   "source": [
    "input_str = ['--network=lstmlm' '--tt_num_iter=100000', \n",
    "             '--early_stop', '--bptt_len=36', '--batch_sz=128', '--t_lrn_rate=1.0', \n",
    "             '--t_lrn_decay=invlin', '--t_lrn_decay_rate=0.5', \n",
    "             '--m_word_features=500', '--t_optimizer=sgd', '--t_clip_norm=5',\n",
    "             '--m_num_layers=2', '--m_dropout=0.5', \n",
    "             '--m_hidden_size=500', '--tt_produce_predictions',\n",
    "             '--m_tie_weights']\n",
    "args = main.parse_input(input_str)\n",
    "# main.train_network(args.network, args, TEXT, (train, val, test))\n",
    "model = LSTMLM2(TEXT, **main.prepare_kwargs(args, 'm'))\n",
    "trainer = LangTrainer(TEXT, model, use_hidden=True,\n",
    "                        **main.prepare_kwargs(args, 't'))\n",
    "\n",
    "train_iter, val_iter, _ = torchtext.data.BPTTIterator.splits(\n",
    "    (train, val, test), batch_size=args.batch_sz, device=-1,\n",
    "    bptt_len=args.bptt_len, repeat=False)\n",
    "    \n",
    "if args.early_stop:\n",
    "    le = LangEvaluator(model, TEXT, use_hidden=True)\n",
    "    trainer.train(train_iter, le=le, val_iter=val_iter, verbose=True,\n",
    "                   **main.prepare_kwargs(args, 'tt'))\n",
    "else:\n",
    "    trainer.train(train_iter, verbose=True, **main.prepare_kwargs(args, 'tt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f040e001a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_train_val_plots(trainer, size_epoch, sent_len, num_epochs=None):\n",
    "    if num_epochs is None:\n",
    "        num_epochs = len(trainer.val_perfs)\n",
    "        val_perfs = trainer.val_perfs\n",
    "    else:\n",
    "        val_perfs = trainer.val_perfs[:num_epochs]\n",
    "    train_losses = np.mean(np.reshape(trainer.training_losses[:num_epochs * size_epoch], \n",
    "                              (num_epochs, size_epoch)), axis=1) / sent_len\n",
    "    plt.clf()\n",
    "    # Take exp to get PPL\n",
    "    plt.plot(list(range(num_epochs)), val_perfs)\n",
    "    plt.plot(list(range(num_epochs)), list(np.exp(train_losses)))\n",
    "    plt.yscale('log')\n",
    "    plt.legend(['Validation', 'Training'])\n",
    "    plt.savefig('nnlm_plots.png')\n",
    "\n",
    "create_train_val_plots(trainer, len(train_iter), 1000, num_epochs=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IGNORE STUFF BELOW HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # NOT USED!\n",
    "    # Here arr is a 1-D numpy array; this returns \n",
    "    # groups of n consecutive words (with overlapping)\n",
    "    def get_ngrams(self, arr, n=3):\n",
    "        len_ngrams = arr.shape[0] - n + 1\n",
    "        ngram_inds = np.tile(np.reshape(np.arange(len_ngrams), [len_ngrams, 1]), [1, n]) + \\\n",
    "                    np.tile(np.reshape(np.arange(n), [1, n]), [len_ngrams, 1])\n",
    "        return np.take(arr, ngram_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 5, 8]\n"
     ]
    }
   ],
   "source": [
    "a = [1,2,3]\n",
    "b = [2 * x + i for i,x in enumerate(a)]\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10001, 300])\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.vectors.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pad_token = TEXT.vocab.stoi['<pad>']\n",
    "it = iter(train_iter)\n",
    "cnt = 0\n",
    "for batch in it:\n",
    "    cnt += 1\n",
    "    if (len(np.where(batch.text.data.numpy() == pad_token)[0])):\n",
    "        print(batch.text.data, cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "def f(x):\n",
    "    print(x)\n",
    "\n",
    "a = [1]\n",
    "f(*a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
