{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Text text processing library\n",
    "import torchtext\n",
    "from torchtext.vocab import Vectors\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our input $x$\n",
    "TEXT = torchtext.data.Field()\n",
    "\n",
    "# Data distributed with the assignment\n",
    "train, val, test = torchtext.datasets.LanguageModelingDataset.splits(\n",
    "    path=\".\", \n",
    "    train=\"train.txt\", validation=\"valid.txt\", test=\"valid.txt\", text_field=TEXT)\n",
    "\n",
    "TEXT.build_vocab(train)\n",
    "if debug:\n",
    "    TEXT.build_vocab(train, max_size=1000)\n",
    "\n",
    "train_iter, val_iter, test_iter = torchtext.data.BPTTIterator.splits(\n",
    "    (train, val, test), batch_size=10, device=-1, bptt_len=32, repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10])\n",
      "makes some executives nervous <eos> last year the research and development division of weyerhaeuser co. the large <unk> concern invited a <unk> to its <unk> wash. offices <eos> phil <unk> a software\n",
      "more expensive than direct treasury borrowing said rep. <unk> stark d. calif. the bill 's chief sponsor <eos> the complex financing plan in the s&l bailout law includes raising $ N billion\n"
     ]
    }
   ],
   "source": [
    "it = iter(test_iter)\n",
    "batch = next(it)\n",
    "print(batch.text.size())\n",
    "# print(batch.text[:,3])\n",
    "print(' '.join([TEXT.vocab.itos[i] for i in batch.text[:,4].data]))\n",
    "print(' '.join([TEXT.vocab.itos[i] for i in batch.text[:,5].data]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perplexity goals:\n",
    "count: 120-200\n",
    "feedforward: 100-150\n",
    "recurrent: below 100 (between 80-100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "Iteration 1000\n",
      "Iteration 2000\n"
     ]
    }
   ],
   "source": [
    "tgram = Trigram(TEXT)\n",
    "tgram.train_counts(train_iter)\n",
    "tgram.set_alpha(0.25, 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n"
     ]
    }
   ],
   "source": [
    "le = LangEvaluator(tgram, TEXT, evalmetric='perplexity')\n",
    "le.evaluate(val_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trigram(nn.Module):\n",
    "    def __init__(self, TEXT, **kwargs):\n",
    "        super(Trigram, self).__init__()\n",
    "        self._TEXT = TEXT\n",
    "        self._text_vocab_len = len(TEXT.vocab)\n",
    "        \n",
    "        # Use dictionaries since we don't want to have to \n",
    "        # store the vast majority of bi/tri gram counts \n",
    "        # which are 0.\n",
    "        self.cnts = [dict(), dict(), dict()]\n",
    "        \n",
    "    def set_alpha(self, *args):\n",
    "        self.alphas = list(args)\n",
    "        if len(self.alphas) < 3:\n",
    "            assert len(self.alphas) == 2\n",
    "            self.alphas.append(1 - sum(self.alphas))\n",
    "        \n",
    "    def train_counts(self, train_iter, num_iter=None):\n",
    "        if num_iter is None:\n",
    "            num_iter = len(train_iter)\n",
    "        train_iter = iter(train_iter)\n",
    "        for i in range(num_iter):\n",
    "            batch = next(train_iter)\n",
    "            if i % 1000 == 0:\n",
    "                print('Iteration %d' % i)\n",
    "            self.update_trigram_cnts(torch.t(batch.text).data)\n",
    "            \n",
    "    # Batch is a torch tensor of size [size_batch, sentence_len, size_vocab]; \n",
    "    # this returns the probability vectors for each of the words\n",
    "    # TODO: havven't checked yet!\n",
    "    def forward(self, batch):\n",
    "        ret_arr = torch.zeros(batch.size()[0], batch.size()[1], \n",
    "                              self._text_vocab_len)\n",
    "        for i in range(batch.size()[0]):\n",
    "            for n in range(0,3):\n",
    "                for j in range(batch.size()[1]):\n",
    "                    key = () if max(0, j-n) == j else tuple(batch[i, max(0, j-n):j])\n",
    "                    if key in self.cnts[n]:\n",
    "                        ret_arr[i,:] += self.alphas[n] * self.cnts[n][key]\n",
    "        # Use log probabilities to make everything fit together\n",
    "        return torch.log(ret_arr / torch.sum(ret_arr))\n",
    "                \n",
    "    # Batch is an torch tensor of size [batch_size, bptt_len]\n",
    "    def update_trigram_cnts(self, batch):\n",
    "        # We don't glue rows together since they may be shuffled \n",
    "        # (this is all kind of silly since ideally we'd just do \n",
    "        # this in one big 'sentence', but perhaps we want a 'fair \n",
    "        # comparison'...)\n",
    "        for j in range(batch.shape[0]):\n",
    "            for n in range(0,3):\n",
    "                for k in range(batch.shape[1] - n):\n",
    "                    dict_key = () if k == k+n else tuple(batch[j, k:k+n])\n",
    "                    if not dict_key in self.cnts[n]:\n",
    "                        # We never want to return 0 probability (or else PPL = infty), \n",
    "                        # so make sure we've got mass everywhere\n",
    "                        self.cnts[n][dict_key] = torch.zeros(self._text_vocab_len) if n > 0 \\\n",
    "                                else torch.ones(self._text_vocab_len)\n",
    "                    # Here's where we increment the ocunt\n",
    "                    self.cnts[n][dict_key][batch[j, k+n]] += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LangEvaluator(object):\n",
    "    def __init__(self, model, TEXT, **kwargs):\n",
    "        self._TEXT = TEXT\n",
    "        self.model = model\n",
    "        self.eval_metric = kwargs.get('evalmetric', 'perplexity')\n",
    "        \n",
    "    def evaluate(self, test_iter):\n",
    "        sum_nll = 0\n",
    "        cnt_nll = 0\n",
    "        for i,batch in enumerate(test_iter):\n",
    "            if i % 100 == 0:\n",
    "                print('Iteration %d' % i)\n",
    "            # Model output: [batch_size, sent_len, size_vocab]; these aren't actually \n",
    "            # probabilities if the model is a Trigram, but this doesn't \n",
    "            # matter.\n",
    "            batch_transpose = torch.t(batch.text).contiguous() # [batch_size, sent_len]\n",
    "            log_probs = self.model(batch_transpose)\n",
    "            cnt_nll += batch_transpose.size()[0] * batch_transpose.size()[1]\n",
    "            sum_nll += LangTrainer.loss_nll(batch_transpose.data, log_probs, mode='sum')\n",
    "            \n",
    "        return np.exp(sum_nll / cnt_nll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LangTrainer(object):\n",
    "    def __init__(self, TEXT, model, **kwargs):\n",
    "        self._TEXT = TEXT\n",
    "        self._model = model\n",
    "    \n",
    "    # Here batch is output from a RNN/NNLM/Trigram model:\n",
    "    # [..., size_vocab], and output are the real words: [...]\n",
    "    @staticmethod\n",
    "    def loss_nll(batch, output, mode='mean'):\n",
    "        # [batch_size * sent_len, size_vocab]\n",
    "        vocab_len = output.size()[-1]\n",
    "        output = output.view(-1, vocab_len)\n",
    "        # [batch_size * sent_len]\n",
    "        batch = batch.view(-1, 1)\n",
    "        batch_probs = -1 * torch.gather(output, 1, batch)\n",
    "        if mode == 'mean':\n",
    "            return torch.mean(batch_probs)\n",
    "        else:\n",
    "            return torch.sum(batch_probs)\n",
    "        return\n",
    "    \n",
    "    @staticmethod\n",
    "    def loss_perplexity(*args):\n",
    "        return torch.exp(LangTrainer.loss_nll(*args))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "a = torch.LongTensor([1,2,3])\n",
    "c = tuple(a)\n",
    "b = np.array([1,2,3])\n",
    "d = tuple(b)\n",
    "print(d == c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IGNORE STUFF BELOW HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # NOT USED!\n",
    "    # Here arr is a 1-D numpy array; this returns \n",
    "    # groups of n consecutive words (with overlapping)\n",
    "    def get_ngrams(self, arr, n=3):\n",
    "        len_ngrams = arr.shape[0] - n + 1\n",
    "        ngram_inds = np.tile(np.reshape(np.arange(len_ngrams), [len_ngrams, 1]), [1, n]) + \\\n",
    "                    np.tile(np.reshape(np.arange(n), [1, n]), [len_ngrams, 1])\n",
    "        return np.take(arr, ngram_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.7182818284590451"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
