{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Text text processing library\n",
    "import torchtext\n",
    "from torchtext.vocab import Vectors\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import itertools as it\n",
    "from models import *\n",
    "from helpers import *\n",
    "debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our input $x$\n",
    "TEXT = torchtext.data.Field()\n",
    "\n",
    "# Data distributed with the assignment\n",
    "train, val, test = torchtext.datasets.LanguageModelingDataset.splits(\n",
    "    path=\".\", \n",
    "    train=\"train.txt\", validation=\"valid.txt\", test=\"valid.txt\", text_field=TEXT)\n",
    "\n",
    "TEXT.build_vocab(train)\n",
    "if debug:\n",
    "    TEXT.build_vocab(train, max_size=1000)\n",
    "\n",
    "train_iter, val_iter, test_iter = torchtext.data.BPTTIterator.splits(\n",
    "    (train, val, test), batch_size=10, device=-1, bptt_len=32, repeat=False)\n",
    "\n",
    "# Build the vocabulary with word embeddings\n",
    "url = 'https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.simple.vec'\n",
    "TEXT.vocab.load_vectors(vectors=Vectors('wiki.simple.vec', url=url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10])\n",
      "makes some executives nervous <eos> last year the research and development division of weyerhaeuser co. the large <unk> concern invited a <unk> to its <unk> wash. offices <eos> phil <unk> a software\n",
      "more expensive than direct treasury borrowing said rep. <unk> stark d. calif. the bill 's chief sponsor <eos> the complex financing plan in the s&l bailout law includes raising $ N billion\n"
     ]
    }
   ],
   "source": [
    "it = iter(test_iter)\n",
    "batch = next(it)\n",
    "print(batch.text.size())\n",
    "# print(batch.text[:,3])\n",
    "print(' '.join([TEXT.vocab.itos[i] for i in batch.text[:,4].data]))\n",
    "print(' '.join([TEXT.vocab.itos[i] for i in batch.text[:,5].data]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perplexity goals:\n",
    "count: 120-200\n",
    "feedforward: 100-150\n",
    "recurrent: below 100 (between 80-100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n"
     ]
    }
   ],
   "source": [
    "train_iter, _, _  = torchtext.data.BPTTIterator.splits(\n",
    "    (train, val, test), batch_size=1, device=-1, bptt_len=10000, repeat=False)\n",
    "tgram = Trigram(TEXT)\n",
    "tgram.train_counts(train_iter)\n",
    "tgram.set_alpha(0.25, 0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n",
      "0.3 0.5 191.339042201\n"
     ]
    }
   ],
   "source": [
    "_, val_iter, _  = torchtext.data.BPTTIterator.splits(\n",
    "    (train, val, test), batch_size=1, device=-1, bptt_len=10000, repeat=False)\n",
    "for (a1, a2) in [(0.3, 0.5)]: #it.product(np.arange(0.1, 1, 0.1), repeat=2):\n",
    "    if a1 + a2 >= 1:\n",
    "        continue\n",
    "    tgram.set_alpha(a1, a2)\n",
    "    le = LangEvaluator(tgram, TEXT, evalmetric='perplexity')\n",
    "    print(a1, a2, le.evaluate(val_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNLM(nn.Module):\n",
    "    def __init__(self, TEXT, **kwargs):\n",
    "        super(NNLM, self).__init__()\n",
    "\n",
    "        # Save parameters:\n",
    "        self.activation = kwargs.get('activation', F.tanh)\n",
    "        self.direct_cxn = kwargs.get('direct_cxn', False)\n",
    "        \n",
    "        # V is size of vocab, D is dim of embedding\n",
    "        V = TEXT.vocab.vectors.size()[0]\n",
    "        D = TEXT.vocab.vectors.size()[1]\n",
    "        self.embeddings = nn.Embedding(V, D)\n",
    "        self.embeddings.weight = nn.Parameter(\n",
    "            TEXT.vocab.vectors, requires_grad= \\\n",
    "            kwargs.get('train_embeddings', True))\n",
    "        \n",
    "        in_channels = 1\n",
    "        out_channels = 60\n",
    "        self.kernel_sizes_inner = [6] \n",
    "        self.kernel_size_direct = 6\n",
    "\n",
    "        # List of convolutional layers\n",
    "        self.convs_inner = nn.ModuleList(\n",
    "            [nn.Conv2d(in_channels, out_channels, (K, D),\n",
    "                       padding=(K, 0)) for K in self.kernel_sizes_inner])\n",
    "        \n",
    "        self.conv_direct = nn.Conv2d(\n",
    "            in_channels, V, (K, D),\n",
    "            padding=(self.kernel_size_direct,0))\n",
    "\n",
    "        self.dropout = nn.Dropout(kwargs.get('dropout', 0.5))\n",
    "        \n",
    "        self.linear = nn.Linear(len(self.kernel_sizes_inner) * out_channels, V)\n",
    "    \n",
    "    # x is [batch_sz, sent_len]: words are encoded as integers (indices)\n",
    "    def forward(x):\n",
    "        x = self.embeddings(x) # [btch_sz, sent_len, D]\n",
    "        x = x.unsqueeze(1) # [btch_sz, in_channels, sent_len, D]\n",
    "        # [btch_sz, out_channels, sent_len] * len(kerns)\n",
    "        x = [self.activation(conv(x)).squeeze(3)\\\n",
    "             [:,:,:-(self.kernel_sizes_inner[i]+1)] for \\\n",
    "             i,conv in enumerate(self.convs1)]\n",
    "        # [btch_sz, out_channels * len(kerns), sent_len]\n",
    "        x = torch.cat(x, 1)\n",
    "        # [btch_sz, sent_len, out_channels * len(kerns)]\n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        # x = self.dropout(x) # Bengio et al. doesn't mention dropout \n",
    "        # (it hadn't been 'discovered')\n",
    "        \n",
    "        # [btch_sz, sent_len, V]\n",
    "        x = self.linear(x) # has a bias term\n",
    "        \n",
    "        if self.direct_cxn:\n",
    "            # [btch_sz, V, sent_len]\n",
    "            y = self.conv_direct(x)[:,:,:-(self.kernel_size_direct+1)]\n",
    "            # [btch_sz, sent_len, V]\n",
    "            y = y.permute(0, 2, 1)\n",
    "            \n",
    "        return F.log_softmax(x + y, dim=2)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LangTrainer2(object):\n",
    "    def __init__(self, TEXT, model, **kwargs):\n",
    "        # Settings:\n",
    "        self._optimizer = optimizer(filter(lambda p : p.requires_grad,\n",
    "                                           model.parameters()),\n",
    "                                    lr=kwargs.get('lr', 0.1))        \n",
    "        self.cuda = kwargs.get('cuda', True) and \\\n",
    "            torch.cuda.is_available()\n",
    "        self.clip_norm = kwargs.get('clip_norm', 5)\n",
    "            \n",
    "        self._TEXT = TEXT\n",
    "        self.model = model\n",
    "        # TODO: implement validation thing for early stopping\n",
    "        self.training_losses = list()\n",
    "        self.training_norms = list()\n",
    "        if self.cuda:\n",
    "            self.model.cuda()\n",
    "    \n",
    "    # Here batch is output from a RNN/NNLM/Trigram model:\n",
    "    # [..., size_vocab], and output are the real words: [...]\n",
    "    @staticmethod\n",
    "    def loss_nll(batch, output, mode='mean'):\n",
    "        # [batch_size * sent_len, size_vocab]\n",
    "        vocab_len = output.size()[-1]\n",
    "        output = output.view(-1, vocab_len)\n",
    "        # [batch_size * sent_len]\n",
    "        batch = batch.view(-1, 1)\n",
    "        batch_probs = -1 * torch.gather(output, 1, batch)\n",
    "        if mode == 'mean':\n",
    "            return torch.mean(batch_probs)\n",
    "        else:\n",
    "            return torch.sum(batch_probs)\n",
    "        return\n",
    "    \n",
    "    @staticmethod\n",
    "    def loss_perplexity(*args):\n",
    "        return torch.exp(self.loss_nll(*args))\n",
    "    \n",
    "    def get_feature(self, batch):\n",
    "        return torch.t(batch.text.data).contiguous()\n",
    "\n",
    "    # The labels we use as the true words: same as features\n",
    "    def get_label(self, batch):\n",
    "        return self.get_feature(batch)\n",
    "    \n",
    "    # We are doing a slightly funky thing of taking a \n",
    "    # variable's data and then making a new \n",
    "    # variable...this seems cleaner though\n",
    "    def make_loss(self, batch):\n",
    "        if self.cuda:\n",
    "            feature, label = self.get_feature(batch).cuda(), \\\n",
    "                            self.get_label(batch).cuda()\n",
    "        else:\n",
    "            feature, label = self.get_feature(batch), \\\n",
    "                            self.get_label(batch)\n",
    "        var_feature = autograd.Variable(feature)\n",
    "        var_label = autograd.Variable(label)\n",
    "        loss = self.loss_nll(self.model(var_feature), var_label)\n",
    "        return loss\n",
    "    \n",
    "    def train(self, train_iter, **kwargs):\n",
    "        train_iter = iter(train_iter)\n",
    "        for i in range(kwargs.get('num_iter', 100)):\n",
    "            batch = next(train_iter)\n",
    "            self.model.zero_grad()\n",
    "            loss = self.make_loss(batch)\n",
    "            self.training_losses.append(loss.data.numpy()[0])\n",
    "\n",
    "                \n",
    "                \n",
    "            # Norm clipping\n",
    "            norm = nn.utils.clip_grad_norm(filter(lambda p : p.requires_grad,\n",
    "                                                 model.parameters()), self.clip_norm)\n",
    "            self.training_norms.append(norm.data.numpy()[0])    \n",
    "            if i % kwargs.get('skip_iter', 10) == 0:\n",
    "                print('Iteration %d, loss: %f, norm: %f' % (i, self.training_losses[-1],\n",
    "                                                           self.training_norms[-1]))\n",
    "            loss.backward()\n",
    "            self._optimizer.step()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "a = torch.LongTensor([1,2,3])\n",
    "c = tuple(a)\n",
    "b = np.array([1,2,3])\n",
    "d = tuple(b)\n",
    "print(d == c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IGNORE STUFF BELOW HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # NOT USED!\n",
    "    # Here arr is a 1-D numpy array; this returns \n",
    "    # groups of n consecutive words (with overlapping)\n",
    "    def get_ngrams(self, arr, n=3):\n",
    "        len_ngrams = arr.shape[0] - n + 1\n",
    "        ngram_inds = np.tile(np.reshape(np.arange(len_ngrams), [len_ngrams, 1]), [1, n]) + \\\n",
    "                    np.tile(np.reshape(np.arange(n), [1, n]), [len_ngrams, 1])\n",
    "        return np.take(arr, ngram_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 5, 8]\n"
     ]
    }
   ],
   "source": [
    "a = [1,2,3]\n",
    "b = [2 * x + i for i,x in enumerate(a)]\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10001, 300])\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.vectors.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
