
@inproceedings{wang_baselines_2012,
	address = {Jeju, Republic of Korea},
	title = {Baselines and bigrams: simple, good sentiment and topic classification},
	url = {http://www.aclweb.org/anthology/P/P12/P12-2.pdf#page=118},
	urldate = {2018-01-31},
	booktitle = {Proceedings for the 50th {Meeting} of the {Association} of {Computational} {Linguistics}},
	author = {Wang, Sida and Manning, Christopher},
	year = {2012},
	pages = {119--122},
	file = {P12-2.pdf:/Users/ngolowich/Zotero/storage/ZBRBJ2NE/P12-2.pdf:application/pdf}
}

@article{mikolov_efficient_2013,
	title = {Efficient {Estimation} of {Word} {Representations} in {Vector} {Space}},
	url = {http://arxiv.org/abs/1301.3781},
	abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
	urldate = {2018-01-31},
	journal = {arXiv:1301.3781 [cs]},
	author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	month = jan,
	year = {2013},
	note = {arXiv: 1301.3781},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv\:1301.3781 PDF:/Users/ngolowich/Zotero/storage/8FSCDK9M/Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Ve.pdf:application/pdf;arXiv.org Snapshot:/Users/ngolowich/Zotero/storage/4BUCERA5/1301.html:text/html}
}

@article{collobert_natural_2011,
	title = {Natural {Language} {Processing} (almost) from {Scratch}},
	url = {http://arxiv.org/abs/1103.0398},
	abstract = {We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including: part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements.},
	urldate = {2018-01-31},
	journal = {arXiv:1103.0398 [cs]},
	author = {Collobert, Ronan and Weston, Jason and Bottou, Leon and Karlen, Michael and Kavukcuoglu, Koray and Kuksa, Pavel},
	month = mar,
	year = {2011},
	note = {arXiv: 1103.0398},
	keywords = {Computer Science - Learning, Computer Science - Computation and Language},
	file = {arXiv\:1103.0398 PDF:/Users/ngolowich/Zotero/storage/EE2UKW6H/Collobert et al. - 2011 - Natural Language Processing (almost) from Scratch.pdf:application/pdf;arXiv.org Snapshot:/Users/ngolowich/Zotero/storage/CG46EBDJ/1103.html:text/html}
}

@inproceedings{kim_convolutiona_2014,
	title = {Convolutiona neural networks for sentence classification},
	url = {http://aclweb.org/anthology/D/D14/D14-1181.pdf},
	urldate = {2018-01-31},
	booktitle = {Conference on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	author = {Kim, Yoon},
	year = {2014},
	file = {D14-1181.pdf:/Users/ngolowich/Zotero/storage/EN3D7WKV/D14-1181.pdf:application/pdf}
}

@book{socher_recursive_2013,
	title = {Recursive {Deep} {Models} for {Semantic} {Compositionality} {Over} a {Sentiment} {Treebank}},
	abstract = {Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment compositionality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80 \% up to 85.4\%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7\%, an improvement of 9.7 \% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases. 1},
	author = {Socher, Richard and Perelygin, Alex and Wu, Jean Y. and Chuang, Jason and Manning, Christopher D. and Ng, Andrew Y. and Potts, Christopher},
	year = {2013},
	file = {Citeseer - Full Text PDF:/Users/ngolowich/Zotero/storage/RBNCWQHU/Socher et al. - Recursive Deep Models for Semantic Compositionalit.pdf:application/pdf;Citeseer - Snapshot:/Users/ngolowich/Zotero/storage/DGY235WX/summary.html:text/html;EMNLP2013_RNTN.pdf:/Users/ngolowich/Zotero/storage/HIUK69Q4/EMNLP2013_RNTN.pdf:application/pdf}
}

@article{bojanowski_enriching_2016,
	title = {Enriching {Word} {Vectors} with {Subword} {Information}},
	url = {http://arxiv.org/abs/1607.04606},
	abstract = {Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character \$n\$-grams. A vector representation is associated to each character \$n\$-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.},
	urldate = {2018-01-31},
	journal = {arXiv:1607.04606 [cs]},
	author = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
	month = jul,
	year = {2016},
	note = {arXiv: 1607.04606},
	keywords = {Computer Science - Learning, Computer Science - Computation and Language},
	file = {arXiv\:1607.04606 PDF:/Users/ngolowich/Zotero/storage/3ZMBTE8I/Bojanowski et al. - 2016 - Enriching Word Vectors with Subword Information.pdf:application/pdf;arXiv.org Snapshot:/Users/ngolowich/Zotero/storage/9RSW8ZAR/1607.html:text/html}
}